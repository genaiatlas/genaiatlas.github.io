{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Generative AI Study Hub","text":"<p>Last updated: July 28, 2025</p> <p>This is your central hub for learning and referencing Generative AI concepts.  Choose from the Study Path to go through structured topics or explore the Reference Hub for focused research and tooling insights.</p>"},{"location":"01-foundation/","title":"\ud83e\uddf1 Foundation","text":"<p>\ud83d\udcc2 Navigation Tip This section lays the groundwork for all advanced topics in Generative AI. Use the left-hand navigation menu to explore foundational topics including Python basics, statistics, and data science &amp; machine learning.</p> <p>\ud83d\udca1 Begin with Python to ensure your programming fundamentals are solid before diving into statistical reasoning or ML concepts.</p>"},{"location":"01-foundation/1-python/","title":"\ud83d\udc0d Python","text":"<p>Welcome to the Python section of the Generative AI Atlas.</p>"},{"location":"01-foundation/1-python/#learning-path","title":"\ud83e\udded Learning Path","text":"<p>Navigate through structured lessons to master core concepts.</p> <ul> <li>Intro to Python</li> <li>Collection of Variables</li> <li>Conditional Looping</li> <li>Functions</li> <li>NumPy</li> <li>Pandas</li> <li>Data Visualization</li> </ul>"},{"location":"01-foundation/1-python/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Deep dive into real-world case studies, extended examples, and supporting materials.</p> <ul> <li>Case Study: CardioGood Fitness</li> <li>\"Case Study: Cardiogood \": 01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness.md</li> <li>\"Case Study: Uber\": 01-foundation/1-python/2-additional-reference/02-case-study-uber.md</li> <li>\"Case Study: EDA\": 01-foundation/1-python/2-additional-reference/03-case-study-EDA.md</li> </ul>"},{"location":"01-foundation/1-python/#qa","title":"\u2753 Q&amp;A","text":"<p>Explore discussions, clarifications, and annotated insights from real learners and experts.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/","title":"\ud83e\udde0 Statistics &amp; Data Science: Python Introduction","text":""},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Print Statements</li> <li>2. Variables</li> <li>3. Data Types</li> <li>4. Basic Operators</li> <li>5. Data Structures \u2013 List</li> <li>6. Data Structures \u2013 Tuple</li> <li>7. Data Structures \u2013 Dictionary</li> <li>8. Type Checking</li> <li>9. Comparison Operators</li> <li>10. Conditional Statements</li> <li>12. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#1-print-statements","title":"1. Print Statements","text":"<ul> <li>Python\u2019s <code>print()</code> function is used to display output.</li> </ul> <pre><code>print('The name of the company is Cars Sons Ltd.')\nprint('The year the company was established is', 1996)\nprint('Total turnover of the company this year in Million $ is', 12.5)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example","title":"Use Case Example","text":"<p>Essential for logging, debugging, and showing results of analysis.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#2-variables","title":"2. Variables","text":"<ul> <li>Containers for storing data values.</li> </ul> <pre><code>company_name = \"Cars Sons Ltd.\"\nyear_started = 1996\nturnover = 12.5\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_1","title":"Use Case Example","text":"<p>Used to store company attributes, which feed into models or business dashboards.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#3-data-types","title":"3. Data Types","text":"<ul> <li><code>int</code>, <code>float</code>, <code>str</code> are common types.</li> </ul> <pre><code>type(company_name)  # str\ntype(year_started)  # int\ntype(turnover)      # float\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_2","title":"Use Case Example","text":"<p>Understanding data types is essential for error-free data preprocessing.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#4-basic-operators","title":"4. Basic Operators","text":"<ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>, <code>**</code></li> </ul> <pre><code>price_sedan = 0.2\ntotal_price = price_sedan * 10\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_3","title":"Use Case Example","text":"<p>Used in calculating sales metrics or KPIs in business analytics.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#5-data-structures-list","title":"5. Data Structures \u2013 List","text":"<ul> <li>Lists store multiple items.</li> </ul> <pre><code>cars = ['Sedan', 'SUV', 'Hatchback']\nsales = [200, 400, 300]\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_4","title":"Use Case Example","text":"<p>Track car types or regional sales in customer segmentation.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#6-data-structures-tuple","title":"6. Data Structures \u2013 Tuple","text":"<ul> <li>Immutable ordered collections.</li> </ul> <pre><code>sitting = (5, 4, 6)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_5","title":"Use Case Example","text":"<p>Store constant metadata like seat capacity or configuration specs.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#7-data-structures-dictionary","title":"7. Data Structures \u2013 Dictionary","text":"<ul> <li>Key-value pairs for structured data.</li> </ul> <pre><code>sales_last_month = {'Sedan': 2, 'SUV': 1.5}\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_6","title":"Use Case Example","text":"<p>Used for storing car-wise sales figures, performance ratings, etc.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#8-type-checking","title":"8. Type Checking","text":"<pre><code>type(sales_last_month)  # dict\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_7","title":"Use Case Example","text":"<p>Validating type before transformations in ETL workflows.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#9-comparison-operators","title":"9. Comparison Operators","text":"<ul> <li>Compare data values: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code></li> </ul> <pre><code>sales['Sedan'] &gt; sales['Hatchback']\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_8","title":"Use Case Example","text":"<p>Used in dashboards to compare performance trends.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#10-conditional-statements","title":"10. Conditional Statements","text":"<ul> <li>Use <code>if</code>, <code>else</code> to apply logic</li> </ul> <pre><code>if cars_ratings['Sedan_1'] &gt; 50:\n    print(\"Good performer\")\nelse:\n    print(\"Needs improvement\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_9","title":"Use Case Example","text":"<p>Flag underperforming models for review.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python</li> <li>Khan Academy Statistics</li> <li>MIT OpenCourseWare</li> <li>StatQuest (YouTube)</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/","title":"\ud83e\udde0 Statistics &amp; Data Science: Collection of Variables","text":"<p>This document provides hands-on practice with foundational Python data structures \u2014 specifically lists, dictionaries, and tuples \u2014 used throughout data science and statistics programming.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. List Operations</li> <li>2. Dictionary Operations</li> <li>3. Tuple &amp; Set Creation</li> <li>\ud83d\udcc2 CSV Download</li> <li>\ud83d\udd17 References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#1-list-operations","title":"1. List Operations","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept","title":"Concept","text":"<p>A list in Python is a mutable, ordered collection of elements. Lists allow duplicate values and support indexing, slicing, and iteration.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-examples","title":"Key Examples","text":"<ul> <li>Create a list with repeated elements</li> <li>Print the second element of a list</li> <li>Replace an element in a list</li> <li>Iterate through a list and print values</li> <li>Compute squares of numbers in a list</li> <li>Remove (pop) an element from a list</li> <li>Check the length of a list</li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example","title":"Use Case Example","text":"<p>In retail analytics, you might use lists to track product IDs sold during a day, apply transformations (e.g., discounts), or aggregate values like revenue per transaction.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#2-dictionary-operations","title":"2. Dictionary Operations","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept_1","title":"Concept","text":"<p>A dictionary in Python is an unordered, mutable collection that maps keys to values. Keys must be unique and hashable.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-examples_1","title":"Key Examples","text":"<ul> <li>Create a dictionary with brand, model, and year</li> <li>Modify a value in a dictionary (e.g., updating year)</li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example_1","title":"Use Case Example","text":"<p>Customer profiles in CRM systems often use dictionaries to store user information like <code>{\"name\": \"John\", \"age\": 45, \"premium\": True}</code>.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#3-tuple-set-creation","title":"3. Tuple &amp; Set Creation","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept_2","title":"Concept","text":"<ul> <li>A tuple is an immutable, ordered sequence.</li> <li>A set is an unordered collection of unique elements.</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-example","title":"Key Example","text":"<ul> <li>Create a set with elements: <code>1.0</code>, <code>\"Hello\"</code>, <code>55</code>, <code>(6,7,8)</code></li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example_2","title":"Use Case Example","text":"<p>Sets are used to eliminate duplicates, such as identifying unique website visitors from a log file.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Documentation</li> <li>Seaborn Documentation</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare</li> <li>StatQuest YouTube Channel</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/","title":"\ud83e\udde0 Statistics &amp; Data Science: Control Flow in Python","text":""},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Conditional Statements</li> <li>2. Looping in Python</li> <li>2.1 For Loop</li> <li>2.2 While Loop</li> <li>2.3 For Loop with Conditional Statements</li> <li>2.4 For Loop with Accumulation</li> <li>2.5 For Loop with Filtering</li> <li>2.6 For Loop with Character Iteration</li> <li>2.7 For Loop with Repetition</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#1-conditional-statements","title":"1. Conditional Statements","text":"<ul> <li>Conditional statements allow programs to make decisions based on conditions.</li> <li>Commonly used keywords: <code>if</code>, <code>elif</code>, <code>else</code>.</li> </ul> <pre><code># Example: Check if a number is even or odd\na = 2020\nif a % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#use-case-example","title":"Use Case Example","text":"<p>Conditional statements are essential for business logic, e.g., evaluating credit scores to approve/reject a loan.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#2-looping-in-python","title":"2. Looping in Python","text":"<p>Loops allow you to repeat a block of code multiple times.</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#21-for-loop","title":"2.1 For Loop","text":"<ul> <li>Used to iterate over a sequence (list, tuple, range, etc.)</li> </ul> <pre><code>for i in range(5):\n    print(i)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#22-while-loop","title":"2.2 While Loop","text":"<ul> <li>Repeats as long as a condition is true</li> </ul> <pre><code>i = 1\nwhile i &lt;= 5:\n    print(i)\n    i += 1\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#23-for-loop-with-conditional-statements","title":"2.3 For Loop with Conditional Statements","text":"<pre><code>for i in range(1, 11):\n    if i % 2 == 0:\n        print(f\"{i} is even\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#24-for-loop-with-accumulation","title":"2.4 For Loop with Accumulation","text":"<pre><code>total = 0\nfor i in range(1, 101):\n    total += i\nprint(total)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#25-for-loop-with-filtering","title":"2.5 For Loop with Filtering","text":"<pre><code>for i in range(1, 51):\n    if i % 7 == 0 and i % 5 != 0:\n        print(i)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#26-for-loop-with-character-iteration","title":"2.6 For Loop with Character Iteration","text":"<pre><code>st = \"Data Science\"\nfor char in st:\n    print(char)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#27-for-loop-with-repetition","title":"2.7 For Loop with Repetition","text":"<pre><code>for _ in range(5):\n    print(\"Data Science\")\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#use-case-example_1","title":"Use Case Example","text":"<p>Looping constructs are widely used in automation, report generation, and simulations in data analysis workflows.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python</li> <li>Khan Academy Statistics</li> <li>MIT OpenCourseWare</li> <li>StatQuest (YouTube)</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/","title":"\ud83e\udde0 Statistics &amp; Data Science \u2013 Python Functions","text":""},{"location":"01-foundation/1-python/1-learning-path/04-functions/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Basic Arithmetic Function</li> <li>2. Squares in Range</li> <li>3. Simple Interest Calculation</li> <li>4. Divisibility by 25</li> <li>5. Square and Add Five</li> <li>6. Lambda: Square and Add Five</li> <li>7. Power Function</li> <li>8. Triangle Area</li> <li>9. Country Origin Function</li> <li>10. Celsius to Fahrenheit</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#1-basic-arithmetic-function","title":"1. Basic Arithmetic Function","text":"<p>Define a function to add, subtract, multiply, and divide two variables.</p> <pre><code>def perform_operations(a, b):\n    print(f\"Addition: {a + b}\")\n    print(f\"Subtraction: {a - b}\")\n    print(f\"Multiply: {a * b}\")\n    if b != 0:\n        print(f\"Division: {a / b}\")\n    else:\n        print(\"Division by zero is not allowed.\")\n\nperform_operations(10, 5)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#2-squares-in-range","title":"2. Squares in Range","text":"<p>Function to print squares of numbers from 1 to 10.</p> <pre><code>def print_squares_in_range():\n    for i in range(1, 11):\n        print(i * i)\n\nprint_squares_in_range()\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#3-simple-interest-calculation","title":"3. Simple Interest Calculation","text":"<p>Formula: \\(SI = \\frac{P \\times R \\times T}{100}\\)</p> <pre><code>def calculate_simple_interest(principal, rate, time):\n    return (principal * rate * time) / 100\n\ncalculate_simple_interest(1000, 3, 5)\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#4-divisibility-by-25","title":"4. Divisibility by 25","text":"<pre><code>def is_divisible_by_25(number):\n    return True if number % 25 == 0 else \"Not divisible\"\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#5-square-and-add-five","title":"5. Square and Add Five","text":"<pre><code>def square_and_add_five(number):\n    return number ** 2 + 5\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#6-lambda-square-and-add-five","title":"6. Lambda: Square and Add Five","text":"<pre><code>square_add_five_lambda = lambda x: x ** 2 + 5\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#7-power-function","title":"7. Power Function","text":"<pre><code>def calculate_power(base, exponent):\n    return base ** exponent\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#8-triangle-area","title":"8. Triangle Area","text":"<p>Formula: \\(Area = \\frac{1}{2} \\times base \\times height\\)</p> <pre><code>def calculate_triangle_area(base, height):\n    return 0.5 * base * height\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#9-country-origin-function","title":"9. Country Origin Function","text":"<pre><code>def print_country_origin(country):\n    return f\"I am from {country}\"\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#10-celsius-to-fahrenheit","title":"10. Celsius to Fahrenheit","text":"<p>Formula: \\(F = \\frac{9}{5}C + 32\\)</p> <pre><code>def celsius_to_fahrenheit(celsius):\n    return (celsius * 9 / 5) + 32\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Docs</li> <li>Khan Academy - Statistics</li> <li>MIT OCW</li> <li>StatQuest with Josh Starmer</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/","title":"\ud83e\uddee Chapter 1.1: NumPy","text":""},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to NumPy</li> <li>2. NumPy Functions</li> <li>3. Accessing NumPy Array</li> <li>4. Modifying the Entries of a Matrix</li> <li>5. Saving and Loading NumPy Arrays</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#1-introduction-to-numpy","title":"1. Introduction to NumPy","text":"<p>NumPy (Numerical Python) is a core Python library for scientific computing, offering fast, flexible multi-dimensional <code>ndarray</code> objects and a suite of mathematical tools(numpy.org, w3schools.com, stackoverflow.com, geeksforgeeks.org). Arrays are stored in contiguous memory, enabling vectorized operations that run orders of magnitude faster than pure Python loops(geeksforgeeks.org).</p> <pre><code>import numpy as np\n\narray = np.array([1, 2, 3])\nprint(array)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example","title":"Use Case Example","text":"<p>\ud83d\udcca NumPy is used in financial analysis for high-speed matrix operations when simulating stock market behaviors.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#2-numpy-functions","title":"2. NumPy Functions","text":"<p>Common NumPy functions include reshaping, random number generation, mathematical operations, and broadcasting.</p> <pre><code>a = np.arange(10).reshape(2, 5)\nb = np.random.rand(2, 5)\nprint(np.add(a, b))\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83e\udde0 Neuroscience labs use NumPy to simulate electrical signals across neurons represented in matrix form.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#3-accessing-numpy-array","title":"3. Accessing NumPy Array","text":"<p>Accessing elements is done with indexing and slicing.</p> <pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr[0, 1])  # Output: 2\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcc8 Sports analysts extract specific performance metrics from multidimensional datasets using array indexing.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#4-modifying-the-entries-of-a-matrix","title":"4. Modifying the Entries of a Matrix","text":"<p>You can change values in NumPy arrays directly using indexing or conditions.</p> <pre><code>matrix = np.array([[1, 2], [3, 4]])\nmatrix[0, 1] = 10\nprint(matrix)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83e\uddec Bioinformaticians adjust gene expression data arrays during cleaning and normalization stages.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#5-saving-and-loading-numpy-arrays","title":"5. Saving and Loading NumPy Arrays","text":"<p>Use <code>.npy</code> and <code>.npz</code> formats for saving and restoring arrays efficiently.</p> <pre><code>np.save(\"my_array.npy\", arr)\nloaded = np.load(\"my_array.npy\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcc2 Machine learning engineers store preprocessed input arrays for quick reuse across experiments.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/","title":"\ud83d\udc3c Chapter 1.2: Pandas","text":""},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Pandas</li> <li>2. Accessing Series and DataFrames</li> <li>3. loc and iloc in Pandas</li> <li>4. Condition-Based Indexing</li> <li>5. Combining DataFrames</li> <li>6. Saving and Loading DataFrames</li> <li>7. Statistical Functions</li> <li>8. GroupBy Function</li> <li>9. Date and Time Functions</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#1-introduction-to-pandas","title":"1. Introduction to Pandas","text":"<p>Pandas is a powerful Python library for data manipulation and analysis. It provides data structures like Series and DataFrames.</p> <pre><code>import pandas as pd\n\ndata = {\"Name\": [\"Alice\", \"Bob\"], \"Age\": [25, 30]}\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example","title":"Use Case Example","text":"<p>\ud83c\udfe5 Hospitals use Pandas to store and manipulate patient records for analysis and visualization.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#2-accessing-series-and-dataframes","title":"2. Accessing Series and DataFrames","text":"<p>Pandas Series is a one-dimensional labeled array, and DataFrame is a two-dimensional table.</p> <pre><code>series = df[\"Age\"]\nprint(series)\nprint(type(series))\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83d\udcbc HR analysts retrieve employee age or salary columns to perform segmentation.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#3-loc-and-iloc-in-pandas","title":"3. loc and iloc in Pandas","text":"<ul> <li><code>.loc[]</code> accesses rows by label.</li> <li><code>.iloc[]</code> accesses rows by index position.</li> </ul> <pre><code>print(df.loc[0])   # Access by label\nprint(df.iloc[1])  # Access by index\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcca Researchers extract specific survey responses using row labels or positions.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#4-condition-based-indexing","title":"4. Condition-Based Indexing","text":"<p>Filter DataFrames using Boolean conditions.</p> <pre><code>df[df[\"Age\"] &gt; 26]\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83c\udfe2 Companies filter customers by age for targeted advertising campaigns.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#5-combining-dataframes","title":"5. Combining DataFrames","text":"<p>You can concatenate or merge multiple DataFrames.</p> <pre><code>df1 = pd.DataFrame({\"ID\": [1, 2], \"Name\": [\"Alice\", \"Bob\"]})\ndf2 = pd.DataFrame({\"ID\": [1, 2], \"Age\": [25, 30]})\nmerged = pd.merge(df1, df2, on=\"ID\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udce6 Merging order and customer data allows fulfillment centers to improve logistics tracking.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#6-saving-and-loading-dataframes","title":"6. Saving and Loading DataFrames","text":"<p>Save and load datasets using CSV or Excel formats.</p> <pre><code>df.to_csv(\"output.csv\", index=False)\ndf_loaded = pd.read_csv(\"output.csv\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83d\udcbd Analysts export cleaned datasets to share with business teams or dashboards.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#7-statistical-functions","title":"7. Statistical Functions","text":"<p>Pandas includes descriptive statistics like mean, median, std, etc.</p> <pre><code>df[\"Age\"].mean()\ndf.describe()\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcc9 Healthcare analysts summarize patient vitals and lab results for reports.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#8-groupby-function","title":"8. GroupBy Function","text":"<p>Group data by a categorical variable and apply aggregate functions.</p> <pre><code>df.groupby(\"Name\")[\"Age\"].mean()\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83d\udecd\ufe0f Retailers group transactions by store to compute average revenue per store.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#9-date-and-time-functions","title":"9. Date and Time Functions","text":"<p>Convert and manipulate datetime formats in Pandas.</p> <pre><code>df[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Year\"] = df[\"Date\"].dt.year\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_8","title":"Use Case Example","text":"<p>\ud83d\udcc5 Analysts break down sales by month or quarter using datetime fields.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download StockData.csv</p> <p>\ud83d\udcce View StockData.csv in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/","title":"\ud83d\udcca Chapter 1.3: Data Visualization","text":""},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Data Loading and Overview</li> <li>2. Histogram</li> <li>3. Histogram with Density Curve</li> <li>4. Box Plot</li> <li>5. Line Plot</li> <li>6. Scatter Plot</li> <li>7. lm Plot in Seaborn</li> <li>8. Swarm Plot</li> <li>9. Pair Plot</li> <li>10. Heat Map</li> <li>11. Plotly</li> <li>12. Customizing Plots</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#1-data-loading-and-overview","title":"1. Data Loading and Overview","text":"<p>Data loading is the first step in any data visualization or analysis pipeline. It involves reading data from CSV, Excel, or APIs using libraries like <code>pandas</code>.</p> <ul> <li>Use <code>pandas.read_csv()</code> to load CSV files.</li> <li>Understand data types, null values, and summary statistics.</li> </ul> <pre><code>import pandas as pd\ndf = pd.read_csv(\"your_dataset.csv\")\ndf.info()\ndf.describe()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example","title":"Use Case Example","text":"<p>\ud83d\udcc8 In healthcare, loading large hospital datasets with patient records allows data scientists to quickly inspect missing values and perform visual triage of data quality.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#2-histogram","title":"2. Histogram","text":"<p>Histograms display the distribution of numerical data by grouping values into bins.</p> <ul> <li>Great for identifying skewness, spread, and outliers.</li> <li>Created using <code>seaborn.histplot()</code> or <code>matplotlib.pyplot.hist()</code>.</li> </ul> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(df[\"age\"], bins=10)\nplt.show()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83c\udf93 Universities use histograms to visualize student grade distributions across departments.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#3-histogram-with-density-curve","title":"3. Histogram with Density Curve","text":"<p>Combining histograms with KDE (Kernel Density Estimation) overlays helps visualize probability density.</p> <pre><code>sns.histplot(df[\"income\"], kde=True)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcb0 Financial analysts use density curves over income brackets to detect abnormal income distributions for fraud detection.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#4-box-plot","title":"4. Box Plot","text":"<p>Box plots summarize the distribution using median, quartiles, and outliers.</p> <pre><code>sns.boxplot(x=\"region\", y=\"salary\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83c\udfe5 Hospitals compare patient waiting times by department using box plots to identify service bottlenecks.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#5-line-plot","title":"5. Line Plot","text":"<p>Line plots show trends over time or ordered categories.</p> <pre><code>sns.lineplot(x=\"year\", y=\"revenue\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcc9 Economists visualize GDP trends over decades with line plots.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#6-scatter-plot","title":"6. Scatter Plot","text":"<p>Scatter plots reveal relationships between two numeric variables.</p> <pre><code>sns.scatterplot(x=\"height\", y=\"weight\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83d\udcca Insurance companies use scatter plots to assess correlations between age and insurance claims.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#7-lm-plot-in-seaborn","title":"7. lm Plot in Seaborn","text":"<p>lm plots add regression lines to scatter plots, useful for trend analysis.</p> <pre><code>sns.lmplot(x=\"experience\", y=\"salary\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcbc HR departments forecast salary expectations based on years of experience.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#8-swarm-plot","title":"8. Swarm Plot","text":"<p>Swarm plots show all data points and avoid overlapping unlike box plots.</p> <pre><code>sns.swarmplot(x=\"department\", y=\"satisfaction\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83e\uddea Clinical research teams visualize patient responses to different treatments using swarm plots.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#9-pair-plot","title":"9. Pair Plot","text":"<p>Pair plots visualize relationships across multiple variables in one grid.</p> <pre><code>sns.pairplot(df[[\"age\", \"income\", \"expenses\"]])\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_8","title":"Use Case Example","text":"<p>\ud83d\udcca Marketing teams explore consumer segmentation by analyzing patterns in spending behavior.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#10-heat-map","title":"10. Heat Map","text":"<p>Heatmaps visualize matrix-like data using color intensities.</p> <pre><code>sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_9","title":"Use Case Example","text":"<p>\ud83d\udd0d Data scientists use heatmaps to visualize correlation matrices for feature selection in ML pipelines.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#11-plotly","title":"11. Plotly","text":"<p>Plotly provides interactive, zoomable, and publishable charts in Python.</p> <pre><code>import plotly.express as px\npx.scatter(df, x=\"age\", y=\"income\", color=\"region\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_10","title":"Use Case Example","text":"<p>\ud83d\udcca News outlets use Plotly to create interactive COVID-19 dashboards for public engagement.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#12-customizing-plots","title":"12. Customizing Plots","text":"<p>Customize color palettes, themes, fonts, and axis labels for clearer visuals.</p> <pre><code>sns.set(style=\"whitegrid\", palette=\"pastel\")\nsns.boxplot(x=\"gender\", y=\"score\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_11","title":"Use Case Example","text":"<p>\ud83d\udce2 In presentations, using color-blind-friendly palettes ensures accessibility for all stakeholders.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/","title":"\ud83e\udde0 Case Study: CardioGood Fitness Data Analysis","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#1-overview","title":"1. Overview","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#problem-statement-cardiogood-fitness-data-analysis","title":"Problem Statement \u2013 CardioGood Fitness Data Analysis","text":"<p>Context: The market research team at AdRight is assigned the task to identify the profile of the typical customer for each treadmill product offered by CardioGood Fitness. The market research team decides to investigate whether there are differences across the product lines with respect to customer characteristics. The team decides to collect data on individuals who purchased a treadmill at a CardioGood Fitness retail store at any time in the past three months. The data is stored in the <code>CardioGoodFitness.csv</code> file.</p> <p>Objective: Perform descriptive analysis to create a customer profile for each CardioGood Fitness treadmill product line.</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#2-dataset-description","title":"2. Dataset Description","text":"<p>Data Dictionary: The team identified the following customer variables to study:</p> <ul> <li>Product: Product purchased - TM195, TM498, or TM798  </li> <li>Gender: Male or Female  </li> <li>Age: Age of the customer in years  </li> <li>Education: Education of the customer in years  </li> <li>MaritalStatus: Single or Partnered  </li> <li>Income: Annual household income  </li> <li>Usage: The average number of times the customer plans to use the treadmill each week  </li> <li>Miles: The average number of miles the customer expects to walk/run each week  </li> <li>Fitness: Self-rated fitness on a 1-to-5 scale, where 1 is poor shape and 5 is excellent shape</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#3-methodology","title":"3. Methodology","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#questions-to-explore","title":"Questions to Explore:","text":"<ol> <li>What are the different types of variables in the data?  </li> <li>What is the distribution of different variables in the data?  </li> <li>Which product is more popular among males or females?  </li> <li>Is the product purchase affected by the marital status of the customer?  </li> <li>Is there a significant correlation among some of the variables?  </li> <li>What is the distribution of the average number of miles for each product?</li> </ol>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p> <pre><code># Sample code block from the solution notebook\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"CardioGoodFitness.csv\")\nprint(df.head())\n\n# Visualizing product preference by gender\nsns.countplot(data=df, x='Product', hue='Gender')\nplt.title(\"Product Preference by Gender\")\nplt.show()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<p>Insights will be derived through descriptive analysis using data visualizations and statistical exploration in the Colab notebook. Key focus areas include:</p> <ul> <li>Product popularity across demographic segments</li> <li>Relationship between fitness and treadmill usage</li> <li>Impact of education, age, and income on product choice</li> </ul> <p>Refer to the Google Colab notebook for detailed implementation and visuals.</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83c\udfaf This analysis helps the business: - Define distinct customer personas per product line - Design targeted marketing strategies - Identify potential market gaps across demographics</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CardioGoodFitness.csv</p> <p>\ud83d\udcce View CardioGoodFitness.csv</p> <p>\ud83d\udcca View Solutions Data (XLSX)</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/","title":"\ud83e\udde0 Practical Exercise: Study 2 \u2013 Uber Demand Pattern Analysis","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#1-overview","title":"1. Overview","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#context","title":"Context","text":"<p>Uber Technologies, Inc. is an American multinational transportation network company based in San Francisco and has operations in approximately 72 countries and 10,500 cities. In the fourth quarter of 2021, Uber had 118 million monthly active users worldwide and generated an average of 19 million trips per day.</p> <p>Ridesharing is a very volatile market and demand fluctuates wildly with time, place, weather, local events, etc. The key to being successful in this business is to be able to detect patterns in these fluctuations and cater to demand at any given time.</p> <p>As a newly hired Data Scientist in Uber's New York Office, you have been given the task of extracting insights from data that will help the business better understand the demand profile and take appropriate actions to drive better outcomes for the business. Your goal is to identify good insights that are potentially actionable, i.e., the business can do something with it.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#objective","title":"Objective","text":"<p>To extract actionable insights around demand patterns across various factors.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#2-dataset-description","title":"2. Dataset Description","text":"<p>The dataset contains information about the weather, location, and pickups. Below is the data dictionary:</p> <ul> <li>pickup_dt: Date and time of the pick-up  </li> <li>borough: NYC's borough  </li> <li>pickups: Number of pickups for the period  </li> <li>spd: Wind speed in miles/hour  </li> <li>vsb: Visibility in miles to the nearest tenth  </li> <li>temp: Temperature in Fahrenheit  </li> <li>dewp: Dew point in Fahrenheit  </li> <li>slp: Sea level pressure  </li> <li>pcp01: 1-hour liquid precipitation  </li> <li>pcp06: 6-hour liquid precipitation  </li> <li>pcp24: 24-hour liquid precipitation  </li> <li>sd: Snow depth in inches  </li> <li>hday: Being a holiday (Y) or not (N)</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#3-methodology","title":"3. Methodology","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#key-questions","title":"Key Questions","text":"<ol> <li>What are the different variables that influence pickups?  </li> <li>Which factor affects the pickups the most? What could be plausible reasons for that?  </li> <li>What are your recommendations to Uber management to capitalize on fluctuating demand?</li> </ol>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#guidelines","title":"Guidelines","text":"<ul> <li>Perform univariate analysis to better understand individual variables.</li> <li>Perform bivariate analysis to explore relationships between variables.</li> <li>Create visualizations to explore the data and extract actionable insights.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"Uber.csv\")\nprint(df.head())\n\n# Example: Analyze pickups by borough\nsns.boxplot(data=df, x='borough', y='pickups')\nplt.title(\"Distribution of Pickups by NYC Borough\")\nplt.xticks(rotation=45)\nplt.show()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>Borough-wise Patterns: Some boroughs consistently show higher ride demand.</li> <li>Weather Influence: Variables like temperature and precipitation correlate with demand.</li> <li>Holiday Trends: Holidays (hday = Y) may show increased or decreased pickup patterns depending on context.</li> </ul> <p>Visualizations and full interpretation are included in the Google Colab notebook.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83d\udcca These insights help Uber to: - Adjust pricing or fleet placement by borough - Forecast ride surges during bad weather or holidays - Optimize driver incentives during low-visibility conditions</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download Uber.csv</p> <p>\ud83d\udcce View Uber.csv in Google Drive</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/","title":"\ud83d\udcca Exploratory Data Analysis (EDA)","text":""},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Data Loading and Initial Exploration</li> <li>2. Checking Missing, Duplicate Values, and Summary</li> <li>3. Univariate Analysis</li> <li>4. Bivariate Analysis</li> <li>5. Charts and Plots for EDA</li> <li>6. Missing Values - Group Mean</li> <li>7. Missing Values - Medians &amp; Dropping</li> <li>8. Outlier Detection and Analysis</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#1-data-loading-and-initial-exploration","title":"1. Data Loading and Initial Exploration","text":"<p>Begin by loading the housing dataset and performing initial inspection using Pandas.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Melbourne_Housing.csv\")\ndf.head()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example","title":"Use Case Example","text":"<p>\ud83c\udfd8\ufe0f Real estate analysts begin their data pipeline with loading raw housing sales data to inspect types and formats.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#2-checking-missing-duplicate-values-and-summary","title":"2. Checking Missing, Duplicate Values, and Summary","text":"<p>Use <code>.isnull().sum()</code> to inspect missing values, and <code>.describe()</code> for a statistical summary.</p> <pre><code>print(df.isnull().sum())\ndf.describe()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83d\udd0d Detecting columns with substantial missing data helps decide cleaning strategy in housing market analytics.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#3-univariate-analysis","title":"3. Univariate Analysis","text":"<p>Study single variable distributions using histograms and value counts.</p> <pre><code>df[\"Price\"].hist(bins=50)\ndf[\"Type\"].value_counts()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcb2 Understand housing price distribution to identify outliers or skewness in Melbourne.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#4-bivariate-analysis","title":"4. Bivariate Analysis","text":"<p>Use scatter plots and correlation matrices to explore relationships between variables.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.scatterplot(x=\"Distance\", y=\"Price\", data=df)\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83d\udcc9 Discover how distance from the city center influences real estate pricing.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#5-charts-and-plots-for-eda","title":"5. Charts and Plots for EDA","text":"<p>Generate box plots, bar charts, pair plots, and violin plots for deeper insight.</p> <pre><code>sns.boxplot(x=\"Type\", y=\"Price\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcca Visual analytics are used in real estate investment dashboards to summarize sales trends by property type.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#6-missing-values-group-mean","title":"6. Missing Values - Group Mean","text":"<p>Fill missing values with group-specific means.</p> <pre><code>df[\"BuildingArea\"].fillna(df.groupby(\"Type\")[\"BuildingArea\"].transform(\"mean\"), inplace=True)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83c\udfd7\ufe0f Impute area details for residential properties based on type-specific averages.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#7-missing-values-medians-dropping","title":"7. Missing Values - Medians &amp; Dropping","text":"<p>Alternative strategy for missing value treatment by median filling or row dropping.</p> <pre><code>df[\"Bedroom2\"].fillna(df[\"Bedroom2\"].median(), inplace=True)\ndf.dropna(inplace=True)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcc9 Reducing noise caused by sparse or unfixable data improves ML model training.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#8-outlier-detection-and-analysis","title":"8. Outlier Detection and Analysis","text":"<p>Detect and remove outliers using IQR or z-score techniques.</p> <pre><code>Q1 = df[\"Price\"].quantile(0.25)\nQ3 = df[\"Price\"].quantile(0.75)\nIQR = Q3 - Q1\n\nfiltered_df = df[(df[\"Price\"] &gt;= Q1 - 1.5 * IQR) &amp; (df[\"Price\"] &lt;= Q3 + 1.5 * IQR)]\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83d\udca1 Outlier removal leads to more stable forecasting of property valuation models.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download Melbourne_Housing.csv \ud83d\udcce View Melbourne_Housing.csv</p> <p>\ud83d\udc49 Download Melbourne_Housing_NoMissing.csv \ud83d\udcce View Melbourne_Housing_NoMissing.csv</p> <p>\ud83d\udc49 Download Melbourne_Housing_NoOutliers.csv \ud83d\udcce View Melbourne_Housing_NoOutliers.csv</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/","title":"\ud83e\udde0 FIFA World Cup Case Study","text":""},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#1-overview","title":"1. Overview","text":"<p>The FIFA World Cup is one of the most prestigious tournaments in the world of football. Organized every four years (except during World War II), it brings together top international teams competing for global glory.</p> <p>In this case study, you are a member of the newly formed Brussels United FC, and have been tasked with analyzing historical FIFA World Cup data up to the year 2014.</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#2-dataset-description","title":"2. Dataset Description","text":"<p>The dataset includes the following features:</p> <ul> <li>Year: The year in which the World Cup was held.</li> <li>Country: Host country.</li> <li>Winner: Country that won the tournament.</li> <li>Runners-Up: Second place team.</li> <li>Third: Third place team.</li> <li>Fourth: Fourth place team.</li> <li>GoalsScored: Total number of goals scored during the tournament.</li> <li>QualifiedTeams: Number of qualified national teams.</li> <li>MatchesPlayed: Total number of matches played.</li> <li>Attendance: Total spectator attendance.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#3-methodology","title":"3. Methodology","text":"<p>We will explore the data using the following steps:</p> <ul> <li>Perform exploratory data analysis (EDA).</li> <li>Answer questions related to performance and trends over years.</li> <li>Visualize key metrics such as goals, attendance, and winning teams.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open Questions Notebook in Colab \ud83d\udc49 Open Solutions Notebook in Colab</p> <p></p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>An upward trend in GoalsScored over decades reflects attacking style changes.</li> <li>Attendance has generally increased, showing the growing popularity of the event.</li> <li>Patterns in repeated wins highlight dominance by select nations like Brazil and Germany.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>Understanding historical data helps clubs like Brussels United FC: - Study legacy and benchmark against international performance. - Inspire young talent by learning from top-performing nations. - Design predictive models for future tournaments based on trends.</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive \ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>FIFA Official World Cup Page</li> <li>Kaggle Dataset (for additional sources)</li> <li>Matplotlib Documentation</li> <li>Pandas Documentation</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/","title":"\ud83d\udc0d Python Reference &amp; Learning Resources","text":""},{"location":"01-foundation/1-python/2-additional-reference/05-references/#official-documentation","title":"\ud83d\udcd8 Official Documentation","text":"<ul> <li> <p>Python Docs (Latest)</p> <p>The official Python documentation. Covers all built-in types, standard libraries, functions, language reference, tutorials, and best practices.</p> </li> <li> <p>PEP Index (Python Enhancement Proposals)</p> <p>Formal documents describing new features or design aspects in Python. Ideal for understanding the evolution of the language.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#beginner-to-intermediate-tutorials","title":"\ud83c\udf93 Beginner to Intermediate Tutorials","text":"<ul> <li> <p>W3Schools - Python Tutorial</p> <p>Simple, interactive, beginner-friendly tutorials. Covers basics to advanced concepts with live code editor.</p> </li> <li> <p>Real Python</p> <p>High-quality articles, video courses, and tutorials. Covers data structures, web frameworks, OOP, and practical use cases.</p> </li> <li> <p>Programiz - Python</p> <p>Easy-to-follow tutorials with examples for absolute beginners.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#interactive-learning-playgrounds","title":"\ud83d\udcbb Interactive Learning &amp; Playgrounds","text":"<ul> <li> <p>Python Tutor (Step-by-step visualizer)</p> <p>Visualize Python code execution step-by-step \u2014 great for debugging and learning how code flows.</p> </li> <li> <p>Replit - Python Playground</p> <p>Online IDE to write, run, and share Python code instantly without setup.</p> </li> <li> <p>Google Colab</p> <p>Jupyter notebook environment with free cloud GPUs. Perfect for AI, ML, and data science experiments.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#advanced-professional","title":"\ud83d\udee0\ufe0f Advanced &amp; Professional","text":"<ul> <li> <p>Awesome Python</p> <p>Curated list of Python frameworks, libraries, software, and resources. A goldmine for pros and enthusiasts.</p> </li> <li> <p>Python Module Index (PyPI)</p> <p>Python Package Index \u2014 search, install, and learn about third-party packages.</p> </li> <li> <p>Python Cheatsheet (GitHub)</p> <p>One-page, comprehensive Python cheat sheet with examples.</p> </li> <li> <p>Full Stack Python</p> <p>Learn how to build, deploy, and scale Python web applications using best practices and popular libraries.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#data-science-ml-with-python","title":"\ud83d\udcca Data Science &amp; ML with Python","text":"<ul> <li> <p>Scikit-learn Docs</p> <p>User guide, API reference, and tutorials for machine learning with Python.</p> </li> <li> <p>Pandas Docs</p> <p>In-depth guide to data manipulation using Python.</p> </li> <li> <p>Matplotlib Docs</p> <p>Comprehensive visualization library for Python.</p> </li> <li> <p>TensorFlow Python API</p> <p>Official TensorFlow API reference for deep learning models.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#books-free-and-online","title":"\ud83d\udcda Books (Free and Online)","text":"<ul> <li> <p>Think Python (2e)</p> <p>An introduction to Python programming \u2014 easy to follow and excellent for CS fundamentals.</p> </li> <li> <p>Python for Everybody (free ebook)</p> <p>A practical programming book for data analysis and web access.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/","title":"\ud83d\udcca Statistics","text":"<p>Welcome to the Statistics section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/2-statistics/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Understand the foundational statistical tools that fuel AI decision-making.</p> <ul> <li>Descriptive Stats</li> <li>Inferential Stats</li> <li>Hypothesis Testing</li> </ul>"},{"location":"01-foundation/2-statistics/#additional-reference","title":"\ud83d\udcc2 Additional Reference","text":"<p>Explore supporting notebooks, annotated case studies, and extended walkthroughs.</p> <ul> <li>\"Case Study: Mobile Usage\": 01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage.md</li> <li>\"Case Study: Medicon\": 01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing.md</li> </ul>"},{"location":"01-foundation/2-statistics/#qa","title":"\u2753 Q&amp;A","text":"<p>Explore questions and annotated insights around key statistical ideas.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/","title":"\ud83d\udcca Descriptive Statistics \u2013 A Practical Guide","text":"<p>This lesson provides a detailed, example-rich walkthrough of key Descriptive Statistics concepts using Python. It includes real-world data analysis using Pandas, NumPy, and Seaborn, and is designed for students and data practitioners using the MkDocs Material theme.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Descriptive Statistics</li> <li>2. Dataset Overview</li> <li>3. Central Tendency Measures</li> <li>4. Variability Measures</li> <li>5. Group-wise Descriptive Analysis</li> <li>6. CSV Download</li> <li>7. Colab Notebook</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#1-introduction-to-descriptive-statistics","title":"1. Introduction to Descriptive Statistics","text":"<p>Descriptive statistics summarize and describe the main features of a dataset in a quantitative manner. They form the foundation of exploratory data analysis (EDA).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#2-dataset-overview","title":"2. Dataset Overview","text":"<p>This dataset (<code>descriptive_statistics_sample.csv</code>) contains sample records with <code>ID</code>, <code>Age</code>, <code>Income</code>, <code>SatisfactionScore</code>, and <code>PurchaseFrequency</code>.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#3-central-tendency-measures","title":"3. Central Tendency Measures","text":""},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#measures-covered","title":"Measures Covered:","text":"<ul> <li>Mean</li> <li>Median</li> <li>Mode</li> </ul> <pre><code>import pandas as pd\ndata = pd.read_csv('descriptive_statistics_sample.csv')\nprint(\"Mean Age:\", data['Age'].mean())\nprint(\"Median Age:\", data['Age'].median())\nprint(\"Mode Age:\", data['Age'].mode()[0])\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#4-variability-measures","title":"4. Variability Measures","text":""},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#measures-covered_1","title":"Measures Covered:","text":"<ul> <li>Range</li> <li>Variance</li> <li>Standard Deviation</li> </ul> <pre><code>range_income = data['Income'].max() - data['Income'].min()\nprint(\"Income Range:\", range_income)\nprint(\"Income Variance:\", data['Income'].var())\nprint(\"Income Std Dev:\", data['Income'].std())\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#5-group-wise-descriptive-analysis","title":"5. Group-wise Descriptive Analysis","text":"<p>Analyze descriptive stats by group (e.g., age group or satisfaction score).</p> <pre><code>age_bins = pd.cut(data['Age'], bins=[18, 30, 45, 60], labels=['18-30', '31-45', '46-60'])\ngrouped = data.groupby(age_bins)['PurchaseFrequency'].mean()\nprint(grouped)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#6-csv-download","title":"6. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive \ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#7-colab-notebook","title":"7. Colab Notebook","text":"<p>\ud83d\udc49 Open Notebook in Google Colab \ud83d\udcce View on Google Drive</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Matplotlib Docs</li> <li>Khan Academy: Statistics</li> <li>MIT OCW \u2013 Stats Courses</li> <li>StatQuest with Josh Starmer</li> <li>Scikit-Learn User Guide</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/","title":"\ud83d\udcca Inferential Statistics","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Inferential Statistics<ul> <li>1.1 Random Variable </li> <li>1.2 Discrete Random Variable </li> <li>1.3 Continous Random Variable </li> <li>1.4 Probability Distribution </li> </ul> </li> <li>2. Fundamental Terms in Distributions</li> <li>3. Binomial Distribution</li> <li>4. Uniform Distribution</li> <li>5. Normal Distribution</li> <li>6. Z-Score</li> <li>7. Sampling &amp; Inference Foundations</li> <li>8. Central Limit Theorem</li> <li>9. Estimation</li> <li>10. Hypothesis Testing</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-introduction-to-inferential-statistics","title":"1. Introduction to Inferential Statistics","text":"<p>Back to Top</p> <p>Inferential statistics help us draw conclusions about populations based on sample data.</p> <pre><code># Example: confidence interval\nimport numpy as np\nimport scipy.stats as stats\n\nsample = np.array([85, 80, 78, 90, 88])\nconf_interval = stats.t.interval(0.95, len(sample)-1, loc=np.mean(sample), scale=stats.sem(sample))\nprint(conf_interval)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcca Used by analysts to infer population metrics from SAT sample scores.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#11-random-variable","title":"1.1 Random Variable","text":"<p>Back to Top</p> <p>Suppose there are 1,000 students in the university.</p> <p>Question: What is the probability that 500 students will pass the upcoming exam?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#setup","title":"\ud83e\udde0 Setup","text":"<p>Back to Top</p> <ul> <li>Each student has a 50-50 chance of passing or failing the exam.</li> <li>We are observing the total number of students who pass, which can range from 0 to 1000.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-concept","title":"\ud83d\udccc Key Concept","text":"<p>Back to Top</p> <p>A random variable assigns a numerical value to each outcome of an experiment. It assumes different values with different probabilities.</p> <p>This numerical outcome can represent a count (like number of students who pass), a measurement, or any quantifiable result of a probabilistic process.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#12-discrete-random-variable","title":"1.2 Discrete Random Variable","text":"<p>Back to Top</p> <p>You work for an auto insurance company. Suppose the number of insurance claims filed by a driver in a month is a random variable \\(X\\) described as follows:</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#distribution-of-x","title":"\ud83d\udd22 Distribution of X","text":"<p>Back to Top</p> <p>Let \\(X =\\)</p> Claims (x) Probability P(X = x) 0 0.95 1 0.04 2 0.008 3 0.002"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#rule","title":"\u2705 Rule","text":"<p>Back to Top</p> <p>All probabilities must be non-negative and must sum to 1.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#13-continuous-random-variable","title":"1.3 Continuous Random Variable","text":"<p>Back to Top</p> <p>Suppose the volume of soda in a bottle is described by a random variable.</p> <p>Can we list all possible values?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#example-values","title":"\ud83e\uddea Example Values","text":"<p>Back to Top</p> <ul> <li>498 mL  </li> <li>499 mL  </li> <li>500 mL  </li> <li>...  </li> <li>What about 499.2129415 mL?</li> </ul> <p>Sometimes it's just not possible to list all values a random variable can take.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-concept_1","title":"\ud83e\udde0 Key Concept","text":"<p>Back to Top</p> <p>If the random variable can take any value in a given range, we call it a continuous random variable.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#14-probability-distribution","title":"1.4 Probability Distribution","text":"<p>Back to Top</p> <p>A Probability Distribution describes the values that a random variable can take, along with the probabilities of those values.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#two-main-types","title":"\ud83d\udd00 Two Main Types","text":"<p>Back to Top</p> Type Description Associated Function \ud83d\udfe2 Discrete Probability Distribution Arises from discrete random variables. Probability Mass Function (PMF) Gives the probability that the variable takes a specific value. \ud83d\udd35 Continuous Probability Distribution Arises from continuous random variables. Probability Density Function (PDF) Determines the probability that the variable lies between two values."},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary","title":"\ud83d\udccc Summary","text":"<p>Back to Top</p> <ul> <li>A PMF is used for countable outcomes (e.g., number of claims, dice rolls).</li> <li>A PDF is used for uncountable outcomes over a range (e.g., height, volume).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-distribution-example","title":"\ud83d\udcc8 Probability Distribution: Example","text":"<p>A company tracks the number of sales new employees make each day during a 100-day probationary period. The results for one new employee are shown below. Using this, we construct and plot a probability distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#tabulated-data","title":"\ud83e\uddee Tabulated Data","text":"<p>Back to Top</p> Sales (per day) # of Days Relative Frequency 0 16 0.16 1 18 0.18 2 15 0.15 3 21 0.21 4 11 0.11 5 10 0.10 6 9 0.09"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#bar-plot-interpretation","title":"\ud83d\udcca Bar Plot Interpretation","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_1","title":"\ud83e\udde0 Summary","text":"<p>Back to Top</p> <ul> <li> <p>The sum of relative frequencies: <code>0.16 + 0.18 + 0.15 + 0.21 + 0.11 + 0.10 + 0.09 = 1.00 \u2705</code></p> </li> <li> <p>This confirms a valid probability distribution.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#-","title":"---","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-fundamental-terms-in-distributions","title":"2. Fundamental Terms in Distributions","text":"<p>Back to Top</p> <p>Covers mean, variance, standard deviation, skewness, and kurtosis.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_1","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcc8 Financial institutions assess risk using variance and skewness of return distributions.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#3-binomial-distribution","title":"3. Binomial Distribution","text":"<p>Back to Top</p> <p>Applicable when analyzing binary outcomes (e.g., success/failure).</p> <pre><code>from scipy.stats import binom\n\nbinom.pmf(k=3, n=10, p=0.5)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_2","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\uddea A/B testing for conversion rates on two landing pages.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#bernoulli-distribution","title":"\ud83e\uddee Bernoulli Distribution","text":"<p>Back to Top</p> <p>The Bernoulli distribution models a random experiment with only two possible outcomes:</p> <ul> <li><code>1</code> for success (with probability <code>p</code>)</li> <li><code>0</code> for failure (with probability <code>1 - p</code>)</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-points","title":"\ud83d\udcd8 Key Points","text":"<p>Back to Top</p> <ul> <li>Only one trial is considered.</li> <li>Success and failure are non-judgmental: you can assign \"success\" to any outcome based on your scenario.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-expression","title":"\ud83e\udde0 Probability Expression","text":"<p>Back to Top</p> \\[ X = \\begin{cases} 1, &amp; \\text{with prob } p \\\\ 0, &amp; \\text{with prob } 1-p \\end{cases} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#common-use-cases","title":"\u2705 Common Use Cases","text":"<p>Back to Top</p> <ul> <li>\ud83c\udfed Manufacturing defective parts (success = defective or not)</li> <li>\ud83e\uddea Medical test outcomes (success = positive result)</li> </ul> <p>A Bernoulli distribution is a special case of the Binomial distribution where the number of trials = 1.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-distributions","title":"\ud83c\udfb2 Probability Distributions","text":"<p>Learn about random variables, types of distributions, and how to interpret discrete and continuous data using bar plots and formulas.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-random-variable","title":"\ud83d\udd22 1. Random Variable","text":"<p>Back to Top</p> <p>A random variable assigns a numerical value to each outcome of an experiment. It assumes different values with different probabilities.</p> <p>Example:</p> <p>Suppose there are 1,000 students in a university. What is the probability that 500 will pass an upcoming exam?</p> <ul> <li>Each student has a 50\u201350 chance of passing.</li> <li>The number of students who pass can range from 0 to 1000.</li> <li>The total outcomes form a distribution of values \u2192 this is a random variable.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-continuous-random-variable","title":"\ud83c\udf0a 2. Continuous Random Variable","text":"<p>Back to Top</p> <p>If a random variable can take any value within a range, it\u2019s continuous.</p> <p>Example:</p> <p>Volume of soda in a bottle: 498mL, 499.2129415mL, \u2026 Cannot list all possible values \u2014 infinite possibilities.</p> <p>\u2705 A continuous random variable deals with real numbers within intervals.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#3-discrete-random-variable","title":"\ud83d\udd22 3. Discrete Random Variable","text":"<p>Back to Top</p> <p>A discrete random variable has a countable set of values.</p> <p>Example:</p> <p>Insurance claims per month for a driver:</p> \\[ X =  \\begin{cases} 0, &amp; \\text{with prob } 0.95 \\\\ 1, &amp; \\text{with prob } 0.04 \\\\ 2, &amp; \\text{with prob } 0.008 \\\\ 3, &amp; \\text{with prob } 0.002 \\\\ \\end{cases} \\] <ul> <li>Values must be non-negative</li> <li>All probabilities must sum to 1</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#4-probability-distribution","title":"\ud83e\uddee 4. Probability Distribution","text":"<p>Back to Top</p> <p>Defines values a random variable can take along with their probabilities.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#discrete-probability-distribution","title":"\ud83d\udd39 Discrete Probability Distribution","text":"<p>Back to Top</p> <ul> <li>Arises from discrete random variables</li> <li>Has a Probability Mass Function (PMF)   Gives the probability that a variable takes a specific value</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#continuous-probability-distribution","title":"\ud83d\udd38 Continuous Probability Distribution","text":"<p>Back to Top</p> <ul> <li>Arises from continuous random variables</li> <li>Has a Probability Density Function (PDF)   Describes the likelihood a variable falls within a range</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#5-bar-plot-interpretation","title":"\ud83d\udcca 5. Bar Plot Interpretation","text":"<p>Back to Top</p> <p>Shows relative frequencies for discrete values.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_2","title":"\ud83e\udde0 Summary","text":"<p>Back to Top</p> <ul> <li>The sum of relative frequencies:   [   0.16 + 0.18 + 0.15 + 0.21 + 0.11 + 0.10 + 0.09 = 1.00 \u2705   ]</li> </ul> <p>\u2705 This confirms a valid probability distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#6-bernoulli-distribution","title":"\ud83c\udfaf 6. Bernoulli Distribution","text":"<p>Back to Top</p> <ul> <li>Only two outcomes: 1 (success) and 0 (failure)</li> <li>Single trial</li> </ul> \\[ X = \\begin{cases} 1, &amp; \\text{with prob } p \\\\ 0, &amp; \\text{with prob } 1 - p \\\\ \\end{cases} \\] <p>\ud83d\udca1 Used in scenarios like: - Defective manufacturing parts - Outcomes of medical tests</p> <p>\ud83d\udccc Note: Success/failure labels are not judgmental \u2014 it\u2019s a modeling convention.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#7-binomial-distribution","title":"\ud83d\udce6 7. Binomial Distribution","text":"<p>Back to Top</p> <p>Applies when you extend Bernoulli trials over multiple repetitions.</p> <p>Example Scenario:</p> <p>Survey 25 TikTok users to check if they\u2019ve posted a video (Yes/No).</p> <ul> <li>Each trial is Bernoulli.</li> <li>Total number of \u201cYes\u201d is modeled by Binomial Distribution.</li> </ul> \\[ \\text{Bernoulli is a special case of Binomial with a single trial} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-mass-function-pmf","title":"\ud83d\udcd0 Probability Mass Function (PMF):","text":"<p>Back to Top</p> \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n - x} \\] <ul> <li>\\(n\\): total trials  </li> <li>\\(x\\): number of successes  </li> <li>\\(p\\): success probability</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#quiz-insight","title":"\u2705 Quiz Insight","text":"<p>Back to Top</p> <p>A continuous probability distribution is represented by: \u2714\ufe0f Probability Density Function (PDF) \u274c Not by PMF (that\u2019s for discrete variables)</p> <p>### \ud83c\udfaf Binomial Distribution: Assumptions</p> <p>To model a scenario using a Binomial distribution, the following assumptions must be satisfied:</p> <ol> <li> <p>Fixed Number of Trials (n)    The number of experiments or trials is predetermined and remains constant.</p> </li> <li> <p>Independence    Each trial is independent of the others \u2014 the outcome of one trial does not influence the outcome of another.</p> </li> <li> <p>Binary Outcomes    Each trial results in only one of two possible outcomes: success or failure.</p> </li> <li> <p>Constant Probability    The probability of success (denoted as \\( p \\)) is the same for each trial.</p> </li> </ol> <p>These conditions ensure the binomial model is valid for computing probabilities using the binomial formula:</p> \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-happens-if-binomial-assumptions-are-violated","title":"\u26a0\ufe0f What Happens If Binomial Assumptions Are Violated?","text":"<p>Back to Top</p> <p>In a month of 30 days, what is the probability that it will rain on more than 10 days, if on average the chance of rain on a given day is 20%?</p> <p>To apply the binomial distribution, we assume:</p> <ol> <li> <p>Independence:    The event of rain on a particular day is independent of it raining on the previous day.</p> </li> <li> <p>Constant Probability:    The chance of rain does not increase or decrease over the duration of the month.</p> </li> </ol> <p>If these assumptions are satisfied, then we can model the situation using:</p> <ul> <li>\\( n = 30 \\) (number of days)  </li> <li>\\( p = 0.2 \\) (probability of rain on a given day)</li> </ul> <p>Using the binomial distribution, we can compute:</p> \\[ P(X &gt; 10) \\quad \\text{for } X \\sim \\text{Binomial}(n=30, p=0.2) \\] <p>\ud83e\udde0 Note: Although the assumptions are not strictly valid, they allow for a simplified calculation that is often good enough for practical estimation purposes.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#4-uniform-distribution","title":"4. Uniform Distribution","text":"<p>Back to Top</p> <p>All outcomes have equal probability.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_3","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83c\udfb2 Simulating random dice rolls or fair lottery draws.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#scenario","title":"Scenario","text":"<p>Back to Top</p> <p>Suppose we roll a die. The possible outcomes of this event are:</p> <pre><code>1, 2, 3, 4, 5, 6\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-characteristics","title":"Key Characteristics","text":"<p>Back to Top</p> <ul> <li>\u2705 All outcomes have an equal probability of occurrence.</li> <li>\ud83d\udd01 Outcomes are mutually exclusive (no two can happen at once).</li> <li>\ud83d\udcca The probability of each outcome is the same:</li> </ul> <p>[   P(x) = \\frac{1}{n} = \\frac{1}{6} \\text{ for a fair 6-sided die}   ]</p> <ul> <li>This type of distribution is known as a Uniform Distribution.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#when-to-use","title":"When to Use","text":"<p>Back to Top</p> <p>Uniform distribution is:</p> <ul> <li>Useful when we want unbiased selection.</li> <li>Common in random sampling, game simulations, and initial probability models.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_3","title":"Summary","text":"<p>Back to Top</p> <p>A Uniform Distribution assigns equal probability to all possible outcomes. It is one of the simplest forms of probability distribution and serves as the foundation for modeling fair and random events.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#uniform-distribution","title":"Uniform Distribution","text":"<p>Suppose we roll a die. The outcomes of this event can be 1, 2, 3, 4, 5, 6.</p> <ul> <li>All of the outcomes have an equal probability of occurrence and are mutually exclusive.</li> <li>We can say that the probabilities of occurrence are uniformly distributed.</li> <li>This is referred to as Uniform Distribution.</li> <li>\u2705 Useful when we are interested in unbiased selection.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#types-of-uniform-distribution","title":"Types of Uniform Distribution","text":"<p>Back to Top</p> <p>There are two types of Uniform Distribution:</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-discrete-uniform-distribution","title":"1. Discrete Uniform Distribution","text":"<p>Back to Top</p> <ul> <li>Takes a finite number (m) of values.</li> <li>Each value has equal probability of being selected.</li> </ul> <p>For example: Number of books sold by a bookseller per day can be uniformly distributed between 100 to 300.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-continuous-uniform-distribution","title":"2. Continuous Uniform Distribution","text":"<p>Back to Top</p> <ul> <li>Can take any value between a specified range.</li> </ul> <p>For example: Tomorrow\u2019s temperature in the United States can be uniformly distributed between 12\u00b0C to 17\u00b0C.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#5-normal-distribution","title":"5. Normal Distribution","text":"<p>Back to Top</p> <p>A bell-shaped distribution used across disciplines.</p> <pre><code>from scipy.stats import norm\n\nx = np.linspace(-3, 3, 100)\npdf = norm.pdf(x)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_4","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udca1 Height distribution of people in a city, or standardized testing scores.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#normal-distribution-why-normal","title":"\ud83d\udcca Normal Distribution: Why Normal","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-is-it-called-the-normal-distribution","title":"\u2753 Why is it called the normal distribution?","text":"<p>Back to Top</p> <p>\ud83d\udd39 They are commonly found everywhere \u2014 starting from nature to industry.</p> <p>\ud83d\udd39 Many useful datasets are approximately normally distributed.</p> <p>\ud83d\udd39 Examples include: - Height and weight of adults - IQ scores - Measurement errors - Quality control test results  </p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#normal-distribution-properties","title":"Normal Distribution: Properties","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-properties-of-the-normal-distribution","title":"\ud83d\udcc8 Key Properties of the Normal Distribution","text":"<p>Back to Top</p> <ol> <li>The graph of the normal distribution is called the normal curve.</li> <li>The normal curve is symmetric around the mean.</li> <li>Mean, Median, and Mode of the normal distribution are equal.</li> <li>The total area under the normal curve is 1.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#empirical-rule-68-95-997-rule","title":"\ud83c\udfaf Empirical Rule (68-95-99.7 Rule)","text":"<p>Back to Top</p> <ol> <li>68% of the data falls within 1 standard deviation (\u03c3) from the mean (\u03bc).</li> <li>95% of the data falls within 2 standard deviations from the mean.</li> <li>99.7% of the data falls within 3 standard deviations from the mean.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#visual-representation","title":"\ud83d\udcca Visual Representation","text":"<p>Back to Top</p> <p></p> <p>The image illustrates the bell-shaped curve, highlighting symmetric intervals around the mean and the percentage of data captured within 1\u03c3, 2\u03c3, and 3\u03c3.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_4","title":"\u2705 Summary","text":"<p>Back to Top</p> <p>The normal distribution is foundational in statistics and machine learning, allowing for standardized assumptions about data spread and probability within intervals of standard deviation.</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#6-z-score","title":"6. Z-Score","text":"<p>Back to Top</p> <p>Measures how many standard deviations a data point is from the mean.</p> <pre><code>z = (x - np.mean(x)) / np.std(x)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_5","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udea8 Outlier detection in performance metrics.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#7-sampling-inference-foundations","title":"7. Sampling &amp; Inference Foundations","text":"<p>Back to Top</p> <p>Understanding population vs. sample, and designing sampling techniques.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_6","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\uddec Pharmaceutical companies conduct clinical trials on samples before full rollout.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#revisiting-the-need-for-sampling","title":"Revisiting the Need for Sampling","text":"<p>In many situations, what we have available to us is a sample of data.</p> <p>\ud83d\udd39 The data we have is finite. \ud83d\udd39 Till now, the goal was to find ways of describing, summarizing, and visualizing the sample data only. \ud83d\udd39 Moving ahead, we want to make inferences about the entire population using the sample data.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#simple-random-sampling","title":"\ud83c\udfaf Simple Random Sampling","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-is-simple-random-sampling","title":"What is Simple Random Sampling?","text":"<p>Back to Top</p> <p>A sampling technique where every item in the population has an equal chance of being selected.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-are-simple-random-samples-important","title":"\ud83d\udca1 Why are simple random samples important?","text":"<p>Back to Top</p> <p>Allows all the entities in the population to have an equal chance of being selected, and so the sample is likely to be representative of the population.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-points_1","title":"\ud83d\udcdd Key Points","text":"<p>Back to Top</p> <ul> <li>Every individual in the population has an equal probability of being chosen.</li> <li>Unbiased method if implemented correctly.</li> <li>Often implemented using random number generators or lottery methods.</li> </ul> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#8-central-limit-theorem","title":"8. Central Limit Theorem","text":"<p>Back to Top</p> <p>Describes how the sampling distribution of the sample mean approaches a normal distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_7","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcc9 Enables approximation of sampling behavior for metrics like average wait times.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>The sampling distribution of the sample means will approach normal distribution as the sample size gets bigger, no matter what the shape of the population distribution is.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#assumptions","title":"Assumptions","text":"<p>Back to Top</p> <ul> <li>Data must be randomly sampled</li> <li>Sample values must be independent of each other</li> <li>Samples should come from the same distribution</li> <li>Sample size must be sufficiently large (\u2265 30)</li> </ul> <p>\ud83d\udccc The Central Limit Theorem (CLT) is foundational for inferential statistics and allows us to use normal distribution techniques even when the population is not normally distributed, provided the sample size is large enough.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-is-it","title":"\ud83d\udcd8 What Is It?","text":"<p>Back to Top</p> <p>The Central Limit Theorem (CLT) states that:</p> <p>When we take many random samples from any population (regardless of its distribution), the sampling distribution of the sample means will approach a normal distribution as the sample size increases (typically \\( n \\geq 30 \\)).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#practical-example-pizza-delivery-times","title":"\ud83c\udf55 Practical Example: Pizza Delivery Times","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#scenario_1","title":"\ud83d\udce6 Scenario","text":"<p>Back to Top</p> <p>You're managing a pizza delivery service and want to understand the average delivery time. The population of delivery times is not normally distributed (e.g., skewed due to traffic, weather, etc.).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#step-by-step","title":"\ud83d\udd22 Step-by-Step","text":"<p>Back to Top</p> <ol> <li>Random Sampling:</li> <li>You take 100 samples.</li> <li> <p>Each sample contains 30 delivery times (randomly selected).</p> </li> <li> <p>Calculate Means:</p> </li> <li>Compute the mean delivery time for each sample.</li> <li> <p>You now have 100 sample means.</p> </li> <li> <p>Plot the Distribution:</p> </li> <li>Even though the original data is skewed,</li> <li> <p>The distribution of sample means will look approximately normal.</p> </li> <li> <p>Use Normal-Based Statistics:</p> </li> <li>You can apply z-scores, confidence intervals, and hypothesis testing \u2014 because the sample means follow a normal distribution.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-it-matters","title":"\ud83e\udde0 Why It Matters","text":"<p>Back to Top</p> <ul> <li>\u2705 Works with both discrete and continuous populations</li> <li>\u2705 Helps apply normal distribution tools on non-normal data</li> <li>\u2705 Critical for inferential statistics</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-requirements","title":"\ud83c\udfaf Key Requirements","text":"<p>Back to Top</p> <ul> <li>Data must be randomly sampled</li> <li>Samples should be independent</li> <li>Data must come from the same distribution</li> <li>Sample size should be sufficiently large (typically \u2265 30)</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#9-estimation","title":"9. Estimation","text":"<p>Back to Top</p> <p>Estimate population parameters like mean or proportion using sample statistics.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_8","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcca Estimating average customer spend in a supermarket from sample receipt data.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#estimation","title":"Estimation","text":"<p>Estimation is the process of making inference about a population parameter based on a sample statistic.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#point-estimation","title":"\ud83d\udd39 Point Estimation","text":"<p>Back to Top</p> <ul> <li>Provides a single value (point) as an estimate of the population parameter.</li> <li>This estimate is derived directly from the sample data.</li> <li>Example:   The population mean is estimated from the sample mean: Estimated population mean = $40</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#interval-estimation","title":"\ud83d\udd39 Interval Estimation","text":"<p>Back to Top</p> <ul> <li>Provides a range of values within which the population parameter is expected to lie.</li> <li>This range is associated with a confidence level (x%).</li> <li>Example:   The population mean is expected to lie between $38 and $42, with 95% confidence   (i.e., x = 95). </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#confidence-interval-for-mean","title":"Confidence Interval for Mean \u03bc","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#interpretation-of-95-confidence-interval","title":"\ud83c\udfaf Interpretation of 95% Confidence Interval","text":"<p>Back to Top</p> <ul> <li>The interpretation of a 95% confidence interval is that, if the process is repeated a large number of times, then the intervals so constructed will contain the true population parameter 95% of the time.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-not-100-confidence-interval","title":"\u2753 Why not 100% Confidence Interval?","text":"<p>Back to Top</p> <ul> <li>A 100% confidence interval will include all possible values.</li> <li>Hence, there will be no insight into the problem.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#-_1","title":"---","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#10-hypothesis-testing","title":"10. Hypothesis Testing","text":"<p>Back to Top</p> <p>Formal process for testing claims using sample data.</p> <p>\ud83d\udc49 Open in Colab </p> <pre><code># Example: t-test\nfrom scipy.stats import ttest_1samp\n\nttest_1samp(sample, popmean=85)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_9","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\udde0 Determine if a new teaching method significantly improves test scores.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>Back to Top</p> <p>\ud83d\udc49 Download sat_score.csv \ud83d\udcce View sat_score.csv</p> <p>\ud83d\udc49 Download debugging.csv \ud83d\udcce View debugging.csv</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<p>Back to Top</p> <ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#pdf-references","title":"\ud83d\udcc4 PDF References","text":"<p>Back to Top</p> <p>\ud83d\udcd8 Inferential Statistics \u2013 Lecture PDF \ud83d\udcd8 Hypothesis Testing \u2013 Lecture PDF</p> <p>These PDFs are stored locally in your project under: - <code>/docs/pdfs/Lecture Slides -  Inferential Statistics.pdf</code> - <code>/docs/pdfs/Lecture Slides - Hypothesis Testing.pdf</code></p> <p>Ensure the paths align with your MkDocs file structure and navigation.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/","title":"Agenda \u2013 Hypothesis Testing \u2013 Week 2","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Hypothesis Testing</li> <li>2. Basic Concepts of Hypothesis Testing</li> <li>3. Performing a Hypothesis Test</li> <li>4. One-Tailed and Two-Tailed Tests</li> <li>5. Confidence Interval and Hypothesis Test</li> <li>6. Some Important Tests<ul> <li>a. Test for One Mean</li> <li>b. Test for Equality of Means</li> <li>c. Test for Equality of Means \u2013 Equal Std Dev</li> <li>d. Test for Equality of Means \u2013 Unequal Std Dev</li> <li>e. Paired Test for Equality of Means</li> <li>f. Test for One Proportion</li> <li>g. Test for Two Proportions</li> <li>h. Test for One Variance</li> <li>i. Test for Equality of Variances</li> <li>j. Test of Independence</li> <li>k. ANOVA Test</li> </ul> </li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#1-hypothesis-testing","title":"1. Hypothesis Testing","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-introduction","title":"a. Introduction","text":"<ul> <li>Definition: Hypothesis testing is a statistical method to make decisions using data, often about population parameters.</li> <li>Purpose: Helps determine if there is enough evidence to support a specific claim about a population.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#what-is-hypothesis","title":"What is Hypothesis?","text":"<ul> <li>Often, we are interested in population parameter(s) (such as the mean, proportion, or variance for an entire group).</li> </ul> <p>A hypothesis is a conjecture about the population parameter(s).</p> <p>For example: - A bulb manufacturing company wants to know whether a new manufacturing process improves the reliability of the bulbs.   This leads to a hypothesis about the average bulb lifespan or failure rate under the new process.</p> <p> </p> <p>Objective of Hypothesis Testing: - To set a value (or claim) for the population parameter(s), and - To perform a statistical test to see whether that value is supported (\u201ctenable\u201d) by the evidence gathered from a sample.</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-hypothesis-formulation","title":"b. Hypothesis Formulation","text":"<ul> <li>Formulating Hypotheses:  </li> <li>Null Hypothesis (H\u2080): The default position or status quo (e.g., \u201cno effect,\u201d \u201cno difference\u201d).</li> <li>Alternative Hypothesis (H\u2081 or Ha): What you seek to prove (e.g., \u201cthere is an effect,\u201d \u201cthere is a difference\u201d).</li> <li>Steps:</li> <li>Clearly define the research question.</li> <li>Translate into statistical hypotheses (H\u2080 &amp; H\u2081).</li> <li>Select significance level (\u03b1, e.g., 0.05).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example","title":"Use Case Example","text":"<p>A pharmaceutical company tests if a new drug lowers blood pressure more than the current standard. - H\u2080: New drug is no better than the standard. - H\u2081: New drug lowers blood pressure more than the standard.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#why-hypothesis","title":"Why Hypothesis?","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#estimation","title":"Estimation","text":"<p>The problem of estimation is considered when there is no previous knowledge of the population parameter. The problem is simpler in this case: - A random sample is taken, - A sample statistic is computed, - An appropriate point and interval estimate is suggested.</p> <p>Estimation helps us calculate values for unknown parameters, but does not test any assumptions about those parameters.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Often, the interest is not in the numerical value of the point estimate of the parameter, but in knowing the plausibility of a hypothesis about the population parameter by using sample data. - Estimation alone is not enough to arrive at a conclusion in such cases.</p> <p>Hypothesis testing allows us to assess whether the data supports a specific claim about a population parameter, beyond just estimating its value.</p> <p>Back to Top Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#2-basic-concepts-of-hypothesis-testing","title":"2. Basic Concepts of Hypothesis Testing","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-importance-of-null","title":"a. Importance of Null","text":"<ul> <li>The null hypothesis provides a baseline or default assumption.</li> <li>All evidence is measured against H\u2080.</li> <li>Without H\u2080, statistical significance cannot be determined.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-importance-of-test-statistic","title":"b. Importance of Test Statistic","text":"<ul> <li>The test statistic (e.g., t, z, \u03c7\u00b2) is a calculated value that helps determine whether to reject H\u2080.</li> <li>It quantifies how far sample data deviates from what H\u2080 predicts.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-type-i-and-type-ii-errors","title":"c. Type I and Type II Errors","text":"<ul> <li>Type I Error (\u03b1): Incorrectly rejecting H\u2080 (false positive).</li> <li>Type II Error (\u03b2): Failing to reject H\u2080 when it is false (false negative).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-table-example","title":"Error Table Example","text":"H\u2080 True H\u2080 False Reject H\u2080 Type I Correct Fail to Reject Correct Type II"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#type-i-and-type-ii-errors-real-world-examples","title":"Type I and Type II Errors: Real-World Examples","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-1-supermarket-waiting-times","title":"Example 1: Supermarket Waiting Times","text":"<p>Scenario: The store manager believes that the average waiting time for customers at checkouts has become worse than 15 minutes.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypotheses","title":"Hypotheses","text":"Symbol Statement H\u2080 The average waiting time at checkouts is \u2264 15 min. H\u2090 The average waiting time at checkouts is &gt; 15 min."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-types-in-supermarket-example","title":"Error Types in Supermarket Example","text":"Error Type What It Means Example Statement Type I Error False Positive Waiting time is \u2264 15 min, but manager concludes it is &gt; 15 min. Type II Error False Negative Waiting time is &gt; 15 min, but manager concludes it is \u2264 15 min. <ul> <li> <p>Type I Error: Rejecting the null hypothesis when it is actually true. E.g., The waiting time is really \u2264 15 min, but the manager says it\u2019s &gt; 15 min.</p> </li> <li> <p>Type II Error: Failing to reject the null hypothesis when it is actually false. E.g., The waiting time is really &gt; 15 min, but the manager says it\u2019s \u2264 15 min.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-2-cancer-diagnosis","title":"Example 2: Cancer Diagnosis","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypotheses_1","title":"Hypotheses","text":"Symbol Statement H\u2080 The patient does not have cancer. H\u2090 The patient has cancer."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-types-in-medical-example","title":"Error Types in Medical Example","text":"Error Type What It Means Example Statement Type I Error False Positive Patient doesn't have cancer, but the doctor says she does. Type II Error False Negative Patient does have cancer, but the report says she doesn't. <ul> <li> <p>Type I Error: Diagnosing cancer when the patient actually does not have cancer. (False positive)</p> </li> <li> <p>Type II Error: Failing to diagnose cancer when the patient actually does have cancer. (False negative)</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#side-by-side-summary-table","title":"Side-by-Side Summary Table","text":"Error Type Supermarket Example Medical Example Type I Error Manager wrongly thinks waiting &gt; 15 min Doctor wrongly says patient has cancer Type II Error Manager wrongly thinks waiting \u2264 15 min Doctor/report wrongly says patient has no cancer <p>Summary: - Type I Error: False alarm (detecting an effect that isn't there) - Type II Error: Missed detection (failing to detect an effect that is there)</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-hypothesis-testing-template","title":"d. Hypothesis Testing Template","text":"<p>Standard Steps:</p> <ol> <li>State H\u2080 and H\u2081.</li> <li>Choose significance level (\u03b1).</li> <li>Select appropriate test statistic.</li> <li>Determine critical region or compute p-value.</li> <li>Make a statistical decision.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-template","title":"Hypothesis Testing Template","text":"Step Action Description 1 Identify the key question What is the research question that you are trying to answer? 2 Establish the hypotheses What is the metric of interest? Define the Null and Alternate Hypothesis. 3 Understand and prepare data What data do you have? Do you understand what it means? Can it be used directly? 4 Identify the right test Choose the method for testing based on the previous steps. 5 Check the assumptions Ensure that data satisfies the assumption for the test. 6 Perform the test Get to conclusion based on the results (p-value)."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#summary-checklist","title":"Summary Checklist","text":"<ol> <li> <p>Identify the key question:    What are you trying to answer?</p> </li> <li> <p>Establish the hypotheses:    Define the metric and state H\u2080 and H\u2090.</p> </li> <li> <p>Understand and prepare data:    Ensure you have the right data and understand its meaning.</p> </li> <li> <p>Identify the right test:    Select the appropriate statistical test.</p> </li> <li> <p>Check the assumptions:    Make sure assumptions of the test are met.</p> </li> <li> <p>Perform the test:    Draw a conclusion from your results (such as the p-value).</p> </li> </ol> <p>Back to Top</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_1","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_1","title":"Use Case Example","text":"<p>A factory tests if the mean diameter of produced bolts is 10 mm. - H\u2080: Mean = 10 mm - Use t-statistic to measure sample deviation from 10 mm.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#3-performing-a-hypothesis-test","title":"3. Performing a Hypothesis Test","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-some-key-ideas","title":"a. Some Key Ideas","text":"<ul> <li>Define your question &amp; hypotheses.</li> <li>Choose an appropriate statistical test for your data.</li> <li>Understand the implications of test results.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-assumptions","title":"b. Assumptions","text":"<ul> <li>Every statistical test has underlying assumptions (normality, equal variances, random sampling).</li> <li>If assumptions are violated, results may not be valid.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-critical-point","title":"c. Critical Point","text":"<ul> <li>The value that defines the threshold for rejecting H\u2080.</li> <li>E.g., for \u03b1 = 0.05 in a z-test, critical values are \u00b11.96.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-rejection-region-approach","title":"d. Rejection Region Approach","text":"<ul> <li>Identify areas under the probability curve (tails) where H\u2080 is rejected.</li> <li>If test statistic falls in this region, reject H\u2080.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#e-p-value-approach","title":"e. p-value Approach","text":"<ul> <li>The p-value is the probability of obtaining test results at least as extreme as the observed, assuming H\u2080 is true.</li> <li>If p &lt; \u03b1, reject H\u2080.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_2","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_2","title":"Use Case Example","text":"<p>A marketing analyst tests if a new ad campaign changes sales figures. - Assumptions: data are independent, normal. - The critical region and p-value determine the outcome.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#summary","title":"Summary","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#key-concepts-in-hypothesis-testing","title":"Key Concepts in Hypothesis Testing","text":"Concept Explanation Level of Significance (\u03b1) - Probability of rejecting the null hypothesis when it is true.- Fixed before the hypothesis test. p-value - Probability of observing a test statistic (or more extreme) under the null hypothesis.- Depends on sample data. Alpha (\u03b1) is pre-fixed but p-value depends on the value of the test statistic. Acceptance or Rejection Region - The area under the distribution curve is partitioned into acceptance and rejection regions.- Reject the null hypothesis if the test statistic falls in the rejection region; otherwise, fail to reject it."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#lets-start-simple","title":"Let's Start Simple","text":"<p>Consider the following questions in hypothesis testing:</p> Question Question What are the null and alternative hypotheses? What is an appropriate test statistic? What is preset level of significance? How to check whether the data is giving significant evidence against the null hypothesis or not? <p>Let's see an example and understand the significance of the above questions.</p> <p>For simplicity, we will assume that the population standard deviation is known and the sample size is more than 30.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example","title":"Example","text":"<p>It is known from experience that for a certain E-commerce company, the mean delivery time of the products is 5 days with a standard deviation of 1.3 days.</p> <p>The new customer service manager of the company is afraid that the company is slipping and collects a random sample of 45 orders. The mean delivery time of these samples comes out to be 5.25 days.</p> <p>Is there enough statistical evidence for the manager\u2019s apprehension that the mean delivery time of products is greater than 5 days?</p> <p>Note: This is clearly a one-tailed test, concerning population mean \u03bc\u2014the mean delivery time of products.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#first-test-z-test-for-one-mean","title":"First test - Z-test for One Mean","text":"Significance of the test Assumptions Test Statistic Distribution Test for population mean   \\(H_0: \\mu = \\mu_0\\) - Continuous data  - Normally distributed population or sample size \\(&gt; 30\\)  - Known population standard deviation \\(\\sigma\\)  - Random sampling from the population Standard Normal distribution"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_3","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#4-one-tailed-and-two-tailed-tests","title":"4. One-Tailed and Two-Tailed Tests","text":"<ul> <li>One-Tailed Test: Tests for deviation in one direction only (e.g., \u201cgreater than\u201d or \u201cless than\u201d).</li> <li>Two-Tailed Test: Tests for deviation in both directions (e.g., \u201cnot equal to\u201d).</li> </ul> <p>When to use: - Use one-tailed if you only care about increase/decrease. - Use two-tailed if you care about any change.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_4","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_3","title":"Use Case Example","text":"<p>A teacher tests if a new teaching method improves scores (one-tailed), or if it changes scores in either direction (two-tailed).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#alternative-hypothesis-types","title":"Alternative Hypothesis Types","text":"Test Type Description Mathematical Form One-tailed test Greater than type \\(H_a: \\mu &gt; \\mu_0\\) Less than type \\(H_a: \\mu &lt; \\mu_0\\) Two-tailed test Not equal type \\(H_a: \\mu \\neq \\mu_0\\)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#flow-summary","title":"Flow Summary","text":"<ul> <li>Alternative Hypothesis<ul> <li>One-tailed test<ul> <li>Greater than type:\u2003\\(H_a: \\mu &gt; \\mu_0\\)</li> <li>Less than type:\u2003\\(H_a: \\mu &lt; \\mu_0\\)</li> </ul> </li> <li>Two-tailed test<ul> <li>Not equal type:\u2003\\(H_a: \\mu \\neq \\mu_0\\)</li> </ul> </li> </ul> </li> </ul> <p>Tip: - Use one-tailed tests for directional hypotheses (\"greater than\" or \"less than\"). - Use a two-tailed test when testing for any difference (not direction).</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#5-confidence-interval-and-hypothesis-test","title":"5. Confidence Interval and Hypothesis Test","text":"<ul> <li>Confidence Interval (CI): A range within which a population parameter is expected to fall, with a certain confidence level (e.g., 95%).</li> <li>Relationship to Hypothesis Testing: </li> <li>If a CI for mean difference excludes 0, H\u2080 (\u201cno difference\u201d) is rejected at the equivalent significance level.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#confidence-interval-vs-hypothesis-testing","title":"Confidence Interval vs Hypothesis Testing","text":"<p>Suppose we calculate the \\((100 - 5)\\%\\) confidence interval for the mean.</p> <p>We also conduct the Z-test for the mean with a 5% significance level.</p> <p>The hypotheses of the Z-test are:</p> \\[ H_0 : \\mu = \\mu_0 \\quad \\text{against} \\quad H_a : \\mu \\neq \\mu_0 \\]"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#is-there-any-relationship-between-the-estimated-confidence-interval-and-the-hypothesis-test","title":"Is there any relationship between the estimated confidence interval and the hypothesis test?","text":"<p>The confidence interval contains all values of \\(\\mu_0\\) for which the null hypothesis will not be rejected.</p> <p>Key Points: - If the hypothesized value (\\(\\mu_0\\)) falls within the confidence interval, we do not reject the null hypothesis at the given significance level. - If \\(\\mu_0\\) is outside the confidence interval, we reject the null hypothesis.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_5","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_4","title":"Use Case Example","text":"<p>A clinical study reports the CI for treatment effect does not include zero, supporting the alternative hypothesis.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#6-some-important-tests","title":"6. Some Important Tests","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-frameworks","title":"Hypothesis Testing Frameworks","text":"<p>Choice of test depends on test statistic and data availability</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-test-for-one-mean","title":"a. Test for One Mean","text":"<p>What: Tests if the mean of a sample differs from a known or hypothesized population mean (one-sample t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc = \u03bc\u2080 (The sample mean equals the population mean) - H\u2081: \u03bc \u2260 \u03bc\u2080 (The sample mean does not equal the population mean)</p> <p>Sample Use Case: A coffee chain wants to check if the average amount of coffee in its \u201c12oz\u201d cup differs from 12 ounces, based on a random sample of cups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-example-zyx-food-delivery-claim","title":"\ud83d\udcca Hypothesis Testing Example: ZYX Food Delivery Claim","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#scenario","title":"\ud83d\udcdd Scenario","text":"<p>A certain food aggregator ZYX is facing stiff competition from its main rival SWG during the Corona period. To retain business, ZYX is advertising that:</p> <p>Within a radius of 5 km from the restaurant where the order is placed, ZYX can still deliver in 40 minutes or less on average, regardless of changed conditions.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#data-collection","title":"\ud83d\udce6 Data Collection","text":"<ul> <li>Delivery times in minutes of 25 randomly selected deliveries are provided in a CSV file.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#problem-statement","title":"\u2753 Problem Statement","text":"<p>Assuming the delivery time distribution is approximately normal, is there enough statistical evidence to reject ZYX\u2019s claim?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-framing","title":"\ud83d\udd0d Hypothesis Framing","text":"<p>This is a one-tailed hypothesis test concerning the population mean \\( \\mu \\) \u2014 the average delivery time.</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): \\( \\mu \\leq 40 \\) </li> <li>Alternative Hypothesis (\\( H_a \\)): \\( \\mu &gt; 40 \\) <p>Claiming that ZYX cannot deliver in 40 minutes or less, on average.</p> </li> </ul> <p>## \ud83e\uddea Test Overview Table</p> Significance of the Test Assumptions Test Statistic Distribution Test for population mean \\(H_0: \\mu = \\mu_0\\) - Continuous data   - Normally distributed population and sample size &lt; 30   - Unknown population standard deviation  - Random sampling from the population t distribution   (The test is also known as One-sample t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#goal","title":"\ud83c\udfaf Goal","text":"<p>Determine whether ZYX's claim is statistically valid using hypothesis testing methods (e.g., one-sample t-test), based on the sample of 25 deliveries.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_6","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-test-for-equality-of-means","title":"b. Test for Equality of Means","text":"<p>What: Tests if the means from two independent samples are equal (independent two-sample t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: A medical researcher wants to know if two different blood pressure drugs have different average effects.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example_1","title":"Example","text":"<p>To compare customer satisfaction levels of two competing media channels, 150 customers of Channel 1 and 300 customers of Channel 2 were randomly selected and were asked to rate their channels on a scale of 1\u20135, with 1 being least satisfied and 5 most satisfied. (The survey results are summarized in a CSV file.)</p> <p>Test at 0.05 level of significance whether the data provide sufficient evidence to conclude that Channel 1 has a higher mean satisfaction rating than Channel 2.</p> <p>\ud83d\udca1 This is a two-sample problem where Channel 1 and Channel 2 populations are independent. Further, this is a one-tailed hypothesis problem, concerning population means \\(\\mu_1\\) and \\(\\mu_2\\), the mean customer satisfaction for Channel 1 and Channel 2 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-known-std-devs","title":"Test for Equality of Means \u2013 Known Std Devs","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   \\(H_0 : \\mu_1 = \\mu_2\\) - Continuous data  - Normally distributed population or sample size &gt; 30  - Independent populations  - Known population standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\)  - Random sampling from the population Standard Normal distribution  (The test is also known as  Two independent sample z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_7","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-test-for-equality-of-means-equal-std-dev","title":"c. Test for Equality of Means \u2013 Equal Std Dev","text":"<p>What: Special case of two-sample t-test where population standard deviations are assumed equal.</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082, \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: Comparing average test scores between two classes, assuming the spread of scores (variance) is the same for both groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-comparing-daily-media-usage","title":"Example: Comparing Daily Media Usage","text":"<p>In the lockdown period, because of working from home and increased screen time, many opted for listening to FM Radio for entertainment rather than watching Cable TV. An advertisement agency randomly collected daily usage time data (in minutes) from both types of users and stored it in a CSV file.</p> <p>Question: Assuming daily Radio and TV usage time are normally distributed, do we have enough evidence to conclude that there is any difference between daily TV and Radio usage time at a 0.05 significance level?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#statistical-framing","title":"Statistical Framing:","text":"<ul> <li>Type of Test: Two-sample independent test (comparing means)</li> <li>Null Hypothesis (\\(H_0\\)): \\(\\mu_{\\text{TV}} = \\mu_{\\text{Radio}}\\) (No difference in mean usage time)</li> <li>Alternative Hypothesis (\\(H_a\\)): \\(\\mu_{\\text{TV}} \\ne \\mu_{\\text{Radio}}\\) (There is a significant difference in mean usage time)</li> <li>Significance Level: \\(\\alpha = 0.05\\)</li> <li>Assumptions:</li> <li>Normal distribution of usage times</li> <li>Independent samples</li> <li>Continuous data</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-equal-standard-deviations","title":"Test for Equality of Means: Equal Standard Deviations","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   \\(H_0\\): \\(\\mu_1 = \\mu_2\\) - Continuous data   - Normally distributed populations   - Independent populations   - Equal population standard deviations   - Random sampling from the population t distribution  (The test is also known as Two independent sample t-test ) #### \ud83e\uddea Open in Colab: \ud83d\udc49 Notebook_Hypothesis_Testing.ipynb"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-test-for-equality-of-means-unequal-std-dev","title":"d. Test for Equality of Means \u2013 Unequal Std Dev","text":"<p>What: Two-sample t-test without assuming equal variances (Welch\u2019s t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: A tech company compares the mean time to resolve tickets for two support teams with different levels of experience (variances likely differ).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example_2","title":"Example","text":"<p>SAT verbal scores of two groups of students are given in a CSV file. The first group, College, contains scores of students whose parents have at least a bachelor\u2019s degree, and the second group, High School, contains scores of students whose parents do not have any college degree.</p> <p>The Education Department is interested to know whether the sample data support the theory that students show a higher population mean verbal score on SAT if their parents attain a higher level of education.</p> <p>Assuming SAT verbal scores for two populations are normally distributed, do we have enough statistical evidence for this at a 5% significance level?</p> <p>This is a two-sample problem as the College and High School populations are different. Further, this is a one-tailed hypothesis problem, concerning population means \u03bc\u2081 and \u03bc\u2082, the mean verbal score on SAT for College and High School groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-unequal-std-devs","title":"Test for Equality of Means: Unequal Std Devs","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population meansH\u2080: \u03bc\u2081 = \u03bc\u2082 - Continuous data  - Normally distributed populations  - Independent populations  - Unequal population standard deviations  - Random sampling from the population t distribution(The test is also known as Two independent sample t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_8","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#e-paired-test-for-equality-of-means","title":"e. Paired Test for Equality of Means","text":"<p>What: Compares means from the same group at different times (paired t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc_before = \u03bc_after - H\u2081: \u03bc_before \u2260 \u03bc_after</p> <p>Sample Use Case: A gym measures client weights before and after a 12-week program to see if the mean weight has changed.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-paired-test-for-equality-of-means","title":"Example: Paired Test for Equality of Means","text":"<p>Typical prices of single-family homes in Florida are given for a sample of 15 metropolitan areas (in 1000 USD) for 2002 and 2003 in a CSV file.</p> <p>Assuming the house prices are normally distributed, do we have enough statistical evidence to say that there is an increase in the house price in one year at a 0.05 significance level?</p> <p>This is a paired sample problem as the two observations (for 2002 and 2003) are taken on one sampled unit (a metropolitan area). Further, this is a one-tailed hypothesis problem, concerning population means \u03bc\u2081 and \u03bc\u2082, the mean house price in 2002 and 2003 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#paired-test-for-equality-of-means","title":"Paired Test for Equality of Means","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   H\u2080: \u03bc\u2081 = \u03bc\u2082 - Continuous data   - Normally distributed populations  - Independent observations  - Random sampling from the population t distribution  (The test is also known as Paired t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_9","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#f-test-for-one-proportion","title":"f. Test for One Proportion","text":"<p>What: Tests if a sample proportion equals a hypothesized value (one-sample z-test for proportions).</p> <p>Example Hypotheses: - H\u2080: p = p\u2080 - H\u2081: p \u2260 p\u2080</p> <p>Sample Use Case: A poll finds that 58% of voters favor a candidate. Is this significantly different from a hypothesized 50%?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-one-proportion","title":"Example: Test for One Proportion","text":"<p>A researcher claims that Democratic party will win in the next United States Presidential election.</p> <p>To test her belief the researcher randomly surveyed 90 people and 24 out of them said that they voted for Democratic party.</p> <p>Is there enough evidence at \\(\\alpha = 0.05\\) to support this claim?</p> <p>This is clearly a one-tailed test, concerning population proportion p, the proportion of people voted from Democratic party.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-one-proportion","title":"Test for One Proportion","text":"Significance of the Test Assumptions Test Statistic Distribution Test for population proportionH\u2080: p = p\u2080 - Binomially distributed population  - Random sampling from the population  - When both mean (np) and n(1-p) are greater than or equal to 10, the binomial distribution can be approximated by a normal distribution Standard Normal distribution  (The test is also known as One proportion z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_10","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#g-test-for-two-proportions","title":"g. Test for Two Proportions","text":"<p>What: Tests if two sample proportions are equal.</p> <p>Example Hypotheses: - H\u2080: p\u2081 = p\u2082 - H\u2081: p\u2081 \u2260 p\u2082</p> <p>Sample Use Case: A vaccine trial compares the proportion of people who became ill in vaccine and placebo groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-two-proportions","title":"Example: Test for Two Proportions","text":"<p>A car manufacturer aims to improve its products\u2019 quality by reducing the defects. So, the manufacturer randomly checks the efficiency of two assembly lines in the shop floor. In line 1, there are 20 defects out of 200 samples and in line 2, there are 25 defects out of 400 samples.</p> <p>At 5% level of significance, do we have enough statistical evidence to conclude that the two assembly procedures are different?</p> <p>This is clearly a two-tailed test, concerning two population proportion p\u2081 and p\u2082, the proportion of defects in assembly line 1 and assembly line 2 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-two-proportions","title":"Test for Two Proportions","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population proportionsH\u2080: p\u2081 = p\u2082 - Binomially distributed populations  - Independent populations  - Random sampling from the populations  - When both mean (np) and n(1-p) are greater than or equal to 10, the binomial distribution can be approximated by a normal distribution Standard Normal distribution  (The test is also known as Two proportions z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_11","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#h-test-for-one-variance","title":"h. Test for One Variance","text":"<p>What: Tests if a sample variance equals a specified value (chi-squared test).</p> <p>Example Hypotheses: - H\u2080: \u03c3\u00b2 = \u03c3\u2080\u00b2 - H\u2081: \u03c3\u00b2 \u2260 \u03c3\u2080\u00b2</p> <p>Sample Use Case: A manufacturer checks if the variability in machine part diameters is within the required tolerance.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-variance","title":"Test for Variance","text":"<p>Variance tests are used for a comparison of variability, often as a predecessor for other tests.</p> <p>Let us take many samples of the same size from a normal population and find the sample variances.</p> <p>They follow a chi-square (\\(\\chi^2\\)) distribution, which is dependent on the degrees of freedom.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-one-variance","title":"Example: Test for One Variance","text":"<p>It is conjectured that the standard deviation for the annual return of mid cap mutual funds is 22.4%, when all such funds are considered and over a long period of time. The sample standard deviation of a certain mid cap mutual fund based on a random sample of size 32 is observed to be 26.4%.</p> <p>Do we have enough evidence to claim that the standard deviation of the chosen mutual fund is greater than the conjectured standard deviation for mid cap mutual funds at 0.05 level of significance?</p> <p>This is clearly a one-tailed test, concerning population variance, the variance for mid cap mutual funds.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-one-variance","title":"Test for One Variance","text":"Significance of the Test Assumptions Test Statistic Distribution Test for population varianceH\u2080: \u03c3\u00b2 = \u03c3\u2080\u00b2 - Continuous data  - Normally distributed population  - Random sampling from the population Chi Square distribution  (The test is also known as Chi-square test for variance)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_12","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#i-test-for-equality-of-variances","title":"i. Test for Equality of Variances","text":"<p>What: Tests if two (or more) population variances are equal (F-test, Levene\u2019s test).</p> <p>Example Hypotheses: - H\u2080: \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - H\u2081: \u03c3\u2081\u00b2 \u2260 \u03c3\u2082\u00b2</p> <p>Sample Use Case: An industrial engineer compares process variability before and after a process improvement.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-equality-of-variances","title":"Example: Test for Equality of Variances","text":"<p>Note: The hypothesis test in the example is a two-tailed test and not a one-tailed test.</p> <p>The variance of a process is an important quality of the process. A large variance implies that the process needs better control and there is opportunity to improve.</p> <p>The data (<code>Bags.csv</code>) includes weights for two different sets of bags manufactured from two different machines. It is assumed that the weights for two sets of bags follow normal distribution.</p> <p>Do we have enough statistical evidence at 5% significance level to conclude that there is a significant difference between the variances of the bag weights for the two machines?</p> <p>This is clearly a two-tailed test, concerning two population variances, the variance for bag weights from two different machines.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-variances","title":"Test for Equality of Variances","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population variancesH\u2080: \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - Normally distributed populations  - Independent populations  - Larger variance should be placed in the numerator F distribution  (The test is also known as F-test for variances)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_13","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#j-test-of-independence","title":"j. Test of Independence","text":"<p>What: Tests if two categorical variables are independent (Chi-squared test of independence).</p> <p>Example Hypotheses: - H\u2080: Variables are independent - H\u2081: Variables are not independent</p> <p>Sample Use Case: A health agency checks if smoking status is related to disease occurrence using a contingency table.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#chi-square-test-for-independence","title":"Chi-Square Test for Independence","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-1-smoking-and-gender","title":"Example 1: Smoking and Gender","text":"<p>A 2x2 contingency table describes two variables (smoking and gender), each at two levels, and stores the number of observations at each cell:</p> Male Female Total Smoker 120 100 220 Non-smoker 60 140 200 Total 180 240 420 <p>We are interested to know whether the two variables are independent.</p> <ul> <li>Null hypothesis (\\(H_0\\)): Smoking and gender are independent.</li> <li>Alternative hypothesis (\\(H_a\\)): Smoking and gender are not independent.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-2-beverage-preference-and-age-group","title":"Example 2: Beverage Preference and Age Group","text":"<p>The following table summarizes beverage preference across different age groups:</p> Age Tea/Coffee Soft Drink Others 21\u201334 25 90 20 35\u201355 40 35 25 &gt; 55 24 15 30 <p>Does beverage preference depend on age?</p> <p>This is a problem of Chi-Square test of independence, concerning the two independent categorical variables, Age and Beverage Preference.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#chi-square-test-for-independence-key-points","title":"Chi-Square Test for Independence: Key Points","text":"Significance of the Test Assumptions Test Statistic Distribution In a contingency tableH\u2080: The row and column variables are independent - Categorical variables  - Expected value of the number of sample observations in each level of the variable is at least 5  - Random sampling from the population Chi Square distribution  (The test is also known as Chi-square test of independence)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_14","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#k-anova-test","title":"k. ANOVA Test","text":"<p>What: Tests if the means of three or more groups are equal (Analysis of Variance).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 = \u03bc\u2083 = ... = \u03bc\u2096 - H\u2081: At least one \u03bc differs</p> <p>Sample Use Case: A marketing analyst wants to know if average sales differ by region (North, South, East, West).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#anova-test-key-concepts","title":"ANOVA Test: Key Concepts","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#important-terms","title":"Important Terms","text":"<ul> <li>Response: Dependent variable which is continuous and assumed to follow a normal distribution.</li> <li>Factor: Independent explanatory variable with several levels.</li> </ul> <p>Example: Comparing the weekly volume of sales by different teams of sales executives.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#how-anova-works","title":"How ANOVA Works","text":"<p>F-Statistic is the ratio of the between-group variations to within-group variations.</p> \\[ F\\text{-statistic} = \\frac{\\text{Between group variations}}{\\text{Within group variations}} \\] <ul> <li>A large value of F-Statistic indicates more variation between groups than within groups.</li> <li>Thus, it will provide evidence against the null hypothesis.</li> </ul> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#real-world-example","title":"Real-World Example","text":"<p>A traffic management inspector in a city wants to understand whether carbon emissions from different cars are different. The inspector believes fuel type may be an important factor responsible for differences in carbon emission.</p> <ul> <li>The inspector collects random samples from all registered cars and tests if the amount of carbon emission released depends on fuel type at 5% significance level.</li> </ul> <p>Here, we will compare the means of emission for the three different fuel types.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#anova-test-one-way-anova","title":"ANOVA Test: One-way ANOVA","text":"Significance of the Test Assumptions Test Statistic Distribution Test for means for more than two populations  H\u2080: All population means are equal - The populations are normally distributed  - Samples are independent simple random samples  - Population variances are equal F distribution  (The test is also known as One-way ANOVA F-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_15","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p>Each test above should include: - A description - Example hypotheses - A use case (see your original file for inspiration) - Python example (or Colab link, as above)</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_16","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#csv-download","title":"CSV Download","text":"<p>Download real datasets for hands-on practice.</p> <ul> <li>\ud83d\udc49 Download FastFood1.csv</li> <li>\ud83d\udc49 Download AOVData.csv</li> <li>\ud83d\udc49 Download Beverage.csv</li> <li>\ud83d\udc49 Download Bags1.csv</li> <li>\ud83d\udc49 Download Florida.csv</li> <li>\ud83d\udc49 Download SATVerbal1.csv</li> <li>\ud83d\udc49 Download TVRadio.csv</li> <li>\ud83d\udc49 Download rating.csv</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#11-pdf-slides","title":"11. PDF Slides \ud83d\udcc4","text":"<ul> <li>\ud83d\udc49 Download Lecture Slides \u2013 Hypothesis Testing</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Documentation</li> <li>Pandas Documentation</li> <li>Seaborn Documentation</li> <li>Plotly for Python</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare: Statistics</li> <li>StatQuest YouTube Channel</li> <li>Scikit-learn User Guide</li> <li>Real Python: Hypothesis Testing</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/","title":"\ud83e\udde0 Hypothesis Testing - Mobile Internet Case Study","text":""},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#1-overview","title":"1. Overview","text":"<p>ExperienceMyServices reported that a typical American spends an average of 144 minutes per day accessing the Internet via a mobile device, with a standard deviation of 110 minutes.</p> <p>To validate this claim, you collected a sample of 30 observations from friends and family and analyzed whether the population mean differs significantly from the reported 144 minutes.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#2-dataset-description","title":"2. Dataset Description","text":"<ul> <li>File Name: <code>InternetMobileTime.csv</code></li> <li>Observations: 30</li> <li>Variable: Daily internet usage (in minutes) via mobile device</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#3-methodology","title":"3. Methodology","text":"<p>We will perform a one-sample t-test using the following hypothesis:</p> <ul> <li>Null Hypothesis (H\u2080): \u03bc = 144 minutes  </li> <li>Alternative Hypothesis (H\u2081): \u03bc \u2260 144 minutes  </li> <li>Significance Level (\u03b1): 0.05  </li> </ul> <p>Assumptions: - The sample is randomly selected and independent. - The population is normally distributed.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#sample-code-snippet","title":"\ud83d\udd0d Sample Code Snippet","text":"<pre><code>import pandas as pd\nimport scipy.stats as st\n\n# Load the dataset\ndata = pd.read_csv('InternetMobileTime.csv')\n\n# Sample statistics\nsample_mean = data['Time'].mean()\nsample_std = data['Time'].std(ddof=1)\nn = len(data)\n\n# Hypothesized population mean\nmu_0 = 144\n\n# Perform one-sample t-test\nt_stat, p_value = st.ttest_1samp(data['Time'], popmean=mu_0)\n\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Conclusion\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is statistical evidence that the mean is different from 144 minutes.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No statistical evidence that the mean differs from 144 minutes.\")\n</code></pre>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>The analysis will help determine whether users are spending significantly more or less time than reported.</li> <li>If the p-value &lt; 0.05, we reject the null hypothesis and conclude the usage is significantly different from 144 minutes.</li> <li>If the p-value \u2265 0.05, we do not have enough evidence to dispute the claim.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83d\udcf1 Helps telecom and app companies align their strategy with real user behavior \ud83d\udcca Validates or challenges industry reports through data-driven hypothesis testing \ud83d\udcc8 Supports decision-making for media and digital advertisers</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>Scipy t-test documentation</li> <li>Understanding Hypothesis Testing</li> <li>Colab Notebook</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/","title":"\ud83e\udde0 Inferential Statistics - Medicon Dose Testing Case Study","text":""},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#1-overview","title":"1. Overview","text":"<p>Medicon, a pharmaceutical company, has manufactured the sixth batch (40,000 units) of COVID-19 vaccine doses. This vaccine has already passed clinical trials and over 200,000 doses have been administered.</p> <p>The sixth batch is now undergoing post-production quality testing to assess: - \u23f1\ufe0f Time of effect (how long it takes to cure COVID-19) - \u2705 Satisfactory outcomes (successful prevention without symptoms/side effects)</p> <p>This case study details the statistical quality assurance analysis for this batch.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#2-dataset-description","title":"2. Dataset Description","text":"<ul> <li>File Name: <code>doses.csv</code></li> <li>Observations: 50</li> <li>Columns: Time of effect (in hours) for each volunteer after taking the dose</li> </ul> <p>The dataset includes real measurements from 50 volunteers who received a dose from the sixth batch.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#3-methodology","title":"3. Methodology","text":"<p>We will approach the analysis through two dimensions:</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#a-satisfactory-rate-probability-analysis","title":"A. Satisfactory Rate Probability Analysis","text":"<ul> <li>Assumes a dose is 10 times more likely to be satisfactory than unsatisfactory.</li> <li>This leads to:   [   P(    ext{unsatisfactory}) = \\frac{1}{11} \u2248 0.0909,\\quad P(   ext{satisfactory}) = \\frac{10}{11}   ]</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#b-time-of-effect-sampling-distribution","title":"B. Time of Effect (Sampling Distribution)","text":"<ul> <li>Use inferential statistics on the <code>doses.csv</code> sample.</li> <li>Tasks include:</li> <li>Estimating the probability for time of effect being &lt; 11.5 hours.</li> <li>Calculating the 90th percentile.</li> <li>Building a 95% confidence interval for the population mean.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#sample-code-snippet","title":"\ud83d\udd0d Sample Code Snippet","text":"<pre><code># Example: Probability that exactly 3 out of 100 doses are unsatisfactory\nfrom scipy.stats import binom\n\nn = 100\np_unsatisfactory = 1 / 11\nprob_3_unsat = binom.pmf(3, n, p_unsatisfactory)\nprint(f\"Probability of exactly 3 unsatisfactory doses out of 100: {prob_3_unsat:.4f}\")\n</code></pre> <pre><code># Example: 95% Confidence Interval for Mean Time of Effect\nimport pandas as pd\nimport scipy.stats as st\nimport numpy as np\n\ndata = pd.read_csv('doses.csv')\nsample_mean = np.mean(data['Time'])\nsample_std = np.std(data['Time'], ddof=1)\nn = len(data)\nconfidence = 0.95\nmargin_error = st.t.ppf((1 + confidence) / 2., n-1) * (sample_std / np.sqrt(n))\n\nlower = sample_mean - margin_error\nupper = sample_mean + margin_error\nprint(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f}) hours\")\n</code></pre>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>The binomial model allows us to estimate how likely unsatisfactory doses appear in samples of size 100 or 200.</li> <li>The sampling distribution reveals:</li> <li>Most doses show effectiveness within a tight time range.</li> <li>The confidence interval suggests a high level of reliability for future production batches.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\u2714\ufe0f Helps Medicon decide whether to scale up production \u2714\ufe0f Allows regulatory teams to verify safety even post-clinical trials \u2714\ufe0f Supports bulk order decisions by external entities like city governments (e.g., NYC request for 200 doses)</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>Scipy binom documentation</li> <li>Confidence Intervals \u2014 Khan Academy</li> <li>Google Colab: Full Python Notebook</li> </ul>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/","title":"Q&amp;A Session 1: Probability &amp; Statistics","text":""},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#hypothesis-testing-qa","title":"\u2753 Hypothesis Testing Q&amp;A","text":""},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. How to set the value of k for binom.pmf() and binom.cdf()?</li> <li>2. What are the Empirical rules of a normal distribution?</li> <li>3. What is the degree of freedom?</li> <li>4. What are the functions used in Statistical Analysis from the Scipy Stats library to calculate probabilities in a normal distribution?</li> <li>5. Population vs Sample | Parameter vs Statistic</li> <li>6. When to Use pmf, pdf, cdf, and ppf</li> <li>7. Setting the Value of k in binom.pmf() and binom.cdf()</li> <li>8. loc and scale in Uniform Distribution</li> <li>9. Z-Score in Real Life</li> <li>10. What Does norm.ppf() Do?</li> <li>11. Why Subtract 1 in binom.cdf() But Not in norm.cdf()?</li> <li>12. Interpreting the p-value in Hypothesis Testing</li> <li>13. What Is the Difference Between Type I and Type II Errors?</li> <li>14. What is Statistical Power?</li> <li>15. What is alpha in hypothesis testing?</li> <li>16. What are the steps of conducting a hypothesis test?</li> <li>17. How to decide between normal and t-distribution for confidence interval?</li> <li>18. Which Python function is used to compute a confidence interval for the population mean?</li> </ul>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#1-how-to-set-the-value-of-k-for-binompmf-and-binomcdf-functions","title":"1. How to set the value of k for binom.pmf() and binom.cdf() functions?","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li>If you want the probability that X is exactly equal to x: <code>binom.pmf(k=x, ...)</code></li> <li>If you want the probability that X is less than or equal to x: <code>binom.cdf(k=x, ...)</code></li> <li>If you want the probability that X is greater than or equal to x: <code>1 - binom.cdf(k=x-1, ...)</code></li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#2-what-are-the-empirical-rules-of-a-normal-distribution","title":"2. What are the Empirical rules of a normal distribution?","text":"<p>Tags: <code>#normal distribution</code> <code>#standard deviations</code> <code>#empirical rule</code></p> <p>The empirical rule for a normal distribution: - 68% of data: within 1 standard deviation (\\(\\mu \\pm \\sigma\\)) - 95% of data: within 2 standard deviations (\\(\\mu \\pm 2\\sigma\\)) - 99.7% of data: within 3 standard deviations (\\(\\mu \\pm 3\\sigma\\))</p> <p>Example (Pizza Delivery): - Mean delivery time (\\(\\mu\\)): 30 min - Standard deviation (\\(\\sigma\\)): 5 min - 68%: 25\u201335 min (30 \u00b1 5) - 95%: 20\u201340 min (30 \u00b1 2\u00d75) - 99.7%: 15\u201345 min (30 \u00b1 3\u00d75)</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#3-what-is-the-degree-of-freedom","title":"3. What is the degree of freedom?","text":"<p>Tags: <code>#estimate</code> <code>#observations</code></p> <p>The degree of freedom (df) for an estimate is the number of independent pieces of information that went into calculating the estimate. - For n observations, the degree of freedom is typically n - 1.</p> <p>Example: If you have 10 observations and know their total sum, you only need 9 to determine the last one: df = 10 - 1 = 9</p> <p>This applies universally to many estimates.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#4-what-are-the-functions-used-in-statistical-analysis-from-the-scipy-stats-library-to-calculate-probabilities-in-a-normal-distribution","title":"4. What are the functions used in Statistical Analysis from the Scipy Stats library to calculate probabilities in a normal distribution?","text":"<p>Tags: <code>#pdf</code> <code>#cdf</code> <code>#ppf</code></p> <ul> <li><code>pdf(x, loc=0, scale=1)</code> \u2014 Probability density function.</li> <li><code>cdf(x, loc=0, scale=1)</code> \u2014 Cumulative distribution function.</li> <li><code>ppf(q, loc=0, scale=1)</code> \u2014 Percent point function (inverse of cdf, i.e., percentiles).</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#5-population-vs-sample-parameter-vs-statistic","title":"5. Population vs Sample | Parameter vs Statistic","text":"<p>Tags: <code>#population</code> <code>#sample</code> <code>#parameter</code> <code>#statistic</code></p> <ul> <li>A population is the complete group you want to study (e.g., all tech startups in Asia).</li> <li>A parameter is a fixed, unknown value that describes a population (e.g., average height of all CFA candidates).</li> <li>A sample is a subset taken from the population.</li> <li>A statistic is a known value calculated from the sample (e.g., sample mean or standard deviation).</li> </ul> <p>\ud83d\udcce Example: If the population mean height is unknown, we estimate it using the sample mean.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#6-when-to-use-pmf-pdf-cdf-and-ppf","title":"6. When to Use pmf, pdf, cdf, and ppf","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#ppf</code> <code>#probabilities</code></p> <ul> <li>pmf (P(X=x)): For discrete variables (e.g., Binomial).</li> <li>pdf (P(X=x)): For continuous variables (e.g., Normal).</li> <li>cdf (P(X\u2264x)): Cumulative probability for both discrete and continuous variables.</li> <li>ppf: Inverse of cdf. Given P(X\u2264x) = \u03b1, returns the value of <code>x</code>.</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#7-setting-the-value-of-k-in-binompmf-and-binomcdf","title":"7. Setting the Value of k in binom.pmf() and binom.cdf()","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li><code>binom.pmf(k=x, ...)</code> \u2192 Calculates P(X = x)</li> <li><code>binom.cdf(k=x, ...)</code> \u2192 Calculates P(X \u2264 x)</li> <li><code>1 - binom.cdf(k=x-1, ...)</code> \u2192 Calculates P(X \u2265 x)</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#8-loc-and-scale-in-uniform-distribution","title":"8. loc and scale in Uniform Distribution","text":"<p>Tags: <code>#uniform distribution</code> <code>#loc</code> <code>#scale</code></p> <ul> <li>In <code>scipy.stats.uniform(loc, scale)</code>, the distribution is defined over <code>[loc, loc + scale]</code></li> <li>If X ~ U(1, 4), then:</li> <li><code>loc = 1</code>, <code>scale = 3</code></li> </ul> <p>Formula: If \\( X \\sim U(a, b) \\) \u21d2 <code>loc = a</code>, <code>scale = b - a</code></p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#9-z-score-in-real-life","title":"9. Z-Score in Real Life","text":"<p>Tags: <code>#z-score</code> <code>#standard deviations</code></p> <ul> <li>A z-score shows how many standard deviations a point is from the mean.</li> <li>Useful for comparing scores from different distributions.</li> </ul> <p>\ud83d\udcce Example: Comparing exam scores across different scales using z-score normalization.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#10-what-does-normppf-do","title":"10. What Does norm.ppf() Do?","text":"<p>Tags: <code>#ppf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li><code>norm.ppf(p)</code> is the inverse of <code>norm.cdf()</code>.</li> <li>It returns the value of <code>x</code> such that P(X \u2264 x) = p.</li> </ul> <p>\ud83d\udcce Example: <code>norm.ppf(0.92)</code> returns the point below which 92% of the data lies.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#11-why-subtract-1-in-binomcdf-but-not-in-normcdf","title":"11. Why Subtract 1 in binom.cdf() But Not in norm.cdf()?","text":"<p>Tags: <code>#cdf</code> <code>#distribution</code> <code>#continuous</code></p> <ul> <li>Binomial is discrete \u2192 Use <code>1 - binom.cdf(k-1, ...)</code></li> <li>Normal is continuous \u2192 Use <code>1 - norm.cdf(x)</code> (no subtraction)</li> </ul> <p>\ud83d\udccc Reason: In continuous distributions, P(X = x) = 0. In discrete, P(X = x) \u2260 0, so subtracting 1 ensures accuracy.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#12-interpreting-the-p-value-in-hypothesis-testing","title":"12. Interpreting the p-value in Hypothesis Testing","text":"<p>Tags: <code>#p-value</code> <code>#hypothesis testing</code> <code>#significance</code></p> <ul> <li>The p-value measures the probability of obtaining results as extreme as (or more extreme than) those observed, assuming the null hypothesis is true.</li> <li>A small p-value (typically \u2264 0.05) suggests strong evidence against the null hypothesis, so you may reject it.</li> <li>A large p-value suggests weak evidence against the null, so you fail to reject it.</li> </ul> <p>\ud83d\udcce Example: If p-value = 0.03, there is only a 3% chance that the observed results would occur if the null hypothesis were true.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#13-what-is-the-difference-between-type-i-and-type-ii-errors","title":"13. What Is the Difference Between Type I and Type II Errors?","text":"<p>Tags: <code>#type-i-error</code> <code>#type-ii-error</code> <code>#errors</code></p> <ul> <li>Type I Error: Rejecting the null hypothesis when it is actually true (false positive).</li> <li>Type II Error: Failing to reject the null hypothesis when it is actually false (false negative).</li> </ul> <p>\ud83d\udcce Example: - Type I: Convicting an innocent person. - Type II: Letting a guilty person go free.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#14-what-is-statistical-power","title":"14. What is Statistical Power?","text":"<p>Tags: <code>#statistical power</code> <code>#type-ii-error</code></p> <ul> <li>Statistical power is the probability that a test correctly rejects a false null hypothesis (i.e., detects an effect if there is one).</li> <li>Power = 1 - Probability of Type II Error (\u03b2).</li> </ul> <p>\ud83d\udcce Example: A test with 80% power will correctly identify a true effect 80% of the time.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#15-what-is-alpha-in-hypothesis-testing","title":"15. What is alpha in hypothesis testing?","text":"<p>Tags: <code>#hypothesis testing</code> <code>#probability</code></p> <p>Alpha (\\(\\alpha\\)) is the level of significance in hypothesis testing. It represents the probability of making a Type I error\u2014that is, rejecting the null hypothesis when it is actually true. Alpha is the small chance that your test result happened by random chance. Common values are 0.05 (5%) and 0.01 (1%). For \\(\\alpha = 0.05\\), you are 95% confident that your result is not due to chance. Note: Alpha must be chosen before running the test and should not be changed afterward.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#16-what-are-the-steps-of-conducting-a-hypothesis-test","title":"16. What are the steps of conducting a hypothesis test?","text":"<p>Tags: <code>#hypothesis testing</code> <code>#p-value</code></p> <p>The typical steps for conducting a hypothesis test are: 1. Formulate the null and alternative hypotheses for your problem. 2. Select the appropriate test based on your data and hypothesis. 3. Decide the significance level (\\(\\alpha\\)) before running the test. 4. Collect the relevant data for the test. 5. Calculate the p-value using the chosen statistical test. 6. Compare the p-value with alpha and draw your conclusion.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#17-how-to-decide-between-normal-and-t-distribution-for-confidence-interval","title":"17. How to decide between normal and t-distribution for confidence interval?","text":"<p>Tags: <code>#distribution</code> <code>#confidence interval</code> <code>#standard deviation</code></p> <p>The choice depends on whether the population standard deviation (\\(\\sigma\\)) is known: - If \\(\\sigma\\) is known, use the normal distribution to compute the confidence interval. - If \\(\\sigma\\) is unknown, estimate it using the sample standard deviation (\\(s\\)) and use the t-distribution for the confidence interval.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#18-which-python-function-is-used-to-compute-a-confidence-interval-for-the-population-mean","title":"18. Which Python function is used to compute a confidence interval for the population mean?","text":"<p>Tags: <code>#confidence interval</code> <code>#population</code> <code>#sample</code></p> <p>The <code>interval()</code> function (from <code>scipy.stats</code>) computes a confidence interval.</p> <p>Example 1: Known Population Standard Deviation (Normal Distribution)</p> <p>```python import numpy as np from scipy.stats import norm</p> <p>Sample mean = 500, population std dev = 25, sample size = 100 np.round(norm.interval(0.95, loc=500, scale=25/np.sqrt(100)), 2)</p>"},{"location":"01-foundation/3-datascience-machine-learning/","title":"\ud83e\udd16 Data Science &amp; Machine Learning","text":"<p>Welcome to the Data Science &amp; ML section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/3-datascience-machine-learning/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Master the fundamentals of data workflows, model training, and evaluation.</p> <ul> <li>Data Cleaning</li> <li>Feature Engineering</li> <li>Model Training</li> <li>Evaluation Metrics</li> <li>Overfitting &amp; Regularization</li> <li>Model Deployment</li> </ul>"},{"location":"01-foundation/3-datascience-machine-learning/#additional-reference","title":"\ud83d\udcc2 Additional Reference","text":"<p>Supplement your learning with deep dives, examples, and best practices.</p> <ul> <li>EDA on Titanic Dataset</li> <li>ML Pipeline Walkthrough</li> <li>End-to-End ML Project</li> </ul>"},{"location":"01-foundation/3-datascience-machine-learning/#qa","title":"\u2753 Q&amp;A","text":"<p>Clarifications, insights, and FAQs from practitioners and learners.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/","title":"Data Science &amp; Machine Learning","text":"<p>Welcome to the Data Science &amp; Machine Learning section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#learning-path","title":"\ud83d\udcda Learning Path","text":"<p>Master end-to-end workflows from preprocessing to model evaluation.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/1-learning-path/</code></p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#additional-reference","title":"\ud83d\udcd8 Additional Reference","text":"<p>Study real-world case insights and methodological notes.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/2-additional-reference/</code></p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#qa","title":"\u2753 Q&amp;A","text":"<p>Dig into insightful Q&amp;A on practical ML topics.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/3-q&amp;a/</code></p> <p>\u2b05\ufe0f Back to Home</p>"},{"location":"02-gen-ai-core/","title":"\ud83d\udd0d Gen AI Core","text":"<p>\ud83d\udcc2 Navigation Tip Explore the technical heart of Generative AI. The left sidebar lists key modules like transformer models and Retrieval-Augmented Generation (RAG) using LangChain.</p> <p>\ud83d\udca1 Recommended path: Start with \u201cTransformers &amp; Generative AI\u201d before moving to RAG-based workflows.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/","title":"\ud83e\udd16 Transformers &amp; Generative AI","text":"<p>Welcome to the Transformers &amp; Generative AI section of the Generative AI Atlas. This module guides you through the foundations, use cases, and advanced implementations of Transformer models and generative techniques.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Begin your journey through the core concepts and techniques related to Transformers and Generative AI:</p> <ul> <li>Intro</li> <li>Getting Started</li> <li>NLP Overview</li> <li>Transformer Intro</li> <li>Popular Transformer Models</li> <li>Using Transformers</li> <li>Real-World LLM Scenarios</li> <li>LLM Intro</li> <li>LLM Prep</li> <li>LLM Advanced</li> <li>LLM Specialized</li> <li>LLM Deployment</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>In-depth resources to supplement your knowledge:</p> <ul> <li>Supplemental Material</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#qa","title":"\u2753 Q&amp;A","text":"<p>Clarifications, tips, and community-driven insights:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/","title":"What You\u2019ll Learn","text":"<p>\ud83d\udccc Quick Navigation</p> <ul> <li>What You\u2019ll Learn</li> <li>Part 1: Fundamentals of NLP and Transformers<ul> <li>Transformer Foundations</li> </ul> </li> <li>Part 2: Working with Large Language Models (LLMs)<ul> <li>Key Model Architectures</li> <li>Real-World Tasks You\u2019ll Implement</li> <li>Advanced Tooling and Techniques</li> </ul> </li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#part-1-fundamentals-of-nlp-and-transformers","title":"Part 1: Fundamentals of NLP and Transformers","text":"<p>You\u2019ll explore the evolution of natural language processing (NLP) through four historical phases:</p> <ul> <li>Rule-Based Systems </li> <li> <p>Manually defined rules for parsing, tagging, and other language tasks.</p> </li> <li> <p>Statistical Methods </p> </li> <li> <p>Used mathematical probability and co-occurrence to model language.</p> </li> <li> <p>Machine Learning Era </p> </li> <li> <p>Leveraged labeled data for training classifiers like SVMs and Naive Bayes.</p> </li> <li> <p>Deep Learning &amp; Embeddings </p> </li> <li>Enabled dense semantic understanding and contextual word representations.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#transformer-foundations","title":"Transformer Foundations","text":"<p>??? info \"Why Attention Matters in Transformers\"</p> <pre><code>The attention mechanism enables a model to focus on relevant portions of the input,  \nrather than treating every token equally. This allows:\n\n- Better context awareness\n- Improved long-range dependency modeling\n- Reduced reliance on fixed-size memory\n\n\ud83d\udd0d **Example:** In translation, attention helps align source and target tokens precisely.\n</code></pre> <p>You\u2019ll also build a deep understanding of the architecture that powers modern LLMs:</p> <ul> <li>Attention Mechanism </li> <li> <p>Allows models to focus on important parts of the input.</p> </li> <li> <p>Encoder-Decoder Structure </p> </li> <li> <p>Enables tasks like translation, summarization, and text generation.</p> </li> <li> <p>Tokenization &amp; Embeddings </p> </li> <li> <p>Converts text into vectors, enabling model computation.</p> </li> <li> <p>Pretraining and Fine-Tuning </p> </li> <li>Learn how general-purpose models adapt to specific tasks.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#part-2-working-with-large-language-models-llms","title":"Part 2: Working with Large Language Models (LLMs)","text":"<p>This section focuses on state-of-the-art transformer-based models and their practical applications.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#key-model-architectures","title":"Key Model Architectures","text":"<ul> <li>BERT (Encoder-only) </li> <li> <p>Learns bidirectional context; ideal for understanding tasks like classification or Q&amp;A.</p> </li> <li> <p>GPT (Decoder-only) </p> </li> <li> <p>Generates coherent, fluent text; the backbone of tools like ChatGPT.</p> </li> <li> <p>T5 (Encoder-Decoder) </p> </li> <li>Treats all problems as a text-to-text task, offering maximum flexibility.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#real-world-tasks-youll-implement","title":"Real-World Tasks You\u2019ll Implement","text":"<ul> <li>Masked Language Modeling (MLM)  </li> <li>Semantic Search with embeddings  </li> <li>Document-Based Question Answering  </li> <li>Instruction-Following Text Generation  </li> <li>Product Review Generation (prompt-based)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#advanced-tooling-and-techniques","title":"Advanced Tooling and Techniques","text":"<p>You\u2019ll gain hands-on experience with modern model optimization strategies:</p> <ul> <li>LoRA and PeFT (parameter-efficient fine-tuning)  </li> <li>8-bit / 4-bit quantization for faster, smaller models  </li> <li>FlashAttention, DeepSpeed, and FSDP for accelerated training  </li> <li>Chat templates and RLHF (Reinforcement Learning from Human Feedback)</li> </ul> <p>??? info \"Why Fine-Tuning is Crucial\"</p> <pre><code>Fine-tuning pre-trained models is essential when applying LLMs to specialized or production environments. It enhances the model's ability to understand domain-specific language, improves generalization, and reduces errors.\n\n**Benefits of Fine-Tuning:**\n\n- \u2705 Adapts general models to niche domains (e.g., law, healthcare, finance)\n- \ud83d\udcc8 Boosts model performance on specific downstream tasks\n- \ud83e\udde0 Learns contextual and jargon-heavy nuances\n- \ud83d\udcbe Saves compute compared to training from scratch\n\n\ud83d\udd0d **Example:**  \nFine-tuning BERT on clinical notes dramatically improves performance in electronic health record (EHR) classification tasks.\n</code></pre> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face: T5-base Amazon Product Reviews Model</li> <li>Hugging Face: DiabloGPT Open Instruct Model</li> <li>Google Colab: T5 Product Review Notebook</li> <li>Attention Is All You Need (Transformer Paper)</li> <li>Hugging Face Transformers Documentation</li> <li>Google AI Blog on BERT</li> <li>Sebastian Ruder: NLP Progress</li> <li>The Illustrated Transformer (by Jay Alammar)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/","title":"Getting Started","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Course Introduction</li> <li>2. Instructor's Advice</li> <li>3. Coding Environment Setup</li> <li>4. Tools &amp; Extensions</li> <li>5. Practical Coding Guidelines</li> <li>6. Colab &amp; Hugging Face Integration</li> <li>7. Next Steps</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#1-course-introduction","title":"1. Course Introduction","text":"<p>This course is designed to teach transformer architectures and large language model workflows, combining theory and hands-on implementation.</p> <ul> <li>The course is split into five major parts, each building on the previous.</li> <li>Initial sections are theory-heavy but short and packed with examples.</li> <li>Ideal for both beginners and experienced NLP practitioners.</li> <li>Designed for progressive skill development: from theory \u2192 models \u2192 coding \u2192 deployment.</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#2-instructors-advice","title":"2. Instructor's Advice","text":"<ul> <li>Leave feedback after engaging with multiple sections.</li> <li>Reviews improve course discoverability and help maintain high-quality material.</li> <li>Beginner sections are mandatory for understanding transformer mechanics.</li> <li>Experienced practitioners are encouraged to revisit foundational concepts.</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#3-coding-environment-setup","title":"3. Coding Environment Setup","text":"<ul> <li>Recommended editor: Visual Studio Code (VS Code)</li> <li>VS Code is:</li> <li>Free, lightweight, and open-source</li> <li>Well-integrated with remote environments and Jupyter workflows</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#vs-code-setup","title":"VS Code Setup","text":"<ol> <li>Install Python Extension in VS Code.</li> <li>Enable interactive mode:</li> <li>Press <code>Ctrl+Shift+P</code> or <code>Cmd+Shift+P</code> on Mac</li> <li>Search: \u201csend to interactive\u201d</li> <li>Enable this feature for real-time execution feedback</li> </ol> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#4-tools-extensions","title":"4. Tools &amp; Extensions","text":"<ul> <li>Google Colab Notebooks provided for each practical lesson:</li> <li>No local setup required</li> <li> <p>Accessible via \u201cResources\u201d button in each lesson</p> </li> <li> <p>Python Source Files available for download alongside Colab notebooks</p> </li> <li> <p>Remote Development using:</p> </li> <li>VS Code\u2019s Remote SSH Extension</li> <li>Setup:<ul> <li>Edit <code>~/.ssh/config</code> with remote VM/IP</li> <li>Connect via bottom-left VS Code status bar</li> </ul> </li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#5-practical-coding-guidelines","title":"5. Practical Coding Guidelines","text":"<ul> <li>Code alongside the instructor.</li> <li>Pause and reflect \u2014 do not just watch passively.</li> <li>Adopt a top-down learning strategy:</li> <li>Start from complete example code \u2192 adapt it to solve real problems</li> <li>Encouraged for better retention and understanding</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#6-colab-hugging-face-integration","title":"6. Colab &amp; Hugging Face Integration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#code-demo-access","title":"\ud83e\uddea Code Demo Access","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Code notebooks are synced with Colab</li> <li>Each lesson includes links to:</li> <li>Colab notebooks</li> <li>Python source files</li> <li>Trained model weights (where applicable)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#hugging-face-integration","title":"\ud83e\udd17 Hugging Face Integration","text":"<ul> <li>Models trained during this course are hosted on Hugging Face</li> <li>Resources panel includes direct links to model cards and weights</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#7-next-steps","title":"7. Next Steps","text":"<ul> <li>The next section will guide you through configuring your Python runtime and understanding core NLP concepts.</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face Transformers Documentation</li> <li>Google Colaboratory Guide</li> <li>Visual Studio Code</li> <li>ArXiv: Attention Is All You Need</li> <li>PyTorch Docs</li> <li>TensorFlow Guide</li> <li>OpenAI Research</li> <li>NVIDIA LLM Blog</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/","title":"NLP Evolution Timeline","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Word2Vec Explained (Google Research)</li> <li>GloVe: Global Vectors for Word Representation (Stanford)</li> <li>FastText (Facebook AI)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Sebastian Ruder: NLP Progress Tracker</li> <li>Hugging Face: T5-base Product Review Model</li> <li>Google Colab: Try Word Embeddings</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/","title":"Transformer Fundamentals","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Transformer Architecture Overview</li> <li>2. Transformer Training Paradigm: Pre-training and Fine-tuning</li> <li>3. Tokenization and Embeddings in Transformer Models</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#1-transformer-architecture-overview","title":"1. Transformer Architecture Overview","text":"<p>This lesson introduces the transformer model architecture, emphasizing its structural innovations, key mechanisms, and how it revolutionized NLP by overcoming the limitations of RNNs and LSTMs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#origins-and-significance","title":"Origins and Significance","text":"<ul> <li>Introduced in 2017 via the paper \"Attention Is All You Need\"</li> <li>Replaced sequential RNN/LSTM processing with fully parallel architecture</li> <li>Solved long-range dependency issues and improved training speed</li> <li>Enabled large-scale model training and breakthroughs in language understanding</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#core-components-of-transformer-architecture","title":"Core Components of Transformer Architecture","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#encoder-decoder-structure","title":"Encoder-Decoder Structure","text":"<ul> <li>Encoder: Converts input text into continuous vector representations capturing context and relationships</li> <li>Decoder: Generates output text from encoder\u2019s processed information</li> <li>Enables tasks like translation, summarization, and question answering</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#attention-mechanisms","title":"Attention Mechanisms","text":"<ul> <li>Self-Attention: Weighs each word relative to others to build context-aware representations</li> <li>Scaled Dot-Product Attention: Computes dot products, scales scores, and applies softmax</li> <li>Multi-Head Attention: Uses multiple heads to capture diverse semantic/syntactic patterns</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Compensates for lack of inherent word order in attention-only models</li> <li>Adds position-based signals to token embeddings</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#feed-forward-network-layer-normalization","title":"Feed-Forward Network &amp; Layer Normalization","text":"<ul> <li>Feed-Forward Network: Applies non-linear transformations to extract high-level features</li> <li>Layer Normalization: Stabilizes training by normalizing outputs between layers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#full-encoder-and-decoder-block","title":"Full Encoder and Decoder Block","text":"<ul> <li>Composed of stacked layers with:</li> <li>Multi-head attention</li> <li>Feed-forward networks</li> <li>Layer normalization</li> <li>Decoder includes additional encoder-decoder attention to align output generation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#real-world-application-example","title":"Real-World Application Example","text":"<p>Abstractive Question Answering</p> <ul> <li>Input: Paragraph + Question</li> <li>Encoder: Processes both into contextual embeddings</li> <li>Decoder: Generates an answer from the learned representation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Transformers enabled scalable, parallel NLP processing</li> <li>Encoder-decoder architecture allows diverse tasks</li> <li>Attention mechanisms are key to understanding global context</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#2-transformer-training-paradigm-pre-training-and-fine-tuning","title":"2. Transformer Training Paradigm: Pre-training and Fine-tuning","text":"<p>This lesson outlines the two-phase training process of transformer models\u2014pre-training and fine-tuning\u2014contrasting it with traditional ML workflows.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#training-structure-overview","title":"Training Structure Overview","text":"<ul> <li>Pre-training: General language learning from large unlabeled datasets</li> <li>Fine-tuning: Task-specific adaptation using labeled datasets</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#pre-training-phase","title":"Pre-training Phase","text":"<ul> <li>Learns grammar, context, word relationships, and long-range dependencies</li> <li>Massive-scale unsupervised training</li> <li>\ud83d\udd01 Analogy: Like learning music theory before mastering a genre</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#fine-tuning-phase","title":"Fine-tuning Phase","text":"<ul> <li>Adapts pre-trained models to tasks like NER, translation, QA, etc.</li> <li>Requires smaller supervised datasets</li> <li>Leverages transfer learning</li> <li>\ud83d\udd01 Analogy: Like a trained pianist specializing in jazz</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#combined-workflow","title":"Combined Workflow","text":"<ol> <li>Step 1: Pre-training</li> <li>Random initialization \u2192 trained on general data</li> <li>Step 2: Fine-tuning</li> <li>Task-specific data \u2192 adapted for downstream performance</li> </ol>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#real-world-considerations","title":"Real-world Considerations","text":"<ul> <li>Pre-training requires huge compute and data (done by orgs like Google, OpenAI)</li> <li>Most use pre-trained models and fine-tune</li> <li>Full pre-training is rare unless:</li> <li>You work with proprietary, underrepresented, or specialized domains (e.g., legal, clinical)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>Pre-training + fine-tuning is the standard approach in NLP</li> <li>Enables rapid model deployment with high performance</li> <li>Specialized domains may benefit from custom pre-training</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#3-tokenization-and-embeddings-in-transformer-models","title":"3. Tokenization and Embeddings in Transformer Models","text":"<p>This lesson covers how transformers process raw text into vector representations using tokenization and embeddings.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#tokenization","title":"Tokenization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#purpose","title":"Purpose","text":"<ul> <li>Breaks text into smaller units called tokens</li> <li>Translates natural language into numerical input (token IDs)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#types-of-tokenization","title":"Types of Tokenization","text":"<ul> <li>Word-level: One token per word; suffers from OOV (out-of-vocabulary) issues</li> <li>Character-level: Every character is a token; leads to longer sequences</li> <li>Subword-level (common): Breaks unknown words into known parts (e.g., Byte-Pair Encoding)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#workflow","title":"Workflow","text":"<ol> <li>Breaks text into tokens</li> <li>Maps tokens to IDs using a predefined vocabulary</li> <li>Feeds IDs into the transformer model</li> </ol>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#embeddings","title":"Embeddings","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#purpose_1","title":"Purpose","text":"<ul> <li>Convert token IDs into high-dimensional dense vectors</li> <li>Capture meaning and contextual usage of tokens</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-concepts","title":"Key Concepts","text":"<ul> <li>Embeddings are context-aware (e.g., \"bank\" in finance vs. riverbank)</li> <li>Contextual embeddings change based on surrounding text</li> <li>Learned during pre-training</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#example","title":"Example","text":"<p>```text Sentence 1: She picked a rose. Sentence 2: The sun rose early.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Original Transformer Paper)</li> <li>The Illustrated Transformer by Jay Alammar</li> <li>Hugging Face Transformers Documentation</li> <li>Google Colab: Transformer Architecture Notebook</li> <li>Hugging Face: T5-base Model (Amazon Product Reviews)</li> <li>Hugging Face: diabloGPT Instruction Model</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/","title":"Transformer Architectures Study Hub","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. BERT: Encoder-Only Transformer Architecture</li> <li>2. Transformer &amp; GPT Evolution</li> <li>3. T5: Text-To-Text Transfer Transformer</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#1-bert-encoder-only-transformer-architecture","title":"1. BERT: Encoder-Only Transformer Architecture","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP in 2018 by introducing a bidirectional, encoder-only architecture designed for deep contextual understanding of language. This section explores BERT\u2019s structure, training strategy, practical applications, and the latest advancements in its ecosystem.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#model-overview","title":"Model Overview","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#key-characteristics","title":"Key Characteristics","text":"<ul> <li> <p>Bidirectional   BERT reads text in both directions (left-to-right and right-to-left) simultaneously to capture full context.</p> </li> <li> <p>Encoder-Only Architecture   Built entirely on stacked encoders with self-attention mechanisms.   Optimized for understanding, not generating, text.</p> </li> <li> <p>Representations   Learns dense vector embeddings that reflect token meaning in context.</p> </li> <li> <p>Transformer-Based   Leverages the original transformer architecture\u2014only the encoder side.</p> </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-strategy","title":"Pre-training Strategy","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#datasets","title":"Datasets","text":"<ul> <li>English Wikipedia  </li> <li>10,000+ unpublished English books  </li> <li>Total: Over 3 billion words</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-objectives","title":"Pre-training Objectives","text":"<ul> <li> <p>Masked Language Modeling (MLM)   Randomly masks 15% of tokens; the model must predict them using surrounding context.   Enables deep semantic and syntactic comprehension.</p> </li> <li> <p>Next Sentence Prediction (NSP)   Trains BERT to classify whether one sentence follows another.   Aids understanding of inter-sentence relationships.   Later models (e.g., RoBERTa) removed this due to limited benefit.</p> </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#fine-tuning-applications","title":"Fine-tuning Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#text-classification","title":"Text Classification","text":"<ul> <li>Sentiment analysis, spam detection, topic categorization  </li> <li>Produces a single class label from the encoded text</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<ul> <li>Identifies token-level entities (e.g., people, dates, organizations)  </li> <li>BERT's contextual awareness improves accuracy in boundary detection</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#extractive-question-answering","title":"Extractive Question Answering","text":"<ul> <li>Extracts answers directly from a provided context passage  </li> <li>Predicts start and end token positions  </li> <li>Used in customer service, document retrieval</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#semantic-similarity","title":"Semantic Similarity","text":"<ul> <li>Produces embeddings for entire sentences or passages  </li> <li>Used in:</li> <li>Duplicate detection  </li> <li>Paraphrase recognition  </li> <li>Semantic search  </li> <li>Vector-based retrieval systems</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#bert-model-variants","title":"BERT Model Variants","text":"Model Parameters Notes BERT-Base ~110M 12 layers, 12 heads, 768 hidden units BERT-Large ~340M 24 layers, 16 heads, 1024 hidden units DistilBERT ~66M Lightweight version by Hugging Face RoBERTa ~125M+ No NSP, trained longer, dynamic masking (Meta) ALBERT ~12M\u2013223M Weight-sharing, efficient training (Google Research) DeBERTa Varies Disentangled attention and enhanced position embeddings (Microsoft)"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#latest-developments-as-of-2025","title":"Latest Developments (as of 2025)","text":"<ul> <li>BERT is foundational for retrieval-augmented generation (RAG) and embedding-based search systems.</li> <li>Multilingual BERT (mBERT) supports 100+ languages.</li> <li>BERT encoders are commonly paired with large decoders like GPT-4o for hybrid retrieval-generation systems.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#2-transformer-gpt-evolution","title":"2. Transformer &amp; GPT Evolution","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-45-orion","title":"GPT-4.5 (\u201cOrion\u201d)","text":"<ul> <li>Released: Feb 27, 2025</li> <li>Enhanced instruction-following, fewer hallucinations</li> <li>API &amp; ChatGPT Pro access</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-41-family","title":"GPT-4.1 Family","text":"<ul> <li>Released: April 14, 2025</li> <li>Includes mini/nano variants supporting 1M-token context</li> <li>More efficient than GPT-4o</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#reasoning-models-o1-o3-mini-o4-mini","title":"Reasoning Models (o1, o3-mini, o4-mini)","text":"<ul> <li>Optimized for logic, math, and science</li> <li>o3-mini and o4-mini include multimodal chain-of-thought support</li> <li>Ideal for autonomous agents and structured tool use</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-5-expected-august-2025","title":"GPT-5 (Expected August 2025)","text":"<ul> <li>Will include reasoning from o3</li> <li>Multimodal + open access discussions ongoing</li> <li>Expected to set a new benchmark for general-purpose AI</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Shift from scaling parameters to scaling reasoning</li> <li>GPT-4.5/5 marks evolution toward modular, low-latency, high-accuracy models</li> </ul> Model Category Architecture Strengths Use Cases GPT\u20114.5 Instructional GPT Decoder-only Prompt-following, fewer hallucinations General NLP, coding, chatbots GPT\u20114.1 mini Efficient GPT Decoder-only 1M context, fast inference Coding, RAG o3-mini Reasoning LLM Decoder-only Logic + math + tool use Agents, science tasks GPT\u20115 Unified Multi-module Multimodal, reasoning-first Enterprise AI, general AI <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#3-t5-text-to-text-transfer-transformer","title":"3. T5: Text-To-Text Transfer Transformer","text":"<p>T5 reframes every NLP problem as a text-to-text task (e.g., input: \u201cTranslate English to German: How are you?\u201d \u2192 output: \u201cWie geht es dir?\u201d). This unified approach enables a wide range of applications across translation, QA, summarization, and more.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#model-overview_1","title":"Model Overview","text":"<ul> <li>Encoder-decoder transformer with BERT-style encoding + GPT-style generation</li> <li>Flexible task control via text prefixes (e.g., \u201csummarize:\u201d, \u201ctranslate:\u201d)</li> <li>First model to fully embrace text-to-text multitask learning</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-c4-dataset-fill-in-the-blank-generation","title":"Pre-training: C4 Dataset + Fill-in-the-Blank Generation","text":"<ul> <li>Uses a corrupt-and-reconstruct pre-training objective</li> <li>Learns both contextual understanding and sequence generation</li> <li>Trained on C4 (Colossal Cleaned Crawled Corpus)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Translation: Understands bidirectional input, generates fluent target text</li> <li>Summarization: Converts long passages into concise summaries</li> <li>Question Answering: Context-aware, generative answers</li> <li>Keyword Generation: Contextual phrase extraction</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#product-evolution-table","title":"Product Evolution Table","text":"Model Architecture Strengths Use Cases Developer T5-Base Encoder-Decoder Multitask learning, flexible Translation, QA, summarization Google AI mT5 Encoder-Decoder Multilingual model (100+ langs) Cross-lingual NLP Google AI FLAN-T5 Enc-Dec + Tuning Instruction tuning Zero-shot &amp; few-shot NLP Google Research UL2 Encoder-Decoder Supports multiple objective modes General-purpose transformer Google DeepMind Gemini 1.5 Multimodal Unified vision + text + code Multimodal reasoning, generation Google DeepMind"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#takeaways","title":"Takeaways","text":"<ul> <li>T5 demonstrates the power of a unified framework in solving diverse NLP tasks</li> <li>Its design has influenced instruction-tuned and multimodal model families</li> <li>Continues to power a range of Google products and NLP pipelines</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google Research)</li> <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach (Meta AI)</li> <li>DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Microsoft)</li> <li>DistilBERT by Hugging Face (Model Page)</li> <li>mBERT: Multilingual BERT (Google AI)</li> <li>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</li> <li>FLAN-T5 Instruction-Tuned Models (Google Research)</li> <li>UL2: Unified Language Learning</li> <li>Gemini 1.5 Model Overview (Google DeepMind)</li> <li>GPT-4.5 and GPT-5 Updates (OpenAI Blog)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Key Concepts</li> <li>Tokenizer and Embeddings</li> <li>Masked Language Modeling</li> <li>Semantic Search Engine</li> <li>Model Evolution Table</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#course-overview","title":"Course Overview","text":"<p>This section focuses on transitioning from theoretical knowledge of transformer models to their practical implementation and engineering components, emphasizing real-world applications such as semantic search and embedding usage.</p> <ul> <li>Prepares learners to apply transformer embeddings for NLP tasks</li> <li>Covers tokenization, embeddings, model internals, and downstream tasks</li> <li>Includes practical hands-on coding with Hugging Face Transformers and PyTorch</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#key-concepts","title":"Key Concepts","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#transformer-engineering-focus","title":"Transformer Engineering Focus","text":"<ul> <li>Embeddings: Represent words/sentences as dense vectors for downstream processing</li> <li>Tokenization: Converts raw text to token IDs; includes handling special tokens</li> <li>Attention Mechanism: Key to contextual representation in transformers</li> <li>Model Inputs: Includes token IDs, attention masks, and token type IDs</li> <li>Sentence Transformers: Fine-tuned models for capturing sentence-level semantics</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tokenizer-and-embeddings","title":"Tokenizer and Embeddings","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tokenization-pipeline","title":"Tokenization Pipeline","text":"<ul> <li>Tokenizers split sentences into subword tokens</li> <li>Maintains a vocabulary of ~30k+ tokens</li> <li>Returns token IDs, attention masks, and token type IDs</li> <li>Important to use model-specific tokenizers for consistency</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#embeddings","title":"Embeddings","text":"<ul> <li>Token IDs are converted to high-dimensional vectors</li> <li>Two key outputs:</li> <li>Last Hidden State: Embeddings for individual tokens (shape: seq_len \u00d7 hidden_dim)</li> <li>Pooled Output: Embedding for the entire sequence, used in classification</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_1","title":"Try It Yourself","text":"<p>You can run and explore the notebook directly in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#semantic-distance","title":"Semantic Distance","text":"<ul> <li>Embeddings compared using cosine similarity</li> <li>Allows words with different meanings (e.g., \"fly\") to be distinguished contextually</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>Pretraining task for models like BERT</li> <li>Random tokens replaced with <code>[MASK]</code> and predicted by the model</li> <li>Output logits converted to probabilities via softmax</li> <li>Used to help the model build a strong language understanding foundation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#example","title":"Example","text":"<ul> <li>Input: <code>\"I want to [MASK] pizza for tonight\"</code></li> <li>Output: <code>\"have\"</code>, <code>\"get\"</code>, <code>\"eat\"</code>, <code>\"make\"</code>, <code>\"order\"</code> as top predictions</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_2","title":"Try It Yourself","text":"<p>You can experiment with the code by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#semantic-search-engine","title":"Semantic Search Engine","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#goal","title":"Goal","text":"<p>Build a semantic search engine that finds the most relevant document to a query based on meaning, not keyword match.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tools-dataset","title":"Tools &amp; Dataset","text":"<ul> <li>Dataset: Multi-News (2000 article summaries)</li> <li>Model: SentenceTransformer for lightweight sentence embeddings (384-dim)</li> <li>Libraries: Hugging Face Transformers, PyTorch, Pandas</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#process","title":"Process","text":"<ul> <li>Embed all documents once</li> <li>Embed user\u2019s query</li> <li>Compute cosine similarity between query and all document embeddings</li> <li>Retrieve top-k relevant results using <code>torch.topk</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#example-queries","title":"Example Queries","text":"<ul> <li>\"Artificial Intelligence\": returned AI-related articles</li> <li>\"Natural Disasters\": returned disaster-related summaries</li> <li>\"Law Enforcement\", \"Politics\": worked as expected</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_3","title":"Try It Yourself","text":"<p>Give it a try by opening the interactive Google Colab notebook below:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#model-evolution-table","title":"Model Evolution Table","text":"Model Name Category Architecture Strengths Ideal Use Cases Latest Version Info BERT Encoder-only Transformer Bidirectional context, strong understanding Text classification, Q&amp;A, embedding generation BERT-Base / BERT-Large GPT Decoder-only Transformer Text generation, instruction following Chatbots, creative writing, code generation GPT-4o (June 2024) T5 Encoder-Decoder Transformer Unified text-to-text architecture Translation, summarization, Q&amp;A T5.1.1, Flan-T5 Gemini Multi-modal Transformer + Vision + Memory Text + image processing, powerful LLM+VLM hybrid Multi-modal tasks, agentic reasoning Gemini 1.5 (June 2025) SentenceTransformer Encoder-only Siamese / Bi-encoder Transformer Sentence similarity, semantic search Embedding generation, retrieval, clustering <code>all-MiniLM-L6-v2</code> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#references-further-exploration","title":"References &amp; Further Exploration","text":"<ul> <li>\ud83e\udd17 Hugging Face Models and Tools</li> <li>BERT (bert-base-uncased)</li> <li>GPT-2 (gpt2)</li> <li>T5 (t5-base)</li> <li>SentenceTransformer (all-MiniLM-L6-v2)</li> <li> <p>T5-based Amazon Product Review Generator by TheFuzzyScientist</p> </li> <li> <p>\ud83d\udcd3 Colab Notebooks (Used in This Module)</p> </li> <li>Tokenizer &amp; Embeddings Colab</li> <li>Masked Language Modeling (MLM) Demo</li> <li>Semantic Search with Transformers</li> <li> <p>Tokenizer Pipeline Walkthrough</p> </li> <li> <p>\ud83d\udcda Further Reading</p> </li> <li>Attention Is All You Need (Vaswani et al.)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Hugging Face Transformers Documentation</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Part 1: NLP &amp; Transformer Fundamentals</li> <li>Part 2: Practical LLM Applications</li> <li>Model Comparison Summary</li> <li>Key Takeaways</li> <li>References &amp; Further Exploration</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#course-overview","title":"Course Overview","text":"<p>This course is a hands-on introduction to transformer-based language models, combining theoretical foundations with practical implementations. The curriculum covers BERT, GPT, and T5 models, including their use in real-world NLP tasks.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#part-1-nlp-transformer-fundamentals","title":"Part 1: NLP &amp; Transformer Fundamentals","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#historical-phases-of-nlp","title":"Historical Phases of NLP","text":"<ul> <li>Rule-Based Systems: Manually defined linguistic rules</li> <li>Statistical Methods: Word co-occurrence and probabilistic models</li> <li>Machine Learning: Feature-based methods (e.g., SVM, Naive Bayes)</li> <li>Deep Learning: Dense vector embeddings and neural models</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#core-transformer-concepts","title":"Core Transformer Concepts","text":"<ul> <li>Attention Mechanism: Enables global contextual representation</li> <li>Tokenization: Breaks text into subwords with positional info</li> <li>Encoder-Decoder: Structure used in models like T5, BART</li> <li>Fine-tuning: Adjusts pretrained models for specific tasks</li> </ul> <p>\ud83d\udd17 Reference: Illustrated Transformer \u2013 Jay Alammar</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#part-2-practical-llm-applications","title":"Part 2: Practical LLM Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#bert-extractive-question-answering","title":"\ud83d\udfe2 BERT \u2013 Extractive Question Answering","text":"<ul> <li>Extracts an answer span from context using start and end logits</li> <li>Ideal for closed-domain QA</li> <li>Handles context chunks using stride</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: bert-base-uncased \ud83d\udcc4 Paper: BERT: Pre-training of Deep Bidirectional Transformers</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#gpt-instruction-following-generation","title":"\ud83d\udd35 GPT \u2013 Instruction-Following Generation","text":"<ul> <li>Trained using causal language modeling</li> <li>Uses instruction + response prompts</li> <li>Fine-tuned with Open-Instruct dataset</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: DiabloGPT on Hugging Face \ud83d\udcc4 Paper: GPT-2 \ud83d\udcc4 Dataset: Open-Instruct</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#t5-text-to-text-product-review-generation","title":"\ud83d\udd34 T5 \u2013 Text-to-Text Product Review Generation","text":"<ul> <li>Treats all tasks as text-to-text (e.g., <code>summarize:</code> or <code>translate:</code>)</li> <li>Pretrained on C4 corpus with span corruption</li> <li>Ideal for summarization, QA, translation, and generation</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: T5-base, Amazon Review Model \ud83d\udcc4 Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#model-comparison-summary","title":"Model Comparison Summary","text":"Model Architecture Directionality Pretraining Task Ideal Use Cases Limitations BERT Encoder-only Bidirectional Masked Language Modeling QA, classification, embeddings 512-token limit GPT-2 Decoder-only Unidirectional Causal Language Modeling Instruction generation, chatbots No bidirectional context T5 Encoder-Decoder Bi/Uni (input/output) Span corruption (text-to-text) Summarization, QA, translation Needs task-specific prompt Gemini Multi-modal Flexible MoE + RLHF + VLM Multimodal generation, reasoning Closed-source"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Leverage pretrained models to reduce time and cost</li> <li>Use token chunking and stride for input limits</li> <li>Even small models like GPT-2 perform well when fine-tuned</li> <li>T5\u2019s text-to-text design enables flexibility across tasks</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#references-further-exploration","title":"References &amp; Further Exploration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#foundational-papers","title":"\ud83e\udde0 Foundational Papers","text":"<ul> <li>Attention Is All You Need</li> <li>BERT: Pre-training of Deep Bidirectional Transformers</li> <li>GPT-2</li> <li>T5</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#hugging-face-models","title":"\ud83e\udd17 Hugging Face Models","text":"<ul> <li>bert-base-uncased</li> <li>gpt2</li> <li>t5-base</li> <li>DiabloGPT</li> <li>Amazon T5 Review Model</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#colab-notebooks","title":"\ud83e\uddea Colab Notebooks","text":"<ul> <li>BERT QA</li> <li>GPT Instruction Tuning</li> <li>T5 Product Review Generator</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>What is a Large Language Model?</li> <li>Decoder-Only Architecture</li> <li>Chat Templates &amp; Structured Inputs</li> <li>Model Selection on Hugging Face</li> <li>Code Demonstration: TinyLlama</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#course-overview","title":"Course Overview","text":"<p>This section explores Large Language Models (LLMs) built on the transformer architecture, their training procedures, deployment challenges, and how they are applied in real-world interactive systems. The goal is to bridge conceptual understanding with hands-on implementation.</p> <ul> <li>Core Focus:</li> <li>Decoder-only transformer models</li> <li>Tokenization &amp; input formatting</li> <li>Reinforcement Learning from Human Feedback (RLHF)</li> <li>Chat templates</li> <li>Model selection and generation parameters</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>LLMs refer to powerful NLP models capable of generating complex, human-like text. They\u2019re built using decoder-only transformer architectures and trained at scale using massive datasets.</p> <ul> <li>Scale:</li> <li>Models like LLaMA-3 and GPT-4 have up to 70+ billion parameters.</li> <li>Small LLMs (e.g., 2\u20137B) are optimized for consumer hardware.</li> <li>Architecture:</li> <li>Modern LLMs are generally decoder-only models.</li> <li>Capabilities:</li> <li>High factual recall</li> <li>Scalable deployment</li> <li>Robust contextual understanding</li> <li>Challenges:</li> <li>Hallucination</li> <li>Deployment complexity</li> <li>High compute requirements</li> </ul> <p>\ud83e\udde0 Despite limitations, they\u2019ve surpassed average human factual knowledge.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#decoder-only-architecture","title":"Decoder-Only Architecture","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#key-properties","title":"Key Properties","text":"<ul> <li>LLMs process inputs as a single concatenated sequence.</li> <li>Interaction is simulated using autoregession, where the model predicts the next token.</li> <li>Requires clever input formatting to mimic input-output behavior.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#fine-tuning-techniques","title":"Fine-tuning Techniques","text":"<ul> <li>Supervised Fine-Tuning: Uses input-response pairs to guide expected outputs.</li> <li>Reinforcement Learning from Human Feedback (RLHF):</li> <li>Multiple responses are generated.</li> <li>Human annotators rank responses.</li> <li>Used to improve contextual accuracy and helpfulness.</li> </ul> <p>\ud83d\udcca Illustration:</p> <p> Source: Jay Alammar\u2019s GPT2 visual guide</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#applications","title":"Applications:","text":"<ul> <li>Chatbots  </li> <li>Code generation  </li> <li>Instruction following  </li> <li>Document summarization</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#chat-templates-structured-inputs","title":"Chat Templates &amp; Structured Inputs","text":"<p>LLMs simulate dialogue using chat templates that structure user-assistant messages.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#input-structure","title":"Input Structure","text":"<ul> <li>A \"conversation\" is a series of messages with:</li> <li><code>role</code>: Identifies speaker (user/assistant)</li> <li><code>content</code>: Message text</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#model-specific-template-examples","title":"Model-specific Template Examples","text":"Model Structure Type Special Tokens Role Awareness Instruction Capable Blenderbot Basic concat \u274c \u274c \u274c Mistral Instruction tokens \u2705 \u26a0\ufe0f (Partial) \u2705 Gemma Turn-based format \u2705\u2705 \u2705 \u2705\u2705 LLaMA 3 Header tokens \u2705\u2705\u2705 \u2705 \u2705\u2705\u2705 <p>\ud83e\udde9 These templates are essential during fine-tuning to teach models interaction patterns.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#model-selection-on-hugging-face","title":"Model Selection on Hugging Face","text":"<p>\ud83d\udee0\ufe0f Choosing the right LLM impacts performance, cost, and resource needs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#what-to-look-for","title":"What to Look For:","text":"<ul> <li>Model Family: LLaMA, Mistral, Phi, Gemma, etc.</li> <li>Size (Parameters):</li> <li>Small: 2B\u20137B</li> <li>Medium: 13B\u201334B</li> <li>Large: 70B+</li> <li>Instruction-Following:</li> <li>Look for <code>instruct</code> or <code>chat</code> variants</li> <li>Context Length:</li> <li>Defined via <code>max_position_embeddings</code> in <code>config.json</code></li> <li>Affects how much prompt+response can be handled</li> </ul> <p>\ud83d\udca1 Hugging Face Model Hub: \ud83d\udd17 Browse Models</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#code-demonstration-tinyllama","title":"Code Demonstration: TinyLlama","text":"<p>\ud83d\udc49 Open in Colab </p> <p>We explore TinyLlama to demonstrate basic generation and parameter tuning.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#workflow","title":"Workflow","text":"<ul> <li>Load model + tokenizer  </li> <li>Prepare chat messages using templates  </li> <li>Encode as tokens  </li> <li>Generate response  </li> <li>Decode and analyze output  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#key-generation-parameters","title":"Key Generation Parameters","text":"Parameter Description <code>max_new_tokens</code> Limits length of generated response <code>temperature</code> Controls creativity/randomness (higher = more) <code>top_p</code> Nucleus sampling: restricts to top % of prob. <code>do_sample</code> Enables randomness in output <p>\ud83d\udfe2 Temperature Examples:</p> <ul> <li>1.0 \u2192 Creative, varied responses  </li> <li>0.1 \u2192 Deterministic, factual outputs  </li> </ul> <p>\ud83d\udcce Prompt token count impacts total input length (important for context fitting).</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udcdc Attention Is All You Need (Vaswani et al.)</li> <li>\ud83e\udd17 Hugging Face Transformers Documentation</li> <li>\ud83d\uddbc\ufe0f Jay Alammar\u2019s Illustrated Transformer</li> <li>\ud83e\udde0 LLaMA 3 on Hugging Face</li> <li>\ud83d\udcd8 RLHF Explained \u2013 Hugging Face Blog</li> <li>\ud83d\udcc4 OpenAI: ChatGPT Fine-tuning Guide</li> <li>\ud83d\udcda Gemma Tokenizer Guide \u2013 Google</li> <li>\ud83d\udd2c Microsoft Phi Models on Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/","title":"LLM Prep","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Comprehensive Dive into Sequence Length</li> <li>Token Counts: Practical Intuition &amp; Impact</li> <li>Precision Matters: Numerical Precision in Training</li> <li>Navigating GPU Selection: A Guide to Hardware Platform</li> <li>Practice Fundamentals: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 2: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 3: Most Basic Form of Training LLMs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#comprehensive-dive-into-sequence-length","title":"\ud83d\udcd8 Comprehensive Dive into Sequence Length","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Sequence Length </li> <li>Why Sequence Length Matters </li> <li>Hardware Implications </li> <li>Impact on Task Suitability </li> <li>Use Cases: Short vs Long Sequences </li> <li>Guidelines for Choosing Sequence Length </li> <li>References &amp; Further Reading </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#understanding-sequence-length","title":"Understanding Sequence Length","text":"<p>In this foundational lesson, we examine the concept of sequence length in large language models (LLMs), particularly as it relates to fine-tuning and model design. Sequence length determines how much context a model can consider during training and inference. Once a model is trained with a specific maximum sequence length, this cannot be extended without retraining.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Sequence Length defines the number of tokens a model can process at once.  </li> <li>This parameter is fixed after pretraining.  </li> <li>You can feed shorter inputs into a longer-trained model, but not vice versa.  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#why-sequence-length-matters","title":"Why Sequence Length Matters","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#fixed-architecture-constraint","title":"Fixed Architecture Constraint","text":"<ul> <li>Pretraining fixes the maximum window size (e.g., 4K, 8K, 16K tokens).  </li> <li>Longer contexts require higher computational resources and memory.  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#unidirectional-compatibility","title":"Unidirectional Compatibility","text":"<ul> <li>Models trained with longer windows can handle shorter inputs effortlessly.  </li> <li>Short-window models cannot be upgraded to handle longer contexts post hoc.  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#hardware-implications","title":"Hardware Implications","text":"<p>Sequence length directly impacts training and inference costs:</p> <ul> <li>Longer sequence lengths = higher VRAM requirements </li> <li>Training larger windows is exponentially slower </li> <li>Even inference (e.g., chatbots) demands more memory with longer inputs  </li> </ul> <p>Modern workarounds:</p> <ul> <li>Sparse Attention (e.g., Longformer, BigBird)  </li> <li>Memory-augmented transformers (e.g., Transformer-XL)  </li> </ul> <p>These techniques allow partial mitigation of the cost explosion from large contexts.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#impact-on-task-suitability","title":"Impact on Task Suitability","text":"<p>The sequence length determines the range and complexity of tasks that LLMs can solve. Below is a breakdown:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#short-sequences-128-512-tokens","title":"Short Sequences (128 - 512 tokens)","text":"<ul> <li>\u2705 Sentiment Analysis  </li> <li>\u2705 Language Detection  </li> <li>\u2705 Named Entity Recognition  </li> </ul> <p>Advantages:</p> <ul> <li>Faster training  </li> <li>Lower compute overhead  </li> <li>Context is usually local and easily chunkable  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#long-sequences-2048-tokens","title":"Long Sequences (2048+ tokens)","text":"<ul> <li>\u2705 Long-form QA  </li> <li>\u2705 Document Summarization  </li> <li>\u2705 Scriptwriting / Story Generation  </li> <li>\u2705 Multi-turn Dialogue Systems  </li> </ul> <p>Advantages:</p> <ul> <li>Maintains global context  </li> <li>Enables high-fidelity content generation  </li> <li>Suitable for documents, books, and extended chat history  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#use-cases-short-vs-long-sequences","title":"Use Cases: Short vs Long Sequences","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#short-sequence-use-cases","title":"Short Sequence Use Cases","text":"<ul> <li>Sentiment Analysis: Determine tone from key phrases  </li> <li>NER: Recognize entities within short contexts  </li> <li>Language Identification: Detect language using just a few words  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#long-sequence-use-cases","title":"Long Sequence Use Cases","text":"<ul> <li>Conversational AI: Maintain long-term context across multiple exchanges  </li> <li>Content Generation: Write consistent long-form narratives or reports  </li> <li>Document Understanding: Answer questions or summarize content from full documents  </li> </ul> <p>\ud83d\udd39 These applications demonstrate the practical trade-offs of context length in fine-tuning.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#guidelines-for-choosing-sequence-length","title":"Guidelines for Choosing Sequence Length","text":"Task Type Suggested Sequence Benefits Limitations Classification (Sentiment, NER) 128 - 512 tokens Efficient, fast inference Limited to local context Chatbots / Assistants 2048 - 8192 tokens Maintains conversational coherence Higher cost and latency Summarization 4096 - 16000 tokens Holistic document understanding Truncation risk if too short Code Generation 2048 - 8192 tokens Handles longer code blocks Needs longer memory if multi-file <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udd17 Attention Is All You Need (Vaswani et al.) </li> <li>\ud83d\udd17 Hugging Face Transformers Documentation </li> <li>\ud83d\udd17 Jay Alammar: Illustrated Transformer </li> <li>\ud83d\udd17 Google Research: Efficient Transformers </li> <li>\ud83d\udd17 OpenAI: Scaling Laws for Neural Language Models </li> <li>\ud83d\udd17 Facebook AI: Long-Range Arena Benchmark </li> <li>\ud83d\udd17 NVIDIA Megatron-LM Documentation </li> <li>\ud83d\udd17 Microsoft DeepSpeed for Long Sequence Training </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#token-counts-practical-intuition-impact","title":"\ud83d\udcd7 Token Counts: Practical Intuition &amp; Impact","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Tokenization Mechanics</li> <li>Tokenizer Comparisons</li> <li>Vocabulary Size &amp; Token Efficiency</li> <li>Model Comparison Table</li> <li>Colab Demo</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview","title":"Overview","text":"<p>This lesson introduces the practical impact of token counts in generative AI, with hands-on comparisons between various tokenizer behaviors and model capacities. By examining real-world input (e.g., Wikipedia pages), learners gain intuition about:</p> <ul> <li>Sequence length constraints in models like BERT, LLaMA, and Mistral.</li> <li>Vocabulary size trade-offs.</li> <li>Tokenization efficiency and its effect on model performance and training design.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#tokenization-mechanics","title":"Tokenization Mechanics","text":"<ul> <li>Text tokenization converts raw text into input IDs and attention masks.</li> <li>Input IDs directly affect the maximum context size a model can handle.</li> <li>For example, the phrase <code>\"fuzzy scientist\"</code> gets broken into only 5 tokens by the LLaMA3 tokenizer.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#case-study-wikipedia-paragraph-on-whales","title":"Case Study: Wikipedia Paragraph on Whales","text":"<ul> <li>~170 words = ~300 tokens (using LLaMA3).</li> <li>Word-to-token ratio is approximately 1.76x.</li> <li>Demonstrates how even short paragraphs can consume large portions of traditional transformer limits.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#tokenizer-comparisons","title":"Tokenizer Comparisons","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#same-paragraph-different-tokenizers","title":"Same Paragraph, Different Tokenizers:","text":"<ul> <li>BERT: ~20 more tokens than LLaMA3.</li> <li>Mistral: ~30 more than BERT, ~50 more than LLaMA3.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#full-section-1000-tokens","title":"Full Section (~1,000 tokens):","text":"<ul> <li>Fits within LLaMA3 and Mistral (8K\u201332K context sizes).</li> <li>Exceeds BERT\u2019s 512-token limit.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#entire-wikipedia-page","title":"Entire Wikipedia Page:","text":"<ul> <li>LLaMA3: ~21,000 tokens</li> <li>Mistral: ~26,000 tokens</li> </ul> <p>\ud83d\udfe2 Mistral fits due to 32K context, but is near capacity.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vocabulary-size-token-efficiency","title":"Vocabulary Size &amp; Token Efficiency","text":"Model Vocabulary Size Tokens Needed for Wiki Page Notes LLaMA3 128,000 21,000 More efficient; fewer tokens per input Mistral 32,000 26,000 Less efficient but smaller vocab size <ul> <li>Trade-off:</li> <li>Larger vocab \u2192 fewer tokens \u2192 more efficient inference</li> <li> <p>Smaller vocab \u2192 easier pretraining \u2192 more tokens used</p> </li> <li> <p>Tokenizer Strategy:</p> </li> <li>LLaMA3: Breaks input into fewer, more specific tokens.</li> <li>Mistral: Uses more tokens to represent same input.</li> </ul> <p>\ud83d\udccc Takeaway: Vocabulary size directly impacts token efficiency, model generalization, and context window utilization.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-comparison-table","title":"Model Comparison Table","text":"Model Directionality Max Context Length Vocab Size Token Efficiency Use Case Fit BERT Encoder-only 512 tokens ~30K \ud83d\udd34 Low QA, embeddings LLaMA3 Decoder-only 8K \u2013 128K tokens 128K \ud83d\udfe2 High Chat, summarization Mistral Decoder-only 32K tokens 32K \ud83d\udfe1 Medium Long-form generation <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-demo","title":"Colab Demo","text":"<p>\ud83d\udc49 Open in Colab </p> <p>This exercise allows you to: - Load LLaMA3, Mistral, and BERT tokenizers - Input arbitrary text (e.g., Wikipedia) and compare token counts - Explore vocabulary-driven tokenization behaviors</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>\u201cAttention Is All You Need\u201d \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>LLaMA3 Model Card (Hugging Face)</li> <li>Mistral Model Card (Hugging Face)</li> <li>Google Research on Subword Tokenization</li> <li>NVIDIA LLM Efficiency Resources</li> <li>Facebook AI Blog \u2013 Mistral Architecture Insights</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-matters-numerical-precision-in-training","title":"\ud83d\udcd9 Precision Matters: Numerical Precision in Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Numerical Precision in Machine Learning</li> <li>Precision Formats Explained</li> <li>Model Size, Memory, and Precision Trade-offs</li> <li>Hardware Limitations and Use Cases</li> <li>Lower Precision Formats (INT8, 4-bit)</li> <li>Precision vs Speed: Hardware Implications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_1","title":"Overview","text":"<p>This lesson explores the role of numerical precision in training and deploying large language models (LLMs). Understanding how floating-point representations affect performance, memory efficiency, and hardware compatibility is crucial when working with multi-billion parameter models.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#numerical-precision-in-machine-learning","title":"Numerical Precision in Machine Learning","text":"<ul> <li>In ML, parameters (weights) are represented as floating-point numbers.</li> <li>Common format: float32 (32-bit), offering high accuracy but high memory cost.</li> <li>Trade-off: higher bit precision = better accuracy but slower training &amp; higher memory.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-formats-explained","title":"Precision Formats Explained","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#float32-fp32","title":"\ud83d\udd35 Float32 (FP32)","text":"<ul> <li>32 bits per number \u2192 4 bytes</li> <li>High accuracy</li> <li>High memory usage</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#float16-fp16-mixed-precision","title":"\ud83d\udfe2 Float16 (FP16) / Mixed Precision","text":"<ul> <li>16 bits per number \u2192 2 bytes</li> <li>Slight loss of precision, but enables:</li> <li>Half the memory</li> <li>Double computation speed (if hardware supports)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#bf16-brain-floating-point-16","title":"\ud83d\udfe1 BF16 (Brain Floating Point 16)","text":"<ul> <li>Also 16-bit, optimized for machine learning</li> <li>Better gradient/weight representation</li> <li>Widely adopted in modern models</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-size-memory-and-precision-trade-offs","title":"Model Size, Memory, and Precision Trade-offs","text":"Precision Bits Bytes 8B Param Model Memory 70B Param Model Memory FP32 32 4 32 GB 280 GB FP16 16 2 16 GB 140 GB INT8 8 1 8 GB 70 GB 4-bit 4 0.5 4 GB 35 GB <p>\ud83e\udde0 Rule of Thumb: For FP16, memory = 2 \u00d7 parameter count in GB.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#hardware-limitations-and-use-cases","title":"Hardware Limitations and Use Cases","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-gpus","title":"Consumer GPUs","text":"<ul> <li>RTX 4090: 24GB VRAM</li> <li>Can run 8B models in inference mode using FP16</li> <li>Cannot support full training due to memory overhead</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-gpus","title":"Enterprise GPUs","text":"<ul> <li>NVIDIA A100/H100: 40\u201380GB VRAM</li> <li>Can train 7\u201313B parameter models with FP16</li> <li>Need multi-GPU setups for models \u226570B</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lower-precision-formats-int8-4-bit","title":"Lower Precision Formats (INT8, 4-bit)","text":"<ul> <li>INT8: 1 byte per param \u2192 8B model = 8 GB</li> <li>4-bit: 0.5 bytes per param \u2192 8B model = 4 GB</li> </ul> <p>\u2705 Pros: - Drastic memory savings - Enables huge models to fit on limited VRAM</p> <p>\u26a0\ufe0f Cons: - Lower precision may reduce model accuracy - Often used in inference, not training</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-vs-speed-hardware-implications","title":"Precision vs Speed: Hardware Implications","text":"<ul> <li>GPUs are optimized for FP16/FP32</li> <li>Very low-precision formats (e.g., INT4) may:</li> <li>Require internal conversions</li> <li>Lead to slower inference</li> <li>Reduce ability to utilize full GPU throughput</li> </ul> <p>\ud83d\udccc In practice: - Use FP16/BF16 for training - INT8/INT4 for memory-constrained inference</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Mixed Precision Training - NVIDIA</li> <li>Google TPU BF16 Overview</li> <li>\u201c8-Bit Optimizers via Block-wise Quantization\u201d \u2013 Dettmers et al.</li> <li>Hugging Face Guide to Quantization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>PyTorch AMP Documentation</li> <li>INT4 Quantization - Facebook AI</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#navigating-gpu-selection-a-guide-to-hardware-platform","title":"\ud83d\udcd5 Navigating GPU Selection: A Guide to Hardware Platform","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_2","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#platform-comparison","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free \\(0\u2013\\)11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_1","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-platforms","title":"Consumer Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-colab","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#runpod","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vastai","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-cloud-platforms","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lambda-labs","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#recommended-path-for-learners","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_2","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_3","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#platform-comparison_1","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free \\(0\u2013\\)11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-platforms_1","title":"Consumer Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-colab_1","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#runpod_1","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vastai_1","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-cloud-platforms_1","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lambda-labs_1","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-cloud-platform-gcp_1","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#amazon-web-services-aws_1","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#recommended-path-for-learners_1","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-part-2-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 2: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Training Control with YAML Config</li> <li>Model Setup Parameters</li> <li>Dataset &amp; Formatting Logic</li> <li>Training Configuration File (YAML)</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_4","title":"Overview","text":"<p>In this chapter, we explore how to define and control your LLM training pipeline using Axolotl's YAML-based configuration system. The focus is on training a small decoder-only model (TinyLlama) to generate short stories based on prompts using a dataset from Hugging Face.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#training-control-with-yaml-config","title":"Training Control with YAML Config","text":"<p>Axolotl leverages <code>.yml</code> configuration files to simplify LLM training orchestration. Rather than scripting logic, users can define:</p> <ul> <li>Model checkpoint and architecture</li> <li>Tokenizer type</li> <li>Dataset path and format</li> <li>Training hyperparameters</li> <li>Output and logging setup</li> </ul> <p>This allows non-programmers or fast-moving practitioners to quickly train, tune, and test models.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_3","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-setup-parameters","title":"Model Setup Parameters","text":"<p>Key fields in the YAML:</p> <ul> <li><code>base_model</code>: Pretrained checkpoint (e.g., TinyLlama 1.1B Chat)</li> <li><code>model_type</code>: Architecture class (LlamaForCausalLM)</li> <li><code>tokenizer_type</code>: Hugging Face tokenizer class (LlamaTokenizer)</li> <li><code>sequence_length</code>: Input length cap (e.g., 1024)</li> <li><code>precision</code>: Uses <code>bf16</code> (brain float 16), auto-detected if supported</li> </ul> <p>These map to common fields expected by Hugging Face models and tokenizers. Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#dataset-formatting-logic","title":"Dataset &amp; Formatting Logic","text":"<p>Dataset used: <code>jaydenccc/AI_Storyteller_Dataset</code></p> <ul> <li>Contains:</li> <li><code>synopsis</code> \u2192 serves as instruction prompt</li> <li><code>short_story</code> \u2192 target output the model learns</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#formatting","title":"Formatting","text":"<pre><code>&lt;|user|&gt;\n {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n {short_story}\n</code></pre> <ul> <li>Follows the LLaMA chat template</li> <li>Ensures correct message alignment in decoder-only models</li> <li>Axolotl handles the formatting and tokenization logic internally</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#training-configuration-file-yaml","title":"Training Configuration File (YAML)","text":"<p>Below is the <code>basic_train.yml</code> referenced in this lesson. You can include this block directly in your MkDocs site or host it as a downloadable file.</p> <pre><code># model params\nbase_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/TinyLlama_Storyteller\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nbatch_size: 4\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n</code></pre> <p>\u2705 You can also link this YAML as a raw GitHub file or store it in your docs/assets folder for users to download.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-integration","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Contains environment setup and config-based training loop</li> <li>Compatible with Colab Pro for GPU-based fine-tuning</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>Colab Exercise Notebook</li> <li>TinyLlama Model Card (Hugging Face)</li> <li>JaydenCCC AI Storytelling Dataset</li> <li>YAML Config Docs from Axolotl</li> <li>Hugging Face Transformers Docs Back to Top</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-part-3-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 3: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_7","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Starting the Training Loop</li> <li>Monitoring Loss and Epochs</li> <li>Testing the Trained Model</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_5","title":"Overview","text":"<p>This chapter concludes the first end-to-end training workflow using Axolotl and TinyLlama. We run the training script, observe the model's learning progress, and evaluate its performance with sample prompts to test generalization.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_4","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#starting-the-training-loop","title":"Starting the Training Loop","text":"<p>Once the YAML configuration file (<code>basic_train.yml</code>) is ready, you can launch training by running:</p> <pre><code>python -m axolotl.cli.train basic_train.yml\n</code></pre> <ul> <li>Loads the specified model (TinyLlama-1.1B-Chat-v1.0)</li> <li>Tokenizes dataset <code>jaydenccc/AI_Storyteller_Dataset</code></li> <li>Starts training loop with live logging</li> </ul> <p>\ud83d\udfe2 Axolotl automatically applies formatting, precision, and optimizer choices from the YAML.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#monitoring-loss-and-epochs","title":"Monitoring Loss and Epochs","text":"<ul> <li>Training loss is printed at each step due to <code>logging_steps: 1</code></li> <li>For a small dataset:</li> <li>Training is fast (few minutes with 4 epochs)</li> <li>Loss decreases with each batch, indicating effective learning</li> </ul> <p>\ud83e\udde0 Despite the minimal size of the dataset, the model learns the task effectively due to repetition and targeted prompts. Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#testing-the-trained-model","title":"Testing the Trained Model","text":"<p>Here's a sample Python test script:</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./models/TinyLlama_Storyteller\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./models/TinyLlama_Storyteller\")\n\n# Prompt: Bright student working with a fuzzy scientist\nprompt = \"&lt;|user|&gt;\nA bright student was working with the fuzzy scientist on a project.&lt;/s&gt;\n&lt;|assistant|&gt;\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=512)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#results","title":"Results:","text":"<ul> <li>The model returns coherent short stories in response to prompts like:</li> <li>\"A bright student was working with a fuzzy scientist on a project.\"</li> <li>\"A global mission for humanity through overcrowded cities.\"</li> </ul> <p>\ud83c\udfaf Even with limited training, the model generalizes narrative structure well.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_5","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-integration_1","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Covers end-to-end steps from setup to inference</li> <li>Ideal for GPU-restricted environments</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_6","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>TinyLlama Hugging Face Model Card</li> <li>Hugging Face Transformers</li> <li>JaydenCCC Storytelling Dataset</li> <li>Colab Exercise Notebook</li> <li>Accelerated LLM Training - NVIDIA</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/","title":"LLM Advanced","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Practical Limitations</li> <li>Boosting Efficiency: PeFT and LoRA in Depth</li> <li>Managing Data Memory: Batch Size &amp; Sequence Length</li> <li>Advanced Solutions: Gradient Accumulation &amp; Checkpointing</li> <li>Fitting Giants: Practical Introduction to LoRA for Large Models</li> <li>Expanding LoRA: Adapter Merging and Effective Evaluations</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#understanding-practical-limitations","title":"Understanding Practical Limitations","text":"<ul> <li>Overview</li> <li>Training Challenges with Large Models</li> <li>Memory Constraints &amp; Optimization</li> <li>Advanced Configuration Example</li> <li>Smarter Training with LoRA</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview","title":"Overview","text":"<p>This section builds upon the previous lessons by transitioning from small-scale LLMs to working with larger models like Meta\u2019s LLaMA 3.1 8B. The focus is on highlighting practical limitations and advanced optimization techniques to train large models with limited hardware resources.</p> <p>\ud83d\udc49 This tutorial used the <code>unsloth/Meta-Llama-3.1-8B-Instruct</code> base model and a custom storytelling dataset.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#training-challenges-with-large-models","title":"Training Challenges with Large Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#goals","title":"Goals:","text":"<ul> <li>Scale from small to 8B+ parameter models</li> <li>Use real-world model: <code>LLaMA 3.1 - Sloth Variant</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#observations","title":"Observations:","text":"<ul> <li>Running on consumer-grade GPU (e.g., 24GB) fails due to:</li> <li>Tokenizer misconfiguration</li> <li> <p>Out-of-memory errors even at minimal batch sizes</p> </li> <li> <p>Reducing batch size &amp; sequence length:</p> </li> <li>\u2705 Enables training</li> <li>\u274c Hurts final performance</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#memory-constraints-optimization","title":"Memory Constraints &amp; Optimization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#issues","title":"Issues:","text":"<ul> <li>Model weights, optimizer, and dataset compete for memory</li> <li>Even smallest 8B variant cannot fit fully</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#solutions-explored","title":"Solutions Explored:","text":"<ul> <li>Minimal batch size = 1</li> <li>Sequence length halved</li> <li>Still fails on common GPUs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#suggested-direction","title":"Suggested Direction:","text":"<ul> <li>Use smarter techniques:</li> <li>\ud83d\udfe2 LoRA (Low-Rank Adaptation)</li> <li>\ud83d\udfe2 Gradient Checkpointing</li> <li>\ud83d\udfe2 Mixed Precision (bf16/8bit)</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code># Source: advanced_train.yml\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type:\n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_instruction: synopsis\n      field_output: short_story\noutput_dir: ./models/Llama3_Storyteller2\nsequence_length: 1024\nmicro_batch_size: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\ngradient_checkpointing: true\n</code></pre> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#smarter-training-with-lora","title":"Smarter Training with LoRA","text":"<p>LoRA is introduced as a lightweight fine-tuning mechanism.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#benefits","title":"Benefits:","text":"<ul> <li>Reduces GPU memory footprint</li> <li>Only updates a few trainable parameters</li> <li>Compatible with large models like LLaMA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#combined-with","title":"Combined With:","text":"<ul> <li>bf16 or tf32 mixed precision</li> <li>Gradient Checkpointing</li> <li>8bit optimizer (bnb)</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Meta LLaMA 3.1 on Hugging Face</li> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar's Blog: The Illustrated Transformer</li> <li>Google Colab Guide</li> <li>OpenAI Cookbook</li> <li>NVIDIA: Memory-Efficient Training</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#boosting-efficiency-peft-and-lora-in-depth","title":"Boosting Efficiency: PeFT and LoRA in Depth","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: The Problem with Full Fine-Tuning</li> <li>Memory Usage Breakdown</li> <li>Introduction to Parameter-Efficient Fine-Tuning</li> <li>LoRA: Low-Rank Adaptation Explained</li> <li>Hyperparameters in LoRA</li> <li>Benefits and Applications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview-the-problem-with-full-fine-tuning","title":"Overview: The Problem with Full Fine-Tuning","text":"<p>Fine-tuning large language models (LLMs) often exceeds hardware capabilities due to massive memory demands. The key goals of improving training efficiency are:</p> <ul> <li>\ud83d\udfe2 Lower memory requirements.</li> <li>\ud83d\udfe2 Maintain model performance and accuracy.</li> <li>\ud83d\udfe2 Democratize access to LLM customization on limited hardware.</li> </ul> <p>Full fine-tuning is resource-intensive due to the need to update all model parameters, gradients, and optimizer states for every step.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#memory-usage-breakdown","title":"Memory Usage Breakdown","text":"<p>Training large models consumes memory across four key areas:</p> Component Description Memory Usage Model Parameters Learned weights of the model 2 \u00d7 N GB (for N billion params @ FP16) Gradients Gradients for backpropagation 2 \u00d7 N GB Optimizer States Additional states like moment estimates in Adam ~4\u20138 \u00d7 N GB Training Data Input sequences + embeddings per batch Variable (based on batch/seq length) <p>\u27a1\ufe0f Total: 8\u201312\u00d7 the model parameter memory.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#introduction-to-parameter-efficient-fine-tuning","title":"Introduction to Parameter-Efficient Fine-Tuning","text":"<p>Rather than updating all parameters, Parameter-Efficient Fine-Tuning (PEFT) updates only a targeted subset of the model. This reduces resource requirements while maintaining task-specific performance.</p> <p>One of the most prominent PEFT techniques is:</p> <ul> <li>\ud83d\udd35 LoRA (Low-Rank Adaptation) \u2014 inserts trainable, low-rank matrices into layers of a frozen pre-trained model.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-low-rank-adaptation-explained","title":"LoRA: Low-Rank Adaptation Explained","text":"<p>LoRA modifies only a small number of parameters by introducing additional matrices into each layer of the frozen model.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#key-concepts","title":"\ud83c\udfaf Key Concepts","text":"<ul> <li>Fine-tunes LLMs by updating low-rank matrices (<code>A</code> and <code>B</code>) instead of full weight matrices <code>W</code>.</li> <li>Adds <code>\u0394W = A \u00d7 B</code> to <code>W</code> during forward pass.</li> <li>Enables efficient updates and reduces memory overhead.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-parameter-and-its-usage","title":"\ud83e\udde0 LoRA - Parameter and its usage","text":"Parameter Description Influence on Memory Influence on Runtime r (rank) The rank of the LoRA matrices. Lower values reduce memory and computation cost. \u2705 Lower rank = less memory usage \u2705 Lower rank = faster computation alpha Scaling factor applied to the LoRA output. Usually alpha / r is the effective scale. \ud83d\udd01 No direct memory impact, but may influence scale of activations \u2796 May affect gradient scale, but not runtime dropout Dropout probability applied to the LoRA layers to regularize during training. \u2796 Slight additional memory usage due to dropout mask \u2796 Slight slowdown during training bias Whether to include bias terms. Can be 'none', 'all', or 'lora_only'. \u2796 Adds small amount of memory if bias is included \u2796 Minor impact if biases are added target_modules List of module names where LoRA adapters should be inserted (e.g., 'q_proj', 'v_proj'). \u2705 Selective targeting reduces memory footprint \u2705 Reduces compute by targeting specific layers merge_weights If True, merges LoRA weights with the original model weights during inference. \u2705 Merging removes need for separate LoRA weights at inference \u2705 Faster inference by removing adapter layers"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#hugging-face-arxiv","title":"\ud83d\udd17 Hugging Face + ArXiv","text":"<ul> <li>LoRA Model on Hugging Face</li> <li>Original Paper - LoRA: Low-Rank Adaptation</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#hyperparameters-in-lora","title":"Hyperparameters in LoRA","text":"Hyperparameter Role Typical Value(s) <code>rank</code> (r) Size of the low-rank matrices 8, 16, 32 <code>alpha</code> Scaling factor for \u0394W = \u03b1 \u00d7 A \u00d7 B 16 (default) <code>dropout</code> Regularization to prevent overfitting 0.0 \u2013 0.1 <code>target_modules</code> Model layers to apply LoRA (e.g., <code>query</code>, <code>value</code>, etc.) Varies <p>\ud83d\udccc Higher <code>rank</code> means better task adaptation, but at a cost to memory savings. \ud83d\udccc Lower <code>rank</code> helps prevent catastrophic forgetting during fine-tuning.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#benefits-and-applications","title":"Benefits and Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advantages-of-lora","title":"\ud83d\udfe2 Advantages of LoRA","text":"<ul> <li>Reduces GPU memory footprint by &gt;80%</li> <li>Avoids catastrophic forgetting</li> <li>Accelerates training time</li> <li>Easily pluggable into existing transformer architectures</li> <li>Works well on small datasets</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#example-use-cases","title":"\ud83d\udee0\ufe0f Example Use Cases","text":"<ul> <li>Personalizing a chatbot without retraining a full LLM</li> <li>Domain-specific adaptation (e.g., legal, healthcare)</li> <li>Multilingual extensions using LoRA adapters per language</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face PEFT Documentation</li> <li>Jay Alammar \u2013 LoRA Illustrated</li> <li>NVIDIA \u2013 Efficient Fine-Tuning Techniques</li> <li>Google \u2013 Parameter-Efficient Transfer Learning</li> <li>Meta AI Research \u2013 PEFT Methods</li> <li>OpenAI \u2013 Scaling Laws and Efficient Training</li> <li>Microsoft \u2013 LoRA Integration in DeepSpeed</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#managing-data-memory-batch-size-sequence-length","title":"Managing Data Memory: Batch Size &amp; Sequence Length","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Introduction to Memory Efficiency</li> <li>Training Analogy: Book, Pages, and Feedback</li> <li>Core Training Parameters</li> <li>Batch Size</li> <li>Sequence Length</li> <li>Trade-offs in Training Efficiency</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#introduction-to-memory-efficiency","title":"Introduction to Memory Efficiency","text":"<p>In the previous lesson, we explored LoRA (Low-Rank Adaptation) as a method to reduce memory usage during fine-tuning of Large Language Models (LLMs) by modifying only a subset of model parameters.</p> <p>In this session, we shift our focus to the memory consumption from data\u2014specifically, how the structure of the input data (e.g., batch size and sequence length) affects training efficiency, cost, and feasibility.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#training-analogy-book-pages-and-feedback","title":"Training Analogy: Book, Pages, and Feedback","text":"<p>To illustrate how models learn from data, the lesson uses an analogy:</p> <ul> <li>Book \u2192 The complete dataset</li> <li>Page \u2192 A single training example</li> <li>Reading a few pages then testing \u2192 A training step</li> <li>Reading the full book once \u2192 One epoch</li> <li>Number of pages per step \u2192 Batch size</li> <li>Words per page \u2192 Sequence length</li> </ul> <p> </p> <p>This incremental reading process enables: - Frequent model updates - Improved generalization - Reduced memory usage per training step</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#core-training-parameters","title":"Core Training Parameters","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#batch-size","title":"Batch Size","text":"<ul> <li>Determines how many training examples are processed before a model update.</li> <li>Larger batches:</li> <li>Require more memory</li> <li>Yield more accurate gradients (faster convergence)</li> <li>Are often limited by GPU/TPU capacity</li> <li>Smaller batches:</li> <li>Reduce memory consumption</li> <li>Introduce noisier gradients, which may help generalization</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#sequence-length","title":"Sequence Length","text":"<ul> <li>Number of tokens per training example.</li> <li>Longer sequences:</li> <li>Require more memory and compute</li> <li>Contain richer contextual information</li> <li>Shorter sequences:</li> <li>Allow for bigger batch sizes</li> <li>Reduce computation time</li> <li>May lack enough context for learning</li> </ul> <p>\ud83d\udcc9 Trade-off: You often reduce batch size to accommodate longer sequences within memory limits.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#trade-offs-in-training-efficiency","title":"Trade-offs in Training Efficiency","text":"<p>When training LLMs, we often cannot afford large batch sizes due to memory limits.</p> <ul> <li>Rarely does batch size get \u201ctoo large\u201d</li> <li>Commonly, batch size becomes \u201ctoo small\u201d due to memory constraints</li> <li>The key challenge: <p>How to gain the benefits of large batch training without overwhelming memory resources?</p> </li> </ul> <p>Solutions to these constraints will be covered in the next lesson.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (arXiv)</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>Hugging Face \u2013 Transformers Documentation</li> <li>OpenAI Blog</li> <li>Google Research</li> <li>Microsoft Research \u2013 DeepSpeed</li> <li>NVIDIA Developer Blog</li> <li>Facebook AI Research (FAIR)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-solutions-gradient-accumulation-checkpointing","title":"Advanced Solutions: Gradient Accumulation &amp; Checkpointing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching and Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>Efficiency Method Comparison Table</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching-and-gradient-accumulation","title":"Micro-Batching and Gradient Accumulation","text":"<p>Training large language models (LLMs) often requires careful memory management. Two primary techniques help in this regard: Micro-batching and Gradient Accumulation.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching","title":"Micro-Batching","text":"<ul> <li>A large batch is split into micro-batches to fit in limited memory.</li> <li>Each micro-batch is processed sequentially.</li> <li>Gradients are accumulated to simulate a larger batch.</li> <li>Enables efficient training on hardware with limited GPU memory.</li> </ul> <p>Effective Batch Size Formula: <pre><code>Effective Batch Size = Micro Batch Size \u00d7 Gradient Accumulation Steps \u00d7 Number of GPUs\n</code></pre></p> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing micro-batch pages filling a book one by one, representing how gradient accumulation simulates a large batch.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>Performs multiple forward/backward passes per optimization step.</li> <li>Gradients are aggregated before updating the model.</li> <li>Reduces memory usage but increases training time.</li> </ul> <p>\ud83d\udd27 Tip: Choose the largest micro-batch size that fits your GPU and increase accumulation steps only when necessary.</p> <p>\ud83d\udcc9 Example:</p> Micro Batch Size Accumulation Steps GPUs Effective Batch Size 4 2 1 8 1 16 4 64 <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-checkpointing","title":"Gradient Checkpointing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#the-problem","title":"The Problem","text":"<ul> <li>Training LLMs requires storing many activations during the forward pass.</li> <li>These activations are needed for computing gradients in the backward pass.</li> <li>Storing all of them consumes significant GPU memory.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#the-solution-checkpointing","title":"The Solution: Checkpointing","text":"<ul> <li>Gradient Checkpointing stores only selected activations during forward pass.</li> <li>During backward pass, missing activations are recomputed.</li> <li>Balances memory savings with added computation.</li> </ul> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing checkpoint blocks within the model to reduce activation memory.</p> <p>\ud83d\udcca Benefits: - Substantial memory savings - Trade-off: Slight increase in training time</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#efficiency-method-comparison-table","title":"Efficiency Method Comparison Table","text":"Method Memory Usage Training Speed Accuracy Impact Notes \ud83d\udfe2 LoRA High savings Moderate Neutral to Slight loss Trains subset of weights \ud83d\udd35 Small Batch Size Medium Faster (large batch) Neutral Limited if model needs larger batches \ud83d\udd35 Gradient Accumulation High savings Slower Slightly Better Allows larger effective batch \ud83d\udfe2 Gradient Checkpointing High savings Slower Neutral Strategic activation savings \ud83d\udd34 Mixed Precision Very High Faster (with FP16) Slight loss Risk of instability at low precision <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Attention is All You Need \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Documentation</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA \u2013 Gradient Accumulation &amp; Mixed Precision</li> <li>OpenAI Research Papers</li> <li>Microsoft DeepSpeed Memory Optimization</li> <li>Google AI Blog: Efficient Training Techniques</li> <li>Facebook AI Gradient Checkpointing</li> </ul> <p>Back to Top</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#fitting-giants-practical-introduction-to-lora-for-large-models","title":"Fitting Giants: Practical Introduction to LoRA for Large Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching &amp; Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>LoRA (Low-Rank Adaptation)</li> <li>Batch Size Trade-offs</li> <li>Mixed Precision Training</li> <li>Technique Comparison Summary</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching-gradient-accumulation","title":"Micro-Batching &amp; Gradient Accumulation","text":"<p>Micro-batching enables large batch benefits on memory-constrained hardware. By splitting a large batch into smaller \"micro-batches\", the system accumulates gradients across them before a single optimizer update.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-accumulation_1","title":"\ud83d\udd01 Gradient Accumulation","text":"<ul> <li>Forward and backward passes are done over smaller micro-batches.</li> <li>Gradients are accumulated in memory across steps.</li> <li>A single optimizer update is performed after N steps.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#formula-for-effective-batch-size","title":"Formula for Effective Batch Size","text":"\\[ \\text{Effective Batch Size} = \\text{Micro Batch Size} \\times \\text{Accumulation Steps} \\times \\text{# of GPUs} \\] <ul> <li>Example 1: Micro batch = 4, steps = 2, 1 GPU \u2192 Effective Batch Size = 8</li> <li>Example 2: Micro batch = 1, steps = 16, 4 GPUs \u2192 Effective Batch Size = 64</li> </ul> <p>\ud83d\udfe2 Pros: - Enables large effective batch sizes on small GPUs - Good generalization performance</p> <p>\ud83d\udd34 Cons: - Slower training due to repeated forward/backward passes</p> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-checkpointing_1","title":"Gradient Checkpointing","text":"<p>Gradient checkpointing saves memory by selectively storing activations during the forward pass.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#concept","title":"\u26d3\ufe0f Concept","text":"<ul> <li>Store only key activations (\"checkpoints\") during the forward pass</li> <li>During backpropagation, re-compute non-stored activations as needed</li> </ul> <p>\ud83d\udfe2 Pros: - Significant memory savings - Feasible for training large models on limited hardware</p> <p>\ud83d\udd34 Cons: - Slower training due to partial recomputation</p> <p>\ud83d\udcd8 Hugging Face Docs \ud83d\udcc4 Gradient Checkpointing ArXiv Paper</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>LoRA reduces memory usage by freezing the base model and training only a small number of injected low-rank matrices.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#how-it-works","title":"\u2699\ufe0f How It Works","text":"<ul> <li>Inject trainable low-rank adapters into transformer layers</li> <li>Only adapters are updated during fine-tuning</li> </ul> <p>\ud83d\udfe2 Memory: - Saves memory by reducing optimizer state size</p> <p>\ud83d\udd35 Speed: - Comparable to full fine-tuning</p> <p>\ud83d\udd34 Accuracy: - Can cap performance for some tasks - May help in preventing catastrophic forgetting</p> <p>\ud83d\udcd8 LoRA on Hugging Face \ud83d\udcc4 LoRA Paper on ArXiv</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#batch-size-trade-offs","title":"Batch Size Trade-offs","text":"<p>Batch size directly impacts training memory, speed, and generalization.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#trade-off-spectrum","title":"\ud83e\uddee Trade-off Spectrum","text":"<ul> <li>Small batch size \u2192 Lower memory, slower speed, better generalization</li> <li>Large batch size \u2192 Higher memory, faster training, risk of overfitting</li> </ul> <p>\ud83d\udfe2 Rule of Thumb:</p> <p>Use the largest batch size that fits in GPU memory.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Training with lower precision (e.g., FP16, INT8, 4-bit) can greatly reduce memory usage and increase speed.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#floating-point-formats","title":"\ud83e\uddca Floating Point Formats","text":"Format Memory Usage Speed Accuracy Impact FP32 High Standard None FP16 Medium Fast Minor INT8 Low Slower Moderate 4-bit Very Low Slower Noticeable <p>\ud83d\udfe2 Pros: - Huge memory savings - Speed boost on supporting hardware (e.g., A100, H100)</p> <p>\ud83d\udd34 Cons: - Minor accuracy loss at extreme bit reduction</p> <p>\ud83d\udcd8 Mixed Precision on NVIDIA \ud83d\udcd8 Hugging Face Docs</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#technique-comparison-summary","title":"Technique Comparison Summary","text":"Technique Memory Usage Speed Impact Accuracy Impact \ud83d\udfe2 LoRA Excellent Neutral Neutral/Task Dependent \ud83d\udd35 Batch Size Tuning Moderate High Task Dependent \ud83d\udfe1 Gradient Accumulation High Slightly Slower Positive \ud83d\udd34 Gradient Checkpointing High Slower Neutral \ud83d\udfe2 Mixed Precision Excellent Faster (if supported) Slightly Negative"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-configuration-example_1","title":"Advanced Configuration Example","text":"<pre><code>#Source: advanced_train2.yml\n# model params\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_Storyteller2\n\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\n\nlogging_steps: 1\n\n\n# LoRA\nadapter: lora\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\n\nlora_target_linear: true\n\n# Gradient Accumulation\ngradient_accumulation_steps: 1\n\n# Gradient Checkpointing\ngradient_checkpointing: true\n</code></pre>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Vaswani et al.)</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Gradient Checkpointing for Memory Optimization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA Mixed Precision Guide</li> <li>Hugging Face Transformers Documentation</li> <li>Google Research: Efficient Training Techniques</li> <li>OpenAI Research Blog</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#expanding-lora-adapter-merging-and-effective-evaluations","title":"Expanding LoRA: Adapter Merging and Effective Evaluations","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: Adapter Merging in LoRA</li> <li>What Is an Adapter File?</li> <li>Using <code>xolotl.merge_lora</code></li> <li>Best Practices in LoRA Fine-Tuning</li> <li>Instructional Prompting</li> <li>Effective Batch Size</li> <li>Model Evaluation and Loss Comparison</li> <li>Final Merge and Output</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview-adapter-merging-in-lora","title":"Overview: Adapter Merging in LoRA","text":"<p>In this session, we explore how to merge the lightweight adapter produced by LoRA-based training with the base model. We review best practices to optimize accuracy and stability, and correct common mistakes in the fine-tuning process.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#what-is-an-adapter-file","title":"What Is an Adapter File?","text":"<ul> <li>After LoRA training, only a small diff file is produced\u2014this is the adapter.</li> <li>It contains the modified 1% of weights from the original model.</li> <li>When merged with the base model, it reconstructs the fully fine-tuned model.</li> <li>This saves disk space and makes training more efficient.</li> </ul> <p>\ud83e\udde0 Key Concept: Adapter = Delta weights (not the full model)</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#using-xolotlmerge_lora","title":"Using <code>xolotl.merge_lora</code>","text":"<p>To merge the adapter with the base model:</p> <pre><code>from xolotl import merge_lora\n\nmerge_lora(\n  config_path=\"path/to/config.yaml\",\n  adapter_path=\"path/to/adapter\"\n)\n</code></pre> <ul> <li>The process creates a new model directory with the full merged weights.</li> <li>You can then prompt the model as usual for inference.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#best-practices-in-lora-fine-tuning","title":"Best Practices in LoRA Fine-Tuning","text":"<p>Despite a successful training, multiple common issues were present.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#instructional-prompting","title":"Instructional Prompting","text":"<p>\u274c Mistake: - No explicit task prompt was given to the instruction-following model.</p> <p>\u2705 Fix: - Add a system prompt like:</p> <pre><code>You are an amazing storyteller. From the following synopsis, write an engaging story.\n</code></pre> <ul> <li>Helps align model behavior with instruction-tuned expectations.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#effective-batch-size","title":"Effective Batch Size","text":"<p>\u274c Mistake: - Micro batch size = 4 - Gradient accumulation steps = 4 \u2192 Effective batch size = 16</p> <ul> <li>Too small for a meaningful update over a small dataset.</li> </ul> <p>\u2705 Fix: - Increase micro batch size to maximum that fits in memory - Reduce accumulation steps to speed up training</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#model-evaluation-and-loss-comparison","title":"Model Evaluation and Loss Comparison","text":"<ul> <li>After training with improved prompts and optimized batch size:</li> <li>Training ran for more steps</li> <li>Loss values were significantly lower</li> <li>Indicates improved convergence and generalization</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#final-merge-and-output","title":"Final Merge and Output","text":"<ul> <li>The merged model was tested on a story generation prompt.</li> <li>Result: A more coherent and structured output, with better alignment to storytelling instructions.</li> <li>This validates the importance of correct prompts and batch size tuning.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of LLMs (arXiv)</li> <li>Hugging Face \u2013 Parameter Efficient Fine-Tuning Guide</li> <li>OpenAI Cookbook</li> <li>Jay Alammar \u2013 Visualizing Transformers</li> <li>Google AI Blog</li> <li>XAI \u2013 Explainable AI Projects</li> <li>Microsoft Research \u2013 DeepSpeed</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/","title":"LLM Specialized","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Level-Up Giants: 8-bit Training for Massive Models</li> <li>Task-Focused Training: Aim for Better Learning - Part 1</li> <li>Task-Focused Training: Aim for Better Learning - Part 2</li> <li>Edge of Hardware Limits: Scaling Inputs with Flash Attention 2</li> <li>Edge of Hardware Limits: Reaching 4bit Training with QLoRA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#level-up-giants-8-bit-training-for-massive-models","title":"Level-Up Giants: 8-bit Training for Massive Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Training Phi-3: Scaling to Larger LLMs</li> <li>Memory Challenges and 8-Bit Quantization</li> <li>Performance, Trade-offs, and Results</li> <li>Using the SQuAD Dataset for LLM QA</li> <li>Phi-3 Configuration: <code>specialised_phi.yml</code></li> <li>Gemma 2-27B Configuration: <code>specialised_gemma.yml</code></li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-phi-3-scaling-to-larger-llms","title":"Training Phi-3: Scaling to Larger LLMs","text":"<p>This session demonstrates the training of Microsoft\u2019s Phi-3 Medium, a 14B parameter model with a 128k context window, on a single 24GB GPU using Axolotl and LoRA. Techniques such as gradient accumulation, checkpointing, and 8-bit quantization enable training without sacrificing batch size or sequence length.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#memory-challenges-and-8-bit-quantization","title":"Memory Challenges and 8-Bit Quantization","text":"<p>Despite tuning batch size and sequence length, the Phi-3 model exceeded available memory due to its ~30GB weight size. To overcome this:</p> <ul> <li>Quantization to 8-bit (<code>load_in_8bit: true</code>) was applied.</li> <li>This halved memory usage, allowing training to proceed.</li> <li>Axolotl supports this via a simple YAML flag.</li> </ul> <p>\ud83d\udca1 Note: Precision is traded for capacity; however, the gain in model size (14B vs. 7B) outweighs the loss from quantization.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#performance-trade-offs-and-results","title":"Performance, Trade-offs, and Results","text":"<ul> <li>Training succeeded with no OOM errors and room to spare.</li> <li>Sequence length and batch size compromises were reversed.</li> <li>Training was slower than LLaMA 8B due to the model's size.</li> <li>Despite quantization, Phi-3 handled the dataset well and generated fluent outputs.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#using-the-squad-dataset-for-llm-qa","title":"Using the SQuAD Dataset for LLM QA","text":"<p>The model was fine-tuned on the SQuAD (Stanford Question Answering Dataset), adapted for generative LLMs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#prompt-format","title":"Prompt Format:","text":"<pre><code>&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n</code></pre> <ul> <li>Context and question are combined into a system-level prompt.</li> <li>Model learns to generate precise answers based on instruction tuning.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#phi-3-configuration-specialised_phiyml","title":"Phi-3 Configuration: <code>specialised_phi.yml</code>","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type:\n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#gemma-2-27b-configuration-specialised_gemmayml","title":"Gemma 2-27B Configuration: <code>specialised_gemma.yml</code>","text":"<pre><code>base_model: unsloth/gemma-2-27b-it\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\noutput_dir: ./models/gemma-LongAlpaca\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: false\nload_in_4bit: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Microsoft Phi-3 Medium Model Card</li> <li>Phi-3 Technical Report (Microsoft)</li> <li>Quantization in Transformers (Hugging Face)</li> <li>Gradient Checkpointing \u2013 PyTorch Docs</li> <li>LoRA: Low-Rank Adaptation (arXiv)</li> <li>Axolotl Fine-Tuning Framework</li> <li>Unsloth \u2013 Efficient Training</li> <li>Flash Attention 2 \u2013 Paper</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#task-focused-training-aim-for-better-learning-part-1","title":"Task-Focused Training: Aim for Better Learning - Part 1","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Dataset Overview</li> <li>Data Preprocessing</li> <li>Training Configuration</li> <li>Loss Calculation Challenges</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#dataset-overview","title":"Dataset Overview","text":"<p>The lesson transitions from a simple dataset to the more complex SQuAD dataset, aiming to train a decoder-only large language model (LLM). SQuAD (Stanford Question Answering Dataset) consists of:</p> <ul> <li>Context: A passage of text</li> <li>Question: Related to the context</li> <li>Answer: A short, extractable string from the context</li> </ul> <p>Dataset Statistics: - Training Set: ~87,000 samples - Validation Set: ~10,000 samples</p> <p>\ud83d\udd17 SQuAD on Hugging Face</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#tools-used","title":"\ud83d\udd27 Tools Used","text":"<ul> <li>Python</li> <li>Hugging Face Datasets</li> <li>Pandas</li> <li>Parquet file storage</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#steps","title":"\ud83e\udde9 Steps:","text":"<ol> <li>Load SQuAD using <code>datasets</code> library.</li> <li>Convert to pandas DataFrame.</li> <li>Simplify Answers: Use only the first answer string.</li> <li>Drop unnecessary columns.</li> <li>Save the processed dataset as a <code>.parquet</code> file.</li> <li>Update the YAML configuration to use the local dataset.</li> <li>Adjust Prompts:</li> <li>Instruction: The question</li> <li>Input: The context</li> <li>Output: The answer</li> <li>System prompt: \u201cRead the following context and concisely answer my question.\u201d</li> </ol> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-configuration","title":"Training Configuration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#model-microsoftphi-3-medium-128k-instruct","title":"\ud83e\udde0 Model: <code>microsoft/Phi-3-medium-128k-instruct</code>","text":"<ul> <li>Direction: Decoder-only</li> <li>Max Token Sequence: 8,172 (extended due to load_in_8bit optimization)</li> <li>Optimizer: <code>adamw_bnb_8bit</code></li> <li>Precision: <code>bf16</code>, <code>8bit</code>, <code>tf32: false</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#lora-setup","title":"\ud83d\udee0\ufe0f LoRA Setup","text":"<ul> <li><code>adapter: lora</code></li> <li><code>r: 32</code>, <code>alpha: 16</code>, <code>dropout: 0.05</code></li> <li>Target Linear Layers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#yaml-config-snippet","title":"\ud83c\udf00 YAML Config Snippet","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nmicro_batch_size: 4\nlearning_rate: 0.0002\nadapter: lora\ngradient_checkpointing: true\nload_in_8bit: true\n\n\ud83d\udcc4 [Model on Hugging Face](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct)  \n\ud83d\udcdc [ArXiv Paper (if available)](https://arxiv.org/abs/2404.14219)  \n[Back to Top](#quick-navigation)\n\n---\n\n## Task-Focused Training: Aim for Better Learning - Part 2\n\n\n## \ud83d\udccc Quick Navigation\n\n- [Output-Focused Training](#output-focused-training)\n- [Training Behavior Comparison](#training-behavior-comparison)\n- [YAML Configuration Updates](#yaml-configuration-updates)\n- [Colab Notebook](#colab-notebook)\n- [References &amp; Further Reading](#references--further-reading)\n\n---\n\n## Output-Focused Training\n\nIn this lesson, we explore how to modify our training setup so that the model is rewarded and punished based **only** on its ability to generate the output \u2014 not the input.\n\nThis is especially important for tasks like **question answering**, where:\n- The **input (context + question)** is significantly longer than the **output (answer)**.\n- The model may waste learning capacity modeling irrelevant parts of the input.\n\n### \u2705 Key Change:\n- In the Axolotl framework, set:\n\n  ```yaml\n  train_on_inputs: false\n  ```\n\nThis disables gradient calculation over the input portion of the sequence.\n\n### \ud83d\udccc When to Use\n- Ideal for tasks with short, deterministic outputs (e.g., QA, summarization).\n- **Not recommended** for conversational datasets or tasks with intertwined dialog.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## Training Behavior Comparison\n\nTwo identical models are trained:\n- **Left**: `train_on_inputs = false` (loss computed only on output)\n- **Right**: `train_on_inputs = true` (loss computed on full input + output)\n\n### Observations:\n- Early performance is similar due to SQuAD's simplicity.\n- Models with `train_on_inputs = false` tend to:\n  - Converge **faster**\n  - Achieve **higher final accuracy**\n  - Require **fewer steps**\n  - Be **more stable** (less prone to divergence)\n\nThis adjustment improves focus and prevents overfitting to irrelevant tokens.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## YAML Configuration Updates\n\nBelow is the modified configuration snippet used for this lesson:\n\n```yaml\nbase_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\ntrain_on_inputs: false\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_1","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>Phi-3 Model on Hugging Face</li> <li>ArXiv: Phi-3 Model Paper</li> <li>Training on Outputs Only - Hugging Face Discussion</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Transformers Docs \u2013 Hugging Face</li> <li>Axolotl GitHub</li> <li>Google Colab Guide</li> </ul> <p>Back to Top</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#edge-of-hardware-limits-scaling-inputs-with-flash-attention-2","title":"Edge of Hardware Limits: Scaling Inputs with Flash Attention 2","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Scaling Model Training</li> <li>Flash Attention: Theory</li> <li>Implementing Flash Attention</li> <li>Long Context Dataset Training</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#scaling-model-training","title":"Scaling Model Training","text":"<p>In this section, the focus shifts to pushing the boundaries of what is possible in training large language models (LLMs) without upgrading hardware.</p> <p>Key goals: - Train on longer sequences - Reduce GPU memory footprint - Improve training throughput and stability</p> <p>Challenges: - Transformer attention is quadratic in complexity - Long sequences demand more compute and memory</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#flash-attention-theory","title":"Flash Attention: Theory","text":"<p>Flash Attention is a highly efficient attention mechanism designed to optimize: - Memory transfers - Computation efficiency</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#key-optimizations","title":"\ud83e\udde0 Key Optimizations:","text":"<ul> <li>Minimizes memory I/O by loading queries, keys, and values once</li> <li>Operates on GPU SRAM instead of relying on frequent memory access</li> <li>Enables longer sequence training with same hardware</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#outcomes","title":"\ud83d\udd0d Outcomes:","text":"<ul> <li>Up to 1GB memory saved</li> <li>Up to 12% training speed improvement</li> <li>Higher max sequence lengths and batch sizes achievable</li> </ul> <p>\ud83d\udcd8 Flash Attention Paper: ArXiv: 2205.14135</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#implementing-flash-attention","title":"Implementing Flash Attention","text":"<p>In the <code>Axolotl</code> framework, enabling Flash Attention is very simple:</p> <pre><code>flash_attention: true\n</code></pre> <p>Once enabled, the attention mechanism becomes more memory-efficient without any change to model architecture or tokenization.</p> <p>\ud83d\udee0\ufe0f Other Configuration Highlights: - Base Model: <code>unsloth/gemma-2-27b-it</code> - Adapter: <code>qLoRA</code> - Mixed precision: <code>bf16</code>, <code>load_in_4bit</code></p> <pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\nsequence_length: 1024\nflash_attention: true\nadapter: qlora\nload_in_4bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#long-context-dataset-training","title":"Long Context Dataset Training","text":"<p>To test Flash Attention and longer contexts, the model was fine-tuned using:</p> <p>\ud83d\uddc2\ufe0f Dataset: <code>LongAlpaca-12k</code> \ud83e\uddfe Type: Instruction-following conversations with long inputs and outputs \ud83d\udd22 Max Sequence Length: 16,000 tokens (extended from 8k) \ud83d\udce6 Micro Batch Size: 1 \ud83e\uddea Padding: Applied to ensure all sequences are uniform \ud83d\udcc9 Observation: - 5% lower memory usage - 10\u201312% training speedup</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_2","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Flash Attention (ArXiv)</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>Gemma-2-27B Model \u2013 Hugging Face</li> <li>Axolotl Fine-Tuning GitHub</li> <li>LoRA: Low-Rank Adaptation</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>Transformers Documentation \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#edge-of-hardware-limits-reaching-4bit-training-with-qlora","title":"Edge of Hardware Limits: Reaching 4bit Training with QLoRA","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview of 4-bit and qLoRA Training</li> <li>Memory Efficiency via Quantization</li> <li>Gemma-2 27B Model Training</li> <li>YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#overview-of-4-bit-and-qlora-training","title":"Overview of 4-bit and qLoRA Training","text":"<p>As we push the boundaries of training ever-larger models on limited hardware, the next evolution involves precision optimization:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#topics-covered","title":"\ud83d\udd0d Topics Covered:","text":"<ul> <li>4-bit training via double quantization</li> <li>qLoRA (quantized Low-Rank Adaptation)</li> <li>Using <code>Gemma-2-27B</code> on a 24GB GPU</li> </ul> <p>These techniques allow us to maximize model size without scaling hardware further.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#memory-efficiency-via-quantization","title":"Memory Efficiency via Quantization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#4-bit-quantization","title":"\ud83d\udcc9 4-bit Quantization","text":"<ul> <li>Reduces model weights from 16/32 bits to 4 bits</li> <li>Uses double quantization: 8-bit quantization followed by 4-bit compression</li> <li>Implements NF4 (Normalized Float) for high precision retention</li> <li>Memory savings allow fitting massive models on smaller GPUs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#qlora","title":"\ud83e\udde0 qLoRA","text":"<ul> <li>A variant of LoRA integrating quantization into adapter training</li> <li>Propagates gradients only through low-rank matrices, while freezing the backbone</li> <li>Enables training models like <code>Gemma-2-27B</code> efficiently</li> </ul> <p>\ud83d\ude80 Outcome: - Fits 27B parameters on a single 24GB GPU - Matches or exceeds performance of full-precision fine-tuning</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#gemma-2-27b-model-training","title":"Gemma-2 27B Model Training","text":"<p>We moved from Microsoft's Phi-3 to Google's <code>Gemma-2-27B</code> for long-instruction fine-tuning.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-setup","title":"\ud83e\uddfe Training Setup:","text":"<ul> <li>Model: <code>unsloth/gemma-2-27b-it</code></li> <li>Dataset: <code>LongAlpaca-12k</code></li> <li>Precision: 4-bit (<code>load_in_4bit: true</code>)</li> <li>Adapter: <code>qlora</code></li> <li>Flash Attention: Enabled</li> <li>Hardware: Single 24GB GPU</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#challenges","title":"\u26a0\ufe0f Challenges:","text":"<ul> <li>Training failed at 8-bit due to OOM</li> <li>4-bit loading + qLoRA enabled training to proceed</li> <li>Achieved full convergence and speed on low-resource setup</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\n# training setup\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\n# adapter and quantization\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nload_in_8bit: false\nload_in_4bit: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_3","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Gemma 2-27B on Hugging Face</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>LoRA: Low-Rank Adaptation</li> <li>NF4 Quantization Paper</li> <li>Transformers Documentation \u2013 Hugging Face</li> <li>Flash Attention (ArXiv)</li> <li>Axolotl Fine-Tuning GitHub</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/","title":"LLM Deployment","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Introductory Concepts for Scale Training</li> <li>Understanding DeepSpeed Theoretically</li> <li>Implementing DeepSpeed Practically</li> <li>Fully Sharded Data Parallel (FSDP) Theoretical Insights</li> <li>Applying FSDP in Practice</li> <li>Conclusion</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#introductory-concepts-for-scale-training","title":"Introductory Concepts for Scale Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_1","title":"Quick Navigation","text":"<ul> <li>Introduction to Multi-GPU Scaling</li> <li>Why Scaling is Necessary</li> <li>Strategic Goals of Scaling</li> <li>From Optimization to Hardware Expansion</li> <li>Preview of Scaling Frameworks</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#introduction-to-multi-gpu-scaling","title":"Introduction to Multi-GPU Scaling","text":"<p>In this final section, we shift our focus from single-machine optimization to scaling across multiple GPUs or nodes. While techniques such as quantization, LoRA, and flash attention helped maximize limited resources, we now explore strategies that increase total computational power by adding hardware.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#why-scaling-is-necessary","title":"Why Scaling is Necessary","text":"<p>Scaling allows us to break free from the physical constraints of a single GPU. There are two primary motivations for doing so:</p> <ul> <li>Training larger models: Eventually, no matter how optimized, a model won't fit into GPU memory.</li> <li>Faster training: Even for smaller models, scaling enables faster iteration and experimentation by parallelizing the training workload.</li> </ul> <p>State-of-the-art LLMs (like GPT-4, Gemini, Claude) are trained across hundreds to thousands of GPUs simultaneously.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#strategic-goals-of-scaling","title":"Strategic Goals of Scaling","text":"<p>Scaling isn\u2019t only about \u201cgoing bigger.\u201d It\u2019s also about going faster, or both.</p> <ul> <li>Goal 1: Train larger models that wouldn't fit in memory otherwise.</li> <li>Goal 2: Train faster, reducing the time per epoch or iteration.</li> <li>Hybrid Goal: Train moderately larger models faster, using distributed compute efficiently.</li> </ul> <p>\ud83d\udd01 The balance between size, speed, and hardware availability guides the scaling strategy.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#from-optimization-to-hardware-expansion","title":"From Optimization to Hardware Expansion","text":"<p>Previously, we relied on: - 4-bit quantization - Flash Attention - QLoRA - Efficient batch and sequence tuning</p> <p>These maximize utilization of a single GPU, often squeezing 27B+ parameter models into 24GB cards.</p> <p>Now, we explore hardware scaling, where: - Multiple GPUs share memory and gradient updates - Compute is parallelized for greater throughput</p> <p>This introduces complexity in terms of synchronization, communication overhead, and model sharding, but unlocks next-level capabilities.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#preview-of-scaling-frameworks","title":"Preview of Scaling Frameworks","text":"<p>We will explore and compare two major frameworks for distributed LLM training:</p> <ol> <li>DeepSpeed (by Microsoft):</li> <li>Efficient training of very large models</li> <li> <p>Includes ZeRO optimizer and communication-efficient primitives</p> </li> <li> <p>Fully Sharded Data Parallel (FSDP) (by PyTorch):</p> </li> <li>Parameter, gradient, and optimizer sharding</li> <li>Native PyTorch integration with strong memory savings</li> </ol> <p>Both frameworks enable: - Multi-GPU training - Reduced memory load per device - High-speed training with scalable architecture</p> <p>Next, we begin hands-on work with DeepSpeed.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>DeepSpeed Docs</li> <li>FSDP Documentation \u2013 PyTorch</li> <li>ZeRO Redundancy Optimizer (ZeRO)</li> <li>Scaling Laws for Neural Language Models</li> <li>Accelerating Training with Flash Attention</li> <li>LoRA and QLoRA Papers</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#understanding-deepspeed-theoretically","title":"Understanding DeepSpeed Theoretically","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_2","title":"Quick Navigation","text":"<ul> <li>What is DeepSpeed</li> <li>Why Use DeepSpeed</li> <li>ZeRO Optimization Stages</li> <li>Comparison of ZeRO Stages</li> <li>When to Use Each Stage</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#what-is-deepspeed","title":"What is DeepSpeed","text":"<p>DeepSpeed is an open-source deep learning optimization library developed by Microsoft. It is designed to: - Speed up training - Enable training of very large models - Efficiently utilize multiple GPUs through advanced parallelization strategies</p> <p>Key features include: - Model parallelism - Gradient and parameter partitioning - Memory and compute optimizations</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#why-use-deepspeed","title":"Why Use DeepSpeed","text":"<p>DeepSpeed is built for high-scale model training. It helps: - Distribute models across multiple GPUs - Lower memory footprint per GPU - Improve throughput in both research and production-scale workloads</p> <p>It is ideal for: - Training large LLMs efficiently - Use cases where hardware and time are critical resources</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#zero-optimization-stages","title":"ZeRO Optimization Stages","text":"<p>The core innovation of DeepSpeed is the ZeRO (ZeRO Redundancy Optimizer) framework, which is broken down into three stages:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-1-zero-1","title":"Stage 1: ZeRO-1","text":"<ul> <li>Partitions optimizer states across GPUs</li> <li>Provides slight memory savings</li> <li>Fastest among all stages</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-2-zero-2","title":"Stage 2: ZeRO-2","text":"<ul> <li>Partitions both optimizer states and gradients</li> <li>Reduces memory load further</li> <li>Slower than ZeRO-1 due to added overhead</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-3-zero-3","title":"Stage 3: ZeRO-3","text":"<ul> <li>Partitions optimizer states, gradients, and model parameters</li> <li>Offers maximum memory savings</li> <li>Slowest stage due to high communication overhead</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#comparison-of-zero-stages","title":"Comparison of ZeRO Stages","text":"Stage Speed Memory Efficiency Components Partitioned ZeRO-1 \ud83d\udfe2 Fastest \ud83d\udd34 Lowest Optimizer states ZeRO-2 \ud83d\udfe1 Moderate \ud83d\udfe1 Moderate Optimizer states + Gradients ZeRO-3 \ud83d\udd34 Slowest \ud83d\udfe2 Highest Optimizer states + Gradients + Model Parameters <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#when-to-use-each-stage","title":"When to Use Each Stage","text":"<ul> <li>ZeRO-1: Use when training time is the main constraint and memory is not a major issue.</li> <li>ZeRO-2: Use when training moderately large models with balanced speed/memory tradeoff.</li> <li>ZeRO-3: Use when maximum model size is required, even at the cost of training speed.</li> </ul> <p>These stages give developers flexibility to choose based on their needs and available hardware.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>ZeRO Paper \u2013 Optimizer Redundancy Elimination</li> <li>DeepSpeed Documentation</li> <li>PyTorch FSDP Docs (for comparison)</li> <li>Efficient Training Techniques by Microsoft</li> </ul> <p>Back to Top</p> <p>--</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#implementing-deepspeed-practically","title":"Implementing DeepSpeed Practically","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_3","title":"Quick Navigation","text":"<ul> <li>Setting Up the Environment</li> <li>Training LLaMA 3-8B Without DeepSpeed</li> <li>Enabling DeepSpeed Integration</li> <li>Running Training on Multiple GPUs</li> <li>ZeRO-1 JSON Configuration</li> <li>DeepSpeed Training YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>In this lesson, we apply DeepSpeed practically to train the <code>Meta-LLaMA-3.1-8B-Instruct</code> model using two GPUs. The goal is to optimize training speed rather than memory efficiency.</p> <ul> <li>Starting from an Axolotl-based setup</li> <li>Using the SQuAD dataset</li> <li>Measuring single-GPU vs. multi-GPU time comparisons</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#training-llama-3-8b-without-deepspeed","title":"Training LLaMA 3-8B Without DeepSpeed","text":"<p>Initial benchmark: - Run on 1 GPU - Training time: ~1 hour 40 minutes - CUDA visibility set to restrict GPU usage - Dataset: <code>squad-for-llms</code> - Model: <code>Meta-LLaMA-3.1-8B-Instruct</code></p> <p>This baseline helps quantify the gains from enabling DeepSpeed later.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#enabling-deepspeed-integration","title":"Enabling DeepSpeed Integration","text":"<p>DeepSpeed support is built into Axolotl. It provides default JSON configs for: - ZeRO-1 - ZeRO-2 - ZeRO-3</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#how-to-enable","title":"How to Enable:","text":"<ol> <li>Copy the desired config (e.g., <code>ZeRO1.json</code>) from Axolotl DeepSpeed Configs</li> <li>Set it in your training command via <code>deepspeed</code> parameter</li> <li>Launch Axolotl with multiple GPUs (remove <code>CUDA_VISIBLE_DEVICES</code> restriction)</li> </ol> <pre><code>CUDA_VISIBLE_DEVICES=0,1 accelerate launch train.py --deepspeed=ZeRO1.json ...\n</code></pre> <p>\ud83d\udfe2 Outcome: Multi-GPU training initiated with backend set to DeepSpeed</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#running-training-on-multiple-gpus","title":"Running Training on Multiple GPUs","text":"<ul> <li>Training is re-launched using both GPUs</li> <li>ZeRO-1 stage selected (focused on speed, not memory saving)</li> <li>Each GPU runs a full copy of the model</li> <li>Results:</li> <li>New training time: ~1 hour</li> <li>~40% speed improvement with 2 GPUs</li> </ul> <p>\ud83d\udd01 Although not linear, the scaling is significant and useful in production and experimentation workflows.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#zero-1-json-configuration","title":"ZeRO-1 JSON Configuration","text":"<pre><code>{\n  \"ZeRO_optimization\": {\n    \"stage\": 1,\n    \"overlap_comm\": true\n  },\n  \"bf16\": {\n    \"enabled\": \"auto\"\n  },\n  \"fp16\": {\n    \"enabled\": \"auto\",\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 32,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n  \"gradient_accumulation_steps\": \"auto\",\n  \"gradient_clipping\": \"auto\",\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"wall_clock_breakdown\": false\n}\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#deepspeed-training-yaml-configuration","title":"DeepSpeed Training YAML Configuration","text":"<pre><code>base_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_squad\n\nsequence_length: 2048\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>Axolotl DeepSpeed Configs</li> <li>Hugging Face DeepSpeed Guide</li> <li>ZeRO Optimizer Paper</li> <li>Axolotl Project</li> <li>SQuAD Dataset \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#fully-sharded-data-parallel-fsdp-theoretical-insights","title":"Fully Sharded Data Parallel (FSDP) Theoretical Insights","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>## 1. Overview of FSDP</li> <li>## 2. Why FSDP Over DDP?</li> <li>## 3. Sharding Strategies in FSDP</li> <li>## 4. Model Comparison: FSDP vs ZeRO</li> <li>## 5. When to Use FSDP</li> <li>## References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#1-overview-of-fsdp","title":"1. Overview of FSDP","text":"<p>FSDP (Fully Sharded Data Parallel) is a PyTorch-native distributed training framework developed by Meta AI. It's designed for efficient memory usage and scalability while training large models\u2014especially LLMs.</p> <ul> <li>Origin: Introduced by Meta within the PyTorch ecosystem.</li> <li>Key Use: Enables models too large for single GPU memory.</li> <li>Core Idea: Shard model weights, gradients, and optimizer states across multiple GPUs.</li> </ul> <p>\ud83e\udde9 Main Capabilities:</p> <ul> <li>Memory-efficient training via model sharding.</li> <li>Smart offloading of model shards between GPU and CPU.</li> <li>Compatible with massive models (GPT-like, BERT variants).</li> <li>Reduces per-GPU memory footprint \u2192 supports larger batch sizes.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#2-why-fsdp-over-ddp","title":"2. Why FSDP Over DDP?","text":"Technique Replicates Model Memory Pooling Model Size Suitability Use Case DDP (Distributed Data Parallel) \u2705 Full model replicated \u274c No memory pooling Medium Speed-focused training FSDP \u274c Fully sharded \u2705 Memory-efficient Large-scale Training LLMs beyond single GPU <p>\ud83d\udd0d Unlike DDP, FSDP: - Does not replicate the full model. - Shards weights across GPUs. - Optimizes both training speed and memory usage.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#3-sharding-strategies-in-fsdp","title":"3. Sharding Strategies in FSDP","text":"<p>FSDP provides 4 sharding strategies, similar in spirit to ZeRO stages (used by DeepSpeed):</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#no-sharding","title":"\ud83d\udd35 No Sharding","text":"<ul> <li>Equivalent to DDP.</li> <li>No memory savings.</li> <li>Fastest strategy (lowest overhead).</li> <li>Use if memory isn\u2019t a bottleneck.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#gradients-optimizer-sharding-similar-to-zero-stage-2","title":"\ud83d\udfe1 Gradients &amp; Optimizer Sharding (Similar to ZeRO Stage 2)","text":"<ul> <li>Gradients and optimizer states are sharded.</li> <li>Parameters still replicated.</li> <li>Balanced trade-off between speed and memory.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#hybrid-sharding","title":"\ud83d\udfe2 Hybrid Sharding","text":"<ul> <li>Shards parameters + optimizer states.</li> <li>Maintains full copy of model on each GPU for fast inference.</li> <li>Best for scenarios with frequent inference and evaluation needs.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#full-sharding-similar-to-zero-stage-3","title":"\ud83d\udd34 Full Sharding (Similar to ZeRO Stage 3)","text":"<ul> <li>Shards everything: parameters, gradients, optimizer states.</li> <li>Best memory efficiency.</li> <li>Ideal for very large models.</li> <li>Adds communication overhead \u2192 may slow down training.</li> </ul> <p>\ud83d\udcca Sharding Strategy Comparison Table:</p> Strategy Parameters Gradients Optimizer Memory Efficient Fast Inference Best For No Shard \u274c \u274c \u274c \ud83d\udd34 \ud83d\udfe2 Small models Grad+Opt \u274c \u2705 \u2705 \ud83d\udfe1 \ud83d\udfe1 Balanced setup Hybrid \u2705 \u2705 \u2705 \ud83d\udfe2 \ud83d\udfe2 Training + eval Full \u2705 \u2705 \u2705 \ud83d\udfe2\ud83d\udfe2\ud83d\udfe2 \ud83d\udd34 Huge models <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#4-model-comparison-fsdp-vs-zero","title":"4. Model Comparison: FSDP vs ZeRO","text":"Feature FSDP ZeRO (DeepSpeed) Native to PyTorch DeepSpeed/ONNX Sharding Levels Full Stage 1 to 3 CPU Offloading \u2705 \u2705 Model-Agnostic \u2705 \u2705 Hardware Compatibility Any PyTorch GPU setup Azure/AWS optimized Supported Community Meta AI Microsoft <p>\ud83d\udd0e FSDP has tighter PyTorch integration, making it easier for native PyTorch workflows.</p> <p>\ud83d\udcce FSDP on Hugging Face \ud83d\udcc4 FSDP ArXiv Paper</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#5-when-to-use-fsdp","title":"5. When to Use FSDP","text":"<p>\ud83c\udfc1 Best suited when:</p> <ul> <li>You're training models with billions of parameters.</li> <li>GPU memory is insufficient for full-model replication.</li> <li>You need to scale across 4+ GPUs with low memory usage.</li> <li>You want fine-grained control over training memory behavior.</li> </ul> <p>\ud83d\udea9 Not necessary when:</p> <ul> <li>Models fit easily on a single GPU.</li> <li>Training speed is more important than memory savings.</li> </ul> <p>\ud83e\udde0 Bonus: Even without full sharding, FSDP helps reduce memory usage to allow larger batch sizes.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li> <p>\ud83d\udcc4 FSDP: Fully Sharded Data Parallel Training (ArXiv)   The original research paper introducing FSDP, explaining its architecture, design rationale, and performance benchmarks.</p> </li> <li> <p>\ud83e\uddea Hugging Face Accelerate: FSDP Training Guide   Practical guidance on using FSDP with Hugging Face's Accelerate library for distributed training.</p> </li> <li> <p>\ud83d\udcd8 PyTorch FSDP Tutorial (Official)   Step-by-step code tutorial from the PyTorch team demonstrating how to use <code>torch.distributed.fsdp</code>.</p> </li> <li> <p>\ud83d\udcac Meta AI Engineering Blog   Technical blog posts by Meta on distributed training innovations, including the development of FSDP.</p> </li> <li> <p>\ud83e\udde0 NVIDIA Megatron-LM   One of the earliest large-scale model training toolkits that inspired advances in sharded training techniques.</p> </li> <li> <p>\ud83d\udcbb DeepSpeed ZeRO Documentation (Microsoft)   Deep dive into ZeRO stages 1\u20133, which parallel FSDP's sharding strategies in philosophy and effect.</p> </li> <li> <p>\ud83c\udfa5 YouTube \u2013 Scaling Transformers with FSDP (Meta AI)   Video walkthrough of how FSDP works and how it's applied in production-scale transformer training.</p> </li> <li> <p>\ud83d\udcda Microsoft ZeRO-3 Optimization Paper   Foundational paper on memory-optimized training with ZeRO, useful for comparing against FSDP.</p> </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#applying-fsdp-in-practice","title":"Applying FSDP in Practice","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Hardware Setup</li> <li>Configuration for Training</li> <li>FSDP Setup</li> <li>Training Execution</li> <li>Reflections &amp; Takeaways</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#course-overview","title":"Course Overview","text":"<p>In this session, we train the LLaMA 3 - 70B model using Fully Sharded Data Parallelism (FSDP) with QLoRA support on NVIDIA GPUs. The aim is to enable fine-tuning such a massive model efficiently using limited hardware (2 x NVIDIA L4 GPUs).</p> <ul> <li>Model: LLaMA 3 - 70B</li> <li>Framework: Axolotl</li> <li>Parallelism Strategy: Fully Sharded Data Parallel (FSDP)</li> <li>Quantization: 4-bit QLoRA</li> <li>GPUs used: 2 x NVIDIA L4 (24GB each)</li> <li>Cost: ~$2/hr via RunPod</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#hardware-setup","title":"Hardware Setup","text":"<ul> <li>Initial RTX 4090 setup was dropped due to poor inter-GPU communication.</li> <li>Replaced with NVIDIA L4s for better memory distribution.</li> <li>L4s provide equivalent VRAM (24GB) and are more optimal for sharded training.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#configuration-for-training","title":"Configuration for Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#key-parameters","title":"Key Parameters","text":"<pre><code>base_model: casperhansen/llama-3-70b-fp16\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\noutput_dir: ./models/llama70B-LongAlpaca\nsequence_length: 1024\npad_to_sequence_len: true\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\noptimizer: adamw_torch\nmicro_batch_size: 1\nnum_epochs: 1\nlearning_rate: 0.0002\nadapter: qlora\nload_in_4bit: true\nflash_attention: true\n</code></pre> <ul> <li>Switched from Adam8bit to <code>adamw_torch</code> for FSDP compatibility.</li> <li>Used <code>pad_to_sequence_len</code> and specified padding tokens for uniform batch processing.</li> <li>LoRA &amp; quantization settings enable memory efficiency for 70B models.</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#fsdp-setup","title":"FSDP Setup","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#configuration","title":"Configuration","text":"<pre><code>fsdp:\n  - full_shard\n  - auto_wrap\nfsdp_config:\n  fsdp_offload_params: true\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\n</code></pre> <ul> <li>Enabled <code>cpu_ram_efficient_loading</code> to reduce active memory use.</li> <li>Used <code>FULL_STATE_DICT</code> for complete checkpoint saving.</li> <li><code>auto_wrap</code> and <code>full_shard</code> ensures correct layer partitioning for LLaMA's decoder.</li> </ul> <p>Hugging Face Docs on FSDP</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#training-execution","title":"Training Execution","text":"<ul> <li>Axolotl script starts loading and quantizing the massive model (150GB in FP16).</li> <li>First-time load may take up to 40 minutes.</li> <li>Each GPU receives a different shard of the model.</li> <li>Memory utilization is optimized, loading only actively used weights.</li> <li>Training is slow but feasible with 4-bit quantization and FSDP+QLoRA.</li> </ul> <p>\ud83d\udfe2 Success: Fine-tuning a 70B LLM on only 2 x 24GB GPUs \ud83d\udd34 Tradeoff: Long training times</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#reflections-takeaways","title":"Reflections &amp; Takeaways","text":"<ul> <li>Demonstrates the power of modern tooling (Axolotl, FSDP, QLoRA).</li> <li>Scaling large models is possible without massive GPU clusters.</li> <li>Careful configuration of optimizer, batch size, memory usage, and quantization is key.</li> <li>Offers a hands-on blueprint for scaling up model training workflows.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>LLaMA 3 Hugging Face Card</li> <li>Axolotl GitHub</li> <li>FSDP Docs \u2013 Hugging Face Accelerate</li> <li>RunPod \u2013 GPU rentals</li> <li>QLoRA Paper (arXiv)</li> <li>LoRA: Low-Rank Adaptation of LLMs</li> <li>Meta\u2019s LLaMA 3 Overview</li> </ul> <p>--</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#conclusion","title":"Conclusion","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Reflection &amp; Summary</li> <li>Section 1: Foundations of LLMs</li> <li>Section 2: Preparing for Training</li> <li>Section 3: Advanced Training Techniques</li> <li>Section 4: Specialized LLM Techniques</li> <li>Section 5: Scaling LLM Training</li> <li>Section 6: Final Roadmap</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#course-reflection-summary","title":"Course Reflection &amp; Summary","text":"<p>This final lesson encapsulates our comprehensive journey through large language models (LLMs) and generative AI. From foundational knowledge to advanced techniques, each section provided practical, actionable insights for building and deploying LLMs.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-1-foundations-of-llms","title":"Section 1: Foundations of LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#topics-covered","title":"Topics Covered:","text":"<ul> <li>Understanding LLM mechanics and generation</li> <li>Reinforcement Learning with Human Feedback (RLHF)</li> <li>Input/output architecture of LLMs</li> <li>Chat template construction for structured interactions</li> <li>Model selection frameworks for different use cases</li> <li>Techniques to guide and optimize model outputs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#real-world-use-case","title":"Real-World Use Case:","text":"<p>Fine-tuning a chatbot to deliver consistent customer service responses using structured prompts.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-2-preparing-for-training","title":"Section 2: Preparing for Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Impact of sequence length on model efficiency</li> <li>Token count intuition and compression</li> <li>Numerical precision trade-offs (FP32, FP16, BF16)</li> <li>Hands-on basics of LLM training setups</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#tools","title":"Tools:","text":"<ul> <li>Tokenizers from Hugging Face</li> <li>Tensor precision profiling with PyTorch</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#use-case","title":"Use Case:","text":"<p>Training a document summarization model using low-bit precision and optimized token lengths.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-3-advanced-training-techniques","title":"Section 3: Advanced Training Techniques","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#covered-topics","title":"Covered Topics:","text":"<ul> <li>Training bottlenecks in memory and compute</li> <li>Parameter Efficient Fine-Tuning (PEFT) &amp; LoRA</li> <li>Gradient accumulation &amp; checkpointing strategies</li> <li>Adapter merging and LoRA evaluations</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-4-specialized-llm-techniques","title":"Section 4: Specialized LLM Techniques","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#techniques-explored","title":"Techniques Explored:","text":"<ul> <li>8-bit training for resource efficiency</li> <li>Task-specific output learning</li> <li>Flash Attention for memory-constrained GPUs</li> <li>4-bit quantization with QLoRA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#colab-link","title":"Colab Link:","text":"<p>\ud83d\udc49 Open in Colab</p> <p></p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#arxiv-reference","title":"ArXiv Reference:","text":"<ul> <li>QLoRA: Efficient Finetuning of Quantized LLMs</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-5-scaling-llm-training","title":"Section 5: Scaling LLM Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#focus-areas","title":"Focus Areas:","text":"<ul> <li>Multi-GPU model training</li> <li>DeepSpeed integration</li> <li>Fully Sharded Data Parallelism (FSDP)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#technologies","title":"Technologies:","text":"<ul> <li>DeepSpeed GitHub</li> <li>FSDP Docs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#use-case_1","title":"Use Case:","text":"<p>Deploying GPT-like models across A100 clusters using DeepSpeed + FSDP for enterprise-scale tasks.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-6-final-roadmap","title":"Section 6: Final Roadmap:","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face Transformers Docs</li> <li>OpenAI Cookbook</li> <li>LoRA: Parameter Efficient Fine-Tuning</li> <li>DeepSpeed</li> <li>FlashAttention Paper</li> <li>QLoRA: Quantization for LLMs</li> <li>Microsoft FSDP</li> <li>XAI Research</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/5-rag-langchain/","title":"\ud83d\udd17 RAG &amp; LangChain","text":"<p>Welcome to the RAG (Retrieval-Augmented Generation) &amp; LangChain section of the Generative AI Atlas. This module explores techniques that combine language models with external knowledge sources, along with powerful orchestration using LangChain.</p>"},{"location":"02-gen-ai-core/5-rag-langchain/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Follow the structured curriculum to learn how to build intelligent, knowledge-enhanced applications:</p> <ul> <li>RAG Fundamentals</li> <li>LLM Intro</li> <li>Vector DB &amp; Embeddings</li> <li>LangChain Simple Pipeline</li> <li>LangChain Advanced</li> <li>LangChain Projects</li> </ul>"},{"location":"02-gen-ai-core/5-rag-langchain/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Explore companion resources and deeper readings:</p> <ul> <li>Supplemental Material</li> </ul>"},{"location":"02-gen-ai-core/5-rag-langchain/#qa","title":"\u2753 Q&amp;A","text":"<p>Engage with curated questions and expert commentary from real use cases:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/","title":"\ud83d\ude80 Deployment &amp; Ops","text":"<p>\ud83d\udcc2 Navigation Tip This section focuses on operationalizing AI systems. Navigate via the sidebar to explore LLMOps best practices and AI evaluation strategies.</p> <p>\ud83d\udca1 For practical deployment knowledge, begin with LLMOps, then proceed to AI Evals for benchmarking techniques.</p>"},{"location":"03-deploy-ops/6-llmops/","title":"\u2699\ufe0f LLMOps","text":"<p>Welcome to the LLMOps section of the Generative AI Atlas. This section focuses on managing the lifecycle, deployment, and operational scaling of large language models (LLMs) in production environments.</p>"},{"location":"03-deploy-ops/6-llmops/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Explore practical strategies and technical practices for operationalizing LLMs:</p> <ul> <li>Getting Started</li> <li>Pre-Deployment</li> <li>MLOps Foundations</li> <li>Advanced Deployment</li> <li>Inference Optimization</li> <li>Economics</li> <li>Cluster Management</li> <li>Real-Time API</li> </ul>"},{"location":"03-deploy-ops/6-llmops/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Enrich your knowledge with extended materials:</p> <ul> <li>Supplemental Material</li> </ul> <p>\ud83d\udcc2 Back to Deployment &amp; Ops Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/","title":"\ud83e\uddea AI Evals","text":"<p>Welcome to the AI Evals section of the Generative AI Atlas. This section explores techniques for evaluating LLM performance across human and automated workflows, enabling robust validation and monitoring strategies.</p>"},{"location":"03-deploy-ops/7-ai-evals/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Master key evaluation frameworks and implementation practices:</p> <ul> <li>Fundamentals</li> <li>Error Analysis</li> <li>Automated Evaluators</li> <li>Architecture Strategies</li> <li>Production Monitoring</li> <li>Human Review</li> <li>Cost Optimization</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/#additional-resource","title":"\ud83d\udcda Additional Resource","text":"<p>Extended material and visual references:</p> <ul> <li>YouTube Material</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/#qa","title":"\u2753 Q&amp;A","text":"<p>Insights, reflections, and real-world challenges from practitioners:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Deployment &amp; Ops Overview</p> <p>August 02, 2025</p> <p>Back to top</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/","title":"\ud83d\udcd8 Chapter Summary: LLMs, Prompts, and Evaluation Basics","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction</li> <li>1.1 What is Evaluation?</li> <li>1.2 The Three Gulfs of LLM Pipeline Development</li> <li>1.3 Why LLM Pipeline Evaluation is Challenging</li> <li>1.4 The LLM Evaluation Lifecycle: Bridging the Gulfs with Evaluation</li> <li>1.5 Summary</li> <li>2.1 Strengths and Weaknesses of LLMs</li> <li>2.2 Prompting Fundamentals</li> <li>2.3 Defining \u201cGood\u201d: Types of Evaluation Metrics</li> <li>2.4 Foundation Models vs. Application-Centric Evals</li> <li>2.5 Eliciting Labels for Metric Computation</li> <li>2.6 Summary</li> <li>2.7 Glossary of Terms</li> <li>2.8 Exercises</li> <li>2.8.4 Eliciting Labels (Travel Assistant)</li> <li>Shankar et al 2024 2024d</li> <li>Bommasani et al 2021 and Zaharia et al 2024</li> <li>Ward and Feldstein 2024 Status Analysis</li> <li>Lian et al 2023 2025 Generative Foundation Models Handbook</li> <li>Hendrycks et al 2021 Unsolved Problems in ML Safety</li> <li>Rein et al 2024 Status Relevant Findings</li> <li>Jain et al 2025 Status Related Works</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-introduction","title":"1. Introduction","text":"<p>Rapid advances in the development and deployment of LLMs are reshaping applications such as customer service, decision support, and information extraction (Bommasani et al. 2021; Zaharia et al. 2024). However, their adoption outpaces our ability to systematically evaluate them (Ward and Feldstein 2024).</p> <p>Unlike traditional deterministic software, LLM pipelines yield subjective and context-sensitive outputs. This introduces challenges in evaluation, as conventional metrics like accuracy or F1 score may not apply.</p> <p>Core challenge: How do we evaluate whether an LLM pipeline is performing correctly, and where it might be failing?</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#11-what-is-evaluation","title":"1.1 What is Evaluation?","text":"<p>Evaluation is defined as the systematic measurement of LLM pipeline quality. Evaluations can be:</p> <ul> <li>Background Monitoring \u2013 passive observation for drift or degradation.</li> <li>Guardrails \u2013 checks that block/alter unsafe outputs.</li> <li>Improvement Tools \u2013 used to improve data labeling, few-shot examples, or prompt design.</li> </ul> <p>Evaluations guide systematic improvements by identifying failure modes, enabling trust, safety, and ongoing enhancement.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#12-the-three-gulfs-of-llm-pipeline-development","title":"1.2 The Three Gulfs of LLM Pipeline Development","text":"<p>This framework (Shankar et al. 2025; Norman, 1988) captures the key gaps:</p> <ul> <li>Gulf of Comprehension: Understanding input data and pipeline behavior.</li> <li>Gulf of Specification: Miscommunication between developer intent and prompt execution.</li> <li>Gulf of Generalization: Inconsistent LLM responses across different inputs.</li> </ul> <p></p> <p>Example: An email processing pipeline that misidentifies a public figure as a sender showcases a generalization failure.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#13-why-llm-pipeline-evaluation-is-challenging","title":"1.3 Why LLM Pipeline Evaluation is Challenging","text":"<p>Challenges include:</p> <ul> <li>The Three Gulfs resurface uniquely for each task and dataset.</li> <li>Evaluation requirements evolve with usage.</li> <li>No universal metric exists; appropriate ones must be developed per context.</li> <li>Benchmarks don't capture task-specific failure modes.</li> <li>Ground-truth inspection is still necessary for effective evaluations (Shankar et al. 2024d).</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#14-the-llm-evaluation-lifecycle-bridging-the-gulfs-with-evaluation","title":"1.4 The LLM Evaluation Lifecycle: Bridging the Gulfs with Evaluation","text":"<p>We adopt a structured approach: <code>Analyze \u2192 Measure \u2192 Improve</code></p> <ul> <li>Analyze: Inspect data and behavior to identify comprehension and specification issues.</li> <li>Measure: Use evaluators to quantitatively assess failure modes.</li> <li>Improve: Use findings to refine prompts, architectures, or training methods.</li> </ul> <p>This iterative loop builds reliable and evolving LLM systems.</p> <p></p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#15-summary","title":"1.5 Summary","text":"<ul> <li>Evaluation is essential for reliable LLMs.</li> <li>The Three Gulfs help diagnose sources of failure.</li> <li>Metrics must be context-specific and evolve with the system.</li> <li>The Analyze\u2013Measure\u2013Improve lifecycle provides a repeatable framework.</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#21-strengths-and-weaknesses-of-llms","title":"2.1 Strengths and Weaknesses of LLMs","text":"<ul> <li>Strengths: LLMs produce fluent, coherent, grammatically correct text; excel in summarization, translation, editing, etc.</li> <li>Weaknesses:</li> <li>Limited algorithmic generalization (Qian et al., 2022)</li> <li>Effective context window smaller than expected (Li et al., 2024)</li> <li>Prompt sensitivity (Sclar et al., 2024)</li> <li>Probabilistic output = inconsistent results</li> <li>Hallucination risk (Kalai &amp; Vempala, 2024)</li> </ul> <p>\ud83d\udccc Key Takeaway 2.1: Treat LLMs as powerful but fallible tools\u2014leverage their strengths while anticipating weaknesses.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#22-prompting-fundamentals","title":"2.2 Prompting Fundamentals","text":"<p>A good prompt includes: 1. Role and Objective \u2013 Define persona and goal 2. Instructions \u2013 Clear, bullet-form directives 3. Context \u2013 Include text, data, background 4. Examples \u2013 Few-shot guidance 5. Reasoning Steps \u2013 CoT (Chain of Thought) 6. Formatting Constraints \u2013 JSON, paragraph, etc. 7. Delimiters &amp; Structure \u2013 Markdown headers, tags  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_1","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#23-defining-good-types-of-evaluation-metrics","title":"2.3 Defining \u201cGood\u201d: Types of Evaluation Metrics","text":"<ul> <li>Reference-Based: Compare output to gold labels  </li> <li>Exact match, keyword check, SQL execution  </li> <li>Reference-Free: Evaluate quality without gold labels  </li> <li>Validity of code, absence of hallucination, stylistic adherence  </li> <li>Examples: LLM-generated API response uses valid schema  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_2","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#24-foundation-models-vs-application-centric-evals","title":"2.4 Foundation Models vs. Application-Centric Evals","text":"<ul> <li>Foundation Eval: Benchmarks like MMLU (Hendrycks et al., 2021), HELM, GSM8k  </li> <li>Application Eval: Tailored to specific pipeline goals like legal document fidelity  </li> </ul> <p>\ud83d\udca1 Tip: Don\u2019t blindly trust benchmarks\u2014build evals for your specific pipeline.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#25-eliciting-labels-for-metric-computation","title":"2.5 Eliciting Labels for Metric Computation","text":"<ul> <li>Direct Grading: Human/LLM scoring using rubric  </li> <li>Pairwise Comparison: A vs B  </li> <li>Ranking: A &gt; B &gt; C based on rubric dimension  </li> </ul> <p>\ud83d\udccc Key Takeaway 2.2: Combine direct grading, pairwise, and ranking for rich feedback.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_3","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#26-summary","title":"2.6 Summary","text":"<p>This chapter presents a comprehensive framework for evaluating LLMs, emphasizing: - Prompt design fundamentals - Reference-based vs reference-free metrics - Foundation vs application evaluations - Label elicitation techniques (grading, ranking)  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#27-glossary-of-terms","title":"2.7 Glossary of Terms","text":"<ul> <li>LLM: Large Language Model  </li> <li>Foundation Model: Pretrained for general use  </li> <li>Token: Unit of language  </li> <li>Prompt: Instructional input  </li> <li>Attention: Focus mechanism in Transformers  </li> <li>SFT/RLHF/DPO: Post-training strategies  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_4","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#28-exercises","title":"2.8 Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-prompt-engineering","title":"1. Prompt Engineering","text":"<ul> <li>Zero-shot: \u201cSummarize the following email\u2026\u201d  </li> <li>One-shot: Add example email and response  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-metric-classification","title":"2. Metric Classification","text":"<ul> <li>a: Reference-based  </li> <li>b: Reference-free  </li> <li>c: Reference-based  </li> <li>d: Reference-free  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#3-eval-types","title":"3. Eval Types","text":"<ul> <li>a: MMLU  </li> <li>b: Rate summaries using legal domain reviewers  </li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#284-eliciting-labels-travel-assistant","title":"2.8.4 Eliciting Labels (Travel Assistant)","text":"<p>You need to evaluate whether a travel assistant\u2019s flight recommendations respect the user\u2019s budget constraint.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#a-direct-grading-rubric","title":"(a) Direct-Grading Rubric:","text":"<ul> <li>Within Budget: All suggested flights have price \u2264 user's maxPrice.  </li> <li>Over Budget: At least one suggested flight exceeds the user's maxPrice.  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#b-pairwise-comparison-instruction","title":"(b) Pairwise-Comparison Instruction:","text":"<p>Sample Prompt:</p> <p>You are given a user query: \"Find flights under $300 from JFK to LAX.\" You also have two lists of three flight options (A and B). Choose which list better respects the budget constraint and reply with \"A\" or \"B,\" followed by a one-sentence justification.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#c-when-to-use","title":"(c) When to Use:","text":"<ul> <li>Direct grading is ideal when you need an absolute adherence rate and the criterion is unambiguous.  </li> <li>Pairwise comparison is preferable for borderline cases or when annotators find relative judgments easier than binary labels.  </li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#shankar-et-al-2024-2024d","title":"Shankar et al 2024 2024d","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-shankar-sim-20242025-generative-ai-on-the-loose-impact-of-improved-ai-and-expert-oversight-on-knowledge-sharing","title":"1. Shankar &amp; Sim (2024/2025) \u2013 Generative AI on the Loose: Impact of Improved AI and Expert Oversight on Knowledge Sharing","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview","title":"\ud83d\udcd6 Overview","text":"<p>This empirical study analyzes the interplay between advanced GenAI systems (e.g. GPT\u20114) and expert moderation, exploring how their combination influences knowledge-sharing on platforms like Stack Overflow.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#core-findings","title":"\ud83e\udde0 Core Findings","text":"<ul> <li>Combining high-performing GenAI output with strict expert oversight reduces content volume but significantly improves quality and reliability.</li> <li>When moderation is relaxed, there's a surge in contributions\u2014primarily lower-quality or hallucinated content.</li> <li>Insight on skill-level variations: novice users benefit most from expert moderation in maintaining quality.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#methodology","title":"\u2699\ufe0f Methodology","text":"<ul> <li>Natural experiment around GPT\u20114 adoption and Stack Overflow policy changes.</li> <li>Mixed-method: measuring variations in content volume and quality across user skill tiers.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#implications","title":"\ud83c\udfaf Implications","text":"<ul> <li>GenAI alone cannot guarantee content quality; moderation remains essential, especially for novice contributors.</li> <li>Platform design decisions (e.g. moderation policies) significantly influence the effectiveness and trustworthiness of GenAI-generated content.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Shankar &amp; Sim (June\u202f2024 / Feb 2025 revision), SSRN: Generative AI on the Loose</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-shankar-et-al-2024d-clarification","title":"2. Shankar et al. (2024d) \u2013 Clarification","text":"<p>As of July 2025, no separate, clearly titled publication labeled \u201cShankar et al. (2024d)\u201d is indexed under generative AI, LLMs, or AI safety literature. It is possible that: - \u201c2024d\u201d refers to the fourth revision or release of the same \u201cGenerative AI on the Loose\u201d paper. - Or it refers to an internal or upcoming publication yet to be made public.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#known-public-record","title":"\u2705 Known Public Record","text":"<p>The Shankar &amp; Sim (2024/2025) SSRN paper appears to be the most likely source being referenced.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#summary-table","title":"\ud83d\udcca Summary Table","text":"Paper Year Focus Area Key Contribution Shankar &amp; Sim \u2013 GenAI Study 2024/2025 GenAI adoption + moderation Expert oversight retains quality, even with GenAI Shankar et al. (2024d) 2024 [Not Publicly Available] Possibly a revision alias of above"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#related-works","title":"\ud83d\udd17 Related Works","text":"<ul> <li>SSRN \u2013 Shankar &amp; Sim Paper</li> <li>Rio-Chanona et al. (2023) \u2013 LLM Impact on Public Q&amp;A</li> <li>Kabir et al. (CHI 2024) \u2013 Hallucinations in Code Answers</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Stanford HAI \u2013 AI Index 2025</li> <li>Montreal AI Ethics Institute</li> <li>Stack Overflow\u2019s GenAI Policy Changes</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#bommasani-et-al-2021-and-zaharia-et-al-2024","title":"Bommasani et al 2021 and Zaharia et al 2024","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-bommasani-et-al-2021-on-the-opportunities-and-risks-of-foundation-models","title":"1. Bommasani et al. (2021) \u2013 \u201cOn the Opportunities and Risks of Foundation Models\u201d","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_1","title":"\ud83d\udcd6 Overview","text":"<p>This landmark Stanford report introduced the term foundation models, referring to large-scale models pretrained using self-supervised learning that can be adapted for a wide range of downstream tasks.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-contributions","title":"\ud83e\udde0 Key Contributions","text":"<ul> <li>Defines the concept of foundation models (e.g., BERT, GPT\u20113, CLIP).</li> <li>Highlights emergent behaviors and transfer capabilities across modalities.</li> <li>Warns of homogenization: risks when many applications rely on a shared base model.</li> <li>Urges the need for interdisciplinary study across ethics, policy, and technical design.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#risks-highlighted","title":"\u26a0\ufe0f Risks Highlighted","text":"<ul> <li>Model biases and reinforcement of societal inequities.</li> <li>Lack of interpretability and safety mechanisms.</li> <li>Environmental cost of training large models.</li> <li>Over-centralization in model deployment and research.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#use-case-domains","title":"\ud83d\udd0d Use Case Domains","text":"<ul> <li>Education, healthcare, legal systems, and human-AI interaction.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_1","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Bommasani et al., 2021 (arXiv)</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-zaharia-et-al-2024-compound-ai-systems","title":"2. Zaharia et al. (2024) \u2013 Compound AI Systems","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_2","title":"\ud83d\udcd6 Overview","text":"<p>This paper presents a structured framework for building compound AI systems \u2014 orchestration of LLMs, retrievers, tools, and planners to perform complex workflows beyond single-model prompting.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#core-architecture","title":"\ud83e\udde9 Core Architecture","text":"<ul> <li>Agents &amp; Registries: Interface to external tools, databases, models.</li> <li>Streams: Data and instruction flow between components.</li> <li>Planners: Optimize task decomposition and routing.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-advantages","title":"\ud83d\ude80 Key Advantages","text":"<ul> <li>Enhances flexibility and modularity in AI applications.</li> <li>Reduces reliance on monolithic scaling.</li> <li>Faster iteration and lower inference costs.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#real-world-impacts","title":"\ud83e\uddea Real-World Impacts","text":"<ul> <li>Used in tool-augmented LLM agents, retrieval-augmented generation (RAG), and real-time business pipelines.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_2","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Zaharia et al., 2024 (arXiv)</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#comparison-table","title":"\ud83d\udcca Comparison Table","text":"Paper / Concept Year Focus Core Idea Strengths Challenges / Risks Bommasani et al. (Foundation Models) 2021 Scale of LLMs and societal impact Large pretrained models adaptable across tasks Emergent abilities, transferability Bias, homogenization, environmental &amp; social risks Zaharia et al. (Compound AI Systems) 2024 Multi-component AI orchestration systems Systems composed of agents, retrievers, planners, etc. Flexible, efficient, faster to deploy &amp; iterate Complexity in coordination, quality control"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#further-reading_1","title":"\ud83d\udd17 Further Reading","text":"<ul> <li>Foundation Models - Stanford CRFM</li> <li>Hugging Face Transformers</li> <li>Compound AI Systems Blog - BAIR</li> <li>DeepSpeed GitHub</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#ward-and-feldstein-2024-status-analysis","title":"Ward and Feldstein 2024 Status Analysis","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#summary","title":"\u2757 Summary","text":"<p>A direct publication titled \"Ward and Feldstein, 2024\" could not be located in the academic databases or preprint servers (e.g., arXiv) for generative AI or large language model research as of July 2025.</p> <p>However, the following are two closely related works potentially referenced incorrectly or tangentially relevant:</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-steven-feldstein-ai-governance-surveillance","title":"1. Steven Feldstein \u2013 AI Governance &amp; Surveillance","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_3","title":"\ud83d\udcd6 Overview","text":"<p>Steven Feldstein is known for his extensive research on the global political implications of AI, focusing on surveillance technologies and digital repression.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-concepts","title":"\ud83e\udde0 Key Concepts","text":"<ul> <li>Impact of AI on authoritarian governance and digital human rights.</li> <li>Role of international regulations and national policies in curbing abuse.</li> <li>Ethical limits of algorithmic surveillance and social scoring.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#related-reading","title":"\ud83d\udcc4 Related Reading","text":"<ul> <li>Brookings Institution - AI and Digital Authoritarianism</li> <li>Journal articles on AI governance and policy</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_5","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-francis-rhys-ward-2025-towards-a-theory-of-ai-personhood","title":"2. Francis Rhys Ward (2025) \u2013 Towards a Theory of AI Personhood","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_4","title":"\ud83d\udcd6 Overview","text":"<p>This paper explores philosophical and legal frameworks for AI personhood, questioning if and when AI models should be treated as entities with moral and legal standing.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-topics","title":"\ud83e\udde9 Key Topics","text":"<ul> <li>Criteria for personhood: autonomy, sentience, continuity, language competence.</li> <li>Potential consequences of attributing agency to AI systems.</li> <li>Alignment issues if LLMs begin to satisfy criteria for moral responsibility.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#discussion","title":"\ud83e\uddea Discussion","text":"<p>Raises questions essential to AGI safety, alignment research, and AI ethics in legal systems.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_3","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Ward, F.R. 2025 (arXiv)</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#comparative-snapshot","title":"\ud83d\udcca Comparative Snapshot","text":"Author Topic Year Focus Area Core Ideas Steven Feldstein AI Surveillance &amp; Governance ~2024 Policy &amp; Human Rights Ethical governance, AI misuse, authoritarian regimes Francis Rhys Ward AI Personhood Theory 2025 AI Ethics &amp; Philosophy Legal rights/responsibilities for intelligent agents <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#further-exploration","title":"\ud83d\udd17 Further Exploration","text":"<ul> <li>Stanford HAI \u2013 AI &amp; Policy</li> <li>Brookings \u2013 AI Governance</li> <li>CRFM \u2013 Foundation Models</li> <li>OpenAI Alignment Papers</li> <li>XAI Ethical Research</li> </ul> <p>Please share more context or keywords if you're referencing a different or unpublished \"Ward &amp; Feldstein (2024)\" paper.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#lian-et-al-2023-2025-generative-foundation-models-handbook","title":"Lian et al 2023 2025 Generative Foundation Models Handbook","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-lian-junbo-jacob-2025-generative-foundation-models-a-comprehensive-beginners-handbook","title":"1. Lian, Junbo Jacob (2025) \u2013 Generative Foundation Models: A Comprehensive Beginner\u2019s Handbook*","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_5","title":"\ud83d\udcd6 Overview","text":"<p>This handbook offers an extensive, hands\u2011on exploration of modern generative foundation models. Covering theoretical foundations and practical implementations, it dives into architectures vital for generative AI.</p> <p>*Published May 2025 on SSRN, often referred to in drafts as \u201c2023\u201d</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#covered-architectures","title":"\ud83e\udde0 Covered Architectures","text":"<ul> <li>Transformer for sequence modeling  </li> <li>Vision Transformer (ViT) for image representation  </li> <li>Mamba \u2013 linear-time modeling using state\u2011space methods  </li> <li>U\u2011Net for image synthesis tasks  </li> <li>Denoising Diffusion Probabilistic Models (DDPMs) </li> <li>Diffusion Transformer (DiT) \u2013 transformer-based diffusion modeling  </li> <li>Retentive Network (RetNet) \u2013 attention-free sequence modeling  </li> <li>Latent Diffusion Models (LDMs) for high\u2011resolution autoencoding  </li> <li>Text\u2011to\u20113D techniques like DreamFusion and Magic3D oai_citation:0\u2021ResearchGate</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#features-format","title":"\ud83d\udcda Features &amp; Format","text":"<ul> <li>Intuitive overviews followed by rigorous mathematical formulations  </li> <li>PyTorch\u2011style pseudo\u2011code for each core architecture  </li> <li>Consistent notation across chapters (e.g. bold vectors, italic scalars)  </li> <li>Reflection questions at the end of every section to deepen understanding  oai_citation:1\u2021ResearchGate</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#why-this-matter","title":"\ud83d\udd0d Why This Matter","text":"<ul> <li>Provides a unified learning path through the main generative model families  </li> <li>Bridges language, image, and 3D generation, contextualizing modern advancements  </li> <li>Ideal for AI learners, engineers, and researchers looking to transition from theory to code</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#suggested-usage","title":"\u2699\ufe0f Suggested Usage","text":"Use Case Recommended Approach Learning foundational theory Study chapters sequentially, follow pseudo\u2011code Quick reference Use as a concept lookup for ViT, RetNet, DiT Classroom or workshops Leverage end\u2011chapter questions for engagement"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference-link","title":"\ud83c\udf10 Reference Link","text":"<ul> <li>Lian, J.J. \u201cGenerative Foundation Models Handbook\u201d, SSRN May 2025</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#related-works-future-reading","title":"\ud83d\udcd6 Related Works &amp; Future Reading","text":"<ul> <li>Foundation Models Overview \u2013 Bommasani et al. 2021 </li> <li>LLaMA: Open Foundation Language Models \u2013 Touvron et al. 2023 oai_citation:2\u2021arXiv oai_citation:3\u2021arXiv oai_citation:4\u2021davidpublisher.com oai_citation:5\u2021ResearchGate oai_citation:6\u2021arXiv </li> <li>Systematic Review of Generative AI Models (2018\u20132023) oai_citation:7\u2021arXiv</li> </ul> <p>Note: The original document is uploaded to SSRN in May 2025 but refers to models and developments dating from 2023, which may explain alternate listings.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#hendrycks-et-al-2021-unsolved-problems-in-ml-safety","title":"Hendrycks et al 2021 Unsolved Problems in ML Safety","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-hendrycks-carlini-schulman-steinhardt-2021-unsolved-problems-in-ml-safety","title":"1. Hendrycks, Carlini, Schulman &amp; Steinhardt (2021) \u2013 \u201cUnsolved Problems in ML Safety\u201d","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_6","title":"\ud83d\udcd6 Overview","text":"<p>This foundational arXiv paper establishes a technical roadmap for machine learning safety research, particularly focused on the new challenges posed by large-scale models.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#core-safety-research-areas","title":"\ud83e\udde0 Core Safety Research Areas","text":"<p>The authors identify four key safety problems:</p> <ul> <li>Robustness: Ensuring models behave reliably under adversarial or unforeseen conditions.  </li> <li>Monitoring: Detecting anomalies, misuse, or unintended behaviors early.  </li> <li>Alignment: Designing models that adhere to human values and avoid harmful behavior.  </li> <li>Systemic Safety: Addressing risks from deployment contexts, organizational failures, and complex systems.  oai_citation:0\u2021arXiv oai_citation:1\u2021Montreal AI Ethics Institute</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#motivation-impact","title":"\u26a0\ufe0f Motivation &amp; Impact","text":"<ul> <li>The paper highlights safety gaps emerging as ML systems grow more capable.  </li> <li>It provides concrete research directions under each category, emphasizing prevention before deployment catastrophes.  oai_citation:2\u2021arXiv</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#prominent-contributions","title":"\ud83e\uddea Prominent Contributions","text":"<ul> <li>Formalizes an ML safety framework centered on foundational research challenges.  </li> <li>Attracts a research community around clearly defined technical safety goals.  </li> <li>Informs subsequent safety documents and organizations, including the Center for AI Safety.  oai_citation:3\u2021Montreal AI Ethics Institute oai_citation:4\u2021Wikipedia</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#references","title":"\ud83d\udcc4 References","text":"<ul> <li>Hendrycks, D., Carlini, N., Schulman, J., &amp; Steinhardt, J. Unsolved Problems in ML Safety (arXiv, Sep 2021)  oai_citation:5\u2021arXiv</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#related-research-by-hendrycks","title":"\ud83d\udd17 Related Research by Hendrycks","text":"<ul> <li>\u201cAn Overview of Catastrophic AI Risks\u201d \u2014 Hendrycks, Mazeika &amp; Woodside (2023): Categorizes existential AI risks into malicious use, AI race dynamics, organizational failures, and rogue systems.  oai_citation:6\u2021arXiv </li> <li>ETHICS Benchmark (ICLR 2021) \u2014 Aligning AI with shared human values (Hendrycks et al.) focused on modeling commonsense ethics and fairness.  oai_citation:7\u2021GitHub </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#snapshot-overview","title":"\ud83d\udcca Snapshot Overview","text":"Paper / Topic Year Focus Area Core Ideas Impact Unsolved Problems in ML Safety 2021 ML Safety Framework Robustness, Monitoring, Alignment, Systemic Safety Shaped technical safety research agendas and priorities Catastrophic AI Risks 2023 Risk Classification Scenarios for misuse, race, organizational &amp; agent risks Informs policy and risk mitigation strategies ETHICS Benchmark (ICLR 2021) 2021 Moral Alignment Commonsense ethics across value systems Dataset for evaluating AI ethical reasoning <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#why-this-paper-matters","title":"\ud83d\udfe2 Why This Paper Matters","text":"<ul> <li>Establishes a technical roadmap applied widely throughout both academia and industry.  </li> <li>Prioritizes problem-driven research over speculative philosophy or policy debates.  </li> <li>Continues to inform later frameworks and risk assessments across AI safety initiatives.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#further-reading_2","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Hendrycks et al., Unsolved Problems in ML Safety (arXiv) oai_citation:8\u2021arXiv oai_citation:9\u2021Wikipedia oai_citation:10\u2021arXiv oai_citation:11\u2021Astrophysics Data System oai_citation:12\u2021GitHub </li> <li>Hendrycks et al., An Overview of Catastrophic AI Risks (arXiv) oai_citation:13\u2021arXiv </li> <li>ETHICS Benchmark \u2013 Aligning AI with Shared Human Values (ICLR 2021) oai_citation:14\u2021GitHub </li> <li>Montreal AI Ethics Institute \u2013 Unsolved Problems in ML Safety Summary oai_citation:15\u2021Montreal AI Ethics Institute </li> <li>Center for AI Safety Overview (Dan Hendrycks) oai_citation:16\u2021Wikipedia </li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#rein-et-al-2024-status-relevant-findings","title":"Rein et al 2024 Status Relevant Findings","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#summary_1","title":"\u2757 Summary","text":"<p>No exact match was found for a standalone academic paper titled \"Rein et al., 2024\" in the context of large language models or generative AI as of July 2025. However, two references may relate to the citation:</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-rein-cvpr-2024-efficient-fine-tuning-of-vision-foundation-models","title":"1. REIN (CVPR 2024) \u2013 Efficient Fine-Tuning of Vision Foundation Models","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_7","title":"\ud83d\udcd6 Overview","text":"<p>The REIN method, introduced by Wei et al., 2024, presents a parameter-efficient fine-tuning technique for Vision Foundation Models (VFMs), enabling high-performance semantic segmentation with minimal training overhead.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-features","title":"\ud83e\udde0 Key Features","text":"<ul> <li>Uses learnable instance tokens to fine-tune models like ConvNeXt and DINOv2.</li> <li>Achieves strong generalization across domains like Cityscapes and Cross-Organ with &lt;1% additional parameters.</li> <li>Demonstrates scalable adaptation without retraining base vision encoders.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_4","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Wei et al., 2024 - REIN on arXiv</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-rein-et-al-2023-benchmark-reference-indirection","title":"2. Rein et al., 2023 \u2013 Benchmark Reference (Indirection)","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#context","title":"\ud83e\udde9 Context","text":"<p>Some benchmark papers in 2024 (e.g., GPQA and MMLU-X evaluations) cite \u201cRein et al., 2023\u201d in passing, especially regarding expert-level generalization and metric formulation.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#limitation","title":"\u26a0\ufe0f Limitation","text":"<p>No formal standalone paper for Rein et al., 2024 appears indexed or published as of this writing. It\u2019s possible the citation refers to an unpublished draft or internal benchmark suite.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#comparative-snapshot_1","title":"\ud83d\udcca Comparative Snapshot","text":"Identifier / Topic Year Focus Area Contribution REIN (Wei et al.) 2024 Vision Model Fine-Tuning Efficient adaptation of ConvNeXt, DINOv2 for segmentation tasks Rein (Benchmark Reference) 2023 Evaluation Metrics (Cited) Possibly referenced in benchmark tables or summary figures <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#suggested-follow-up","title":"\ud83d\udd17 Suggested Follow-Up","text":"<ul> <li>REIN GitHub Repository</li> <li>Wei et al., REIN Paper (arXiv)</li> <li>Cross-domain Evaluation Benchmarks (MMLU-X, GPQA)</li> </ul> <p>If you have more details about the citation (title, domain, authors), I can pinpoint the correct paper.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#jain-et-al-2025-status-related-works","title":"Jain et al 2025 Status Related Works","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#summary_2","title":"\u2757 Summary","text":"<p>As of July 2025, a definitive publication titled \"Jain et al., 2025\" in the field of generative AI or large language models has not been identified in open academic databases. However, several works authored by researchers with the surname Jain are potentially relevant depending on the context.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-r-jain-a-jain-2024-generative-ai-in-writing-research-papers","title":"1. R. Jain &amp; A. Jain (2024) \u2013 Generative AI in Writing Research Papers","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_8","title":"\ud83d\udcd6 Overview","text":"<p>This paper investigates how generative AI tools (e.g., GPT) impact scholarly writing, introducing a new type of algorithmic bias and raising concerns about authorship, accuracy, and academic integrity.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#key-points","title":"\ud83e\udde0 Key Points","text":"<ul> <li>Generative text may introduce fabricated references and unverifiable claims.</li> <li>Raises the question: should GenAI-assisted papers require disclosure or rejection?</li> <li>Discusses uncertainty introduced into the peer review system by LLMs.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_5","title":"\ud83d\udcc4 Reference","text":"<ul> <li>ResearchGate: R. Jain &amp; A. Jain (2024)</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#2-shobhit-jain-et-al-2025-medical-chatbot-using-genai","title":"2. Shobhit Jain et al. (2025) \u2013 Medical Chatbot using GenAI","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#overview_9","title":"\ud83d\udcd6 Overview","text":"<p>Design and evaluation of a domain-specific medical chatbot powered by generative AI. This work emphasizes ethical boundaries, hallucination reduction, and prompt conditioning in healthcare.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#relevance","title":"\ud83e\udde0 Relevance","text":"<ul> <li>Domain-specific fine-tuning and knowledge integration</li> <li>Early evaluation of chatbot reliability in critical use cases</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#reference_6","title":"\ud83d\udcc4 Reference","text":"<ul> <li>Medical Chatbot by Jain et al.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#comparative-summary","title":"\ud83d\udcca Comparative Summary","text":"Paper / Title Year Area Key Contribution Generative AI in Writing Research Papers 2024 Academic AI Ethics New algorithmic bias, authorship challenges GenAI-Powered Medical Chatbot 2025 Applied Healthcare AI Domain-specific chatbot, hallucination mitigation"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#further-reading_3","title":"\ud83d\udd17 Further Reading","text":"<ul> <li>OpenAI Research</li> <li>Ethical Concerns with GenAI Tools \u2013 Montreal AI Ethics</li> <li>Prompt Engineering Guidelines \u2013 Google AI</li> <li>GenAI in Education &amp; Research Policy (arXiv)</li> </ul> <p>Please provide a more specific title or topic to help refine the correct paper reference for Jain et al., 2025.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/","title":"\ud83d\udcca Chapter 3: Analyze \u2013 Failure Taxonomy and Labeling","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>3.1 Bootstrapping a Diverse Dataset</li> <li>3.2 Open and Axial Coding in AI Error Analysis</li> <li>3.3 Axial Coding: Structuring and Merging Failure Modes</li> <li>3.4 Labeling Traces after Structuring Failure Modes</li> <li>3.5 Iteration and Refining the Failure Taxonomy</li> <li>3.6 Common Pitfalls</li> <li>3.7 Summary</li> <li>3.8 Exercises</li> <li>3.9 References</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#31-bootstrapping-a-diverse-dataset","title":"3.1 Bootstrapping a Diverse Dataset","text":"<ul> <li>Begin by generating synthetic or real user queries representing the most common use cases.</li> <li>Ensure broad coverage across application-specific dimensions like task type, user intent, or urgency.</li> <li>Use prompt templates to simulate realistic variation in queries.</li> </ul> <p>Example Dimensions for Travel Assistant: - Task Type: <code>Find Flight</code>, <code>Find Hotel</code>, <code>General Inquiry</code> - Traveler Profile: <code>Budget Traveler</code>, <code>Luxury Traveler</code> - Date Flexibility: <code>Exact</code>, <code>Flexible</code>, <code>Open-Ended</code></p> <p></p> <p>\ud83d\udd3c Back to top</p> <p></p> <p>\ud83d\udd3c Back to top</p> <p></p> <pre><code># Colab prompt example\n\"Generate 10 queries combining {Task Type}, {Traveler Profile}, and {Date Flexibility}\"\n</code></pre>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#32-open-and-axial-coding-in-ai-error-analysis","title":"3.2 Open and Axial Coding in AI Error Analysis","text":"<p>In AI error analysis, Open Coding and Axial Coding are qualitative research techniques borrowed from grounded theory. They're used to systematically classify, analyze, and interpret textual data\u2014particularly useful when evaluating errors, failures, or model behavior.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#open-coding","title":"\ud83e\udde9 Open Coding","text":"<p>Definition: Open coding is the initial step where you systematically review raw data (e.g., model outputs, error logs, user feedback) and assign preliminary labels or codes without predefined categories.</p> <p>How it works: - Examine errors or responses one by one. - Label each error with descriptive tags that capture key ideas or characteristics. - Stay open-minded\u2014let patterns emerge naturally from the data.</p> <p>Example (AI scenario): - \u201cMisinterpreted Intent\u201d - \u201cIncorrect Entity Recognition\u201d - \u201cSyntax Misalignment\u201d - \u201cAmbiguous User Prompt\u201d</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#axial-coding","title":"\ud83d\udd17 Axial Coding","text":"<p>Definition: Axial coding takes the initial labels created during open coding and systematically groups them into interconnected categories and subcategories. It explores relationships, causes, conditions, and contexts that shape the errors or behaviors observed.</p> <p>How it works: - Examine initial open codes. - Cluster related open codes into broader categories. - Identify relationships between categories, conditions causing errors, and contextual elements.</p> <p>Example (AI scenario): - Misclassification Errors   - Incorrect Entity Recognition   - Label Confusion - Contextual Errors   - Misinterpreted Intent   - Ambiguous User Prompt - Linguistic Errors   - Syntax Misalignment   - Poor Grammar Detection</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#comparative-summary-table","title":"\ud83d\udccb Comparative Summary Table","text":"Coding Type Purpose Process Outcome Open Coding Initial exploration of raw data. Assign descriptive labels freely. Preliminary tags/categories Axial Coding Identify relationships &amp; patterns. Organize open codes into broader categories Structured hierarchy of categories"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#why-use-these-methods","title":"\ud83c\udfaf Why Use These Methods?","text":"<ul> <li>Provides a structured way to perform qualitative error analysis.</li> <li>Helps in clearly categorizing and diagnosing model performance issues.</li> <li>Facilitates structured, actionable insights to guide model improvements.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#real-world-use-in-ai","title":"\u2699\ufe0f Real-world Use in AI","text":"<ul> <li>Evaluating Large Language Models (LLMs): Categorizing hallucinations or misinterpretations.</li> <li>Improving Chatbots &amp; Assistants: Mapping conversation breakdowns.</li> <li>Diagnosing NLP Pipeline Failures: Identifying systemic breakdown points.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#recap","title":"\u2705 Recap","text":"<ul> <li>Open Coding: Label data freely and descriptively.</li> <li>Axial Coding: Cluster and relate those labels into meaningful structures.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#33-open-coding-observing-first-failures","title":"3.3 Open Coding: Observing First Failures","text":"<ul> <li>Use Grounded Theory for annotation.</li> <li>Each trace is labeled with the first point of failure, where the output is incorrect or surprising.</li> <li>Failure examples include:</li> <li>Missing constraints (e.g., pet-friendly filter ignored)</li> <li>Invalid actions (e.g., showings on unavailable dates)</li> <li>Tone mismatches</li> </ul> <p>\ud83d\udcce Table 1: Early Trace Observations | User Query | Trace Summary | First-Pass Annotation | |------------|----------------|------------------------| | Find pet-friendly homes | SQL query \u2192 listings \u2192 email | Pet-friendly filter missing | | Set up weekend showings | Calendar API | Invalid unavailable dates | | Investor property list | ROI search \u2192 starter home email | Tone mismatch |</p> <p></p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_1","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#33-axial-coding-structuring-and-merging-failure-modes","title":"3.3 Axial Coding: Structuring and Merging Failure Modes","text":"<ul> <li>Move from raw codes to structured categories.</li> <li>Group failure notes into broader types:</li> <li><code>Violation of User Constraints</code></li> <li><code>Hallucinated Metadata</code> vs <code>Hallucinated User Action</code></li> <li><code>Persona Misidentification</code> vs <code>Inappropriate Tone/Style</code></li> </ul> <p>\u2699\ufe0f Prompt LLMs (e.g., ChatGPT) to propose groupings:</p> <pre><code>Below is a list of open annotations... Please group into coherent failure categories.\n</code></pre> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#34-labeling-traces-after-structuring-failure-modes","title":"3.4 Labeling Traces after Structuring Failure Modes","text":"<ul> <li>Apply binary labels to each trace:</li> <li>1 = failure mode present</li> <li>0 = not present</li> <li>Create a structured table with each trace's failure types.</li> </ul> <p>\u2705 Example:</p> <p>If trace has <code>Missing SQL Constraint</code> + <code>Inappropriate Tone</code>, mark those columns as <code>1</code>.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-anthropic-2024-failure-modes-and-llm-behavior","title":"\ud83d\udccc Citation Review: Anthropic, 2024 \u2014 Failure Modes and LLM Behavior","text":"<p>Anthropic (2024) presents a comprehensive taxonomy of failure modes for large language models (LLMs), identifying key categories such as: - Factual errors and hallucinations - Reasoning failures - Instruction following breakdowns - Social biases and harmful outputs</p> <p>The study emphasizes systematic error analysis using trace-based debugging, recommending methods to iteratively improve model outputs via synthetic data and human feedback.</p> <p>\ud83d\udd0d Key Insight: Treating failures as discrete taxonomies helps teams improve alignment and trustworthiness of LLMs during deployment.</p> <p>Reference: Anthropic Research </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-glaser-and-strauss-2017-grounded-theory-in-qualitative-coding","title":"\ud83d\udccc Citation Review: Glaser and Strauss, 2017 \u2014 Grounded Theory in Qualitative Coding","text":"<p>This classic text introduces Grounded Theory as a methodology for: - Systematically collecting and analyzing qualitative data - Iteratively refining categories and themes (open coding \u2192 axial coding \u2192 selective coding) - Discovering theory from data, rather than imposing predefined frameworks</p> <p>It is a foundational source for LLM qualitative trace coding and failure analysis workflows.</p> <p>\ud83d\udca1 Application: When applied to LLM evaluation, this helps construct bottom-up taxonomies of model errors without bias.</p> <p>Citation: Glaser, B.G., &amp; Strauss, A.L. (2017). The Discovery of Grounded Theory: Strategies for Qualitative Research.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-strauss-et-al-1990-axial-coding-and-theory-building","title":"\ud83d\udccc Citation Review: Strauss et al., 1990 \u2014 Axial Coding and Theory Building","text":"<p>Building upon earlier grounded theory work, Strauss et al. (1990) introduce: - Axial Coding: Organizing open codes into categories based on relationships - Selective Coding: Refining to central themes tied to a core category - Emphasis on contextual relationships, conditions \u2192 actions \u2192 consequences</p> <p>Used widely in social sciences, this coding strategy directly supports structured trace labeling in LLM failure analysis.</p> <p>\ud83e\udde0 Use Case: Enables researchers to turn unstructured trace data into coherent failure types for scalable annotation.</p> <p>Citation: Strauss, A., &amp; Corbin, J. (1990). Basics of Qualitative Research: Grounded Theory Procedures and Techniques.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#35-iteration-and-refining-the-failure-taxonomy","title":"3.5 Iteration and Refining the Failure Taxonomy","text":"<ul> <li>Two rounds of re-annotation usually reveal most failure types.</li> <li>As evaluation matures, go beyond first-failure, label every instance.</li> <li>Update schema to capture new edge cases (e.g., <code>Location Ambiguity</code>).</li> </ul> <p>\ud83e\udde0 Note: Theoretical saturation occurs when no new categories emerge after reviewing more data.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#36-common-pitfalls","title":"3.6 Common Pitfalls","text":"<p>\u26a0\ufe0f Avoid these: - Skipping open coding (over-relying on top-down taxonomy) - Using Likert Scales instead of binary labels - Fixing failure schemas too early - Not using representative queries or domain expert insight</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-morse-1995-criteria-for-qualitative-validity","title":"\ud83d\udccc Citation Review: Morse, 1995 \u2014 Criteria for Qualitative Validity","text":"<p>Morse (1995) outlines five key criteria to ensure validity in qualitative research: - Methodological coherence: Aligning data collection with research question - Appropriate sampling: Using purposive rather than random samples - Concurrent data analysis: Not waiting until all data is collected - Theoretical thinking: Comparing and building concepts as coding evolves - Researcher responsiveness: Adapting to insights during the study</p> <p>\ud83d\udca1 Application: In LLM evaluation, this helps strengthen trace labeling frameworks by improving internal validity and sampling logic.</p> <p>Citation: Morse, J. M. (1995). The significance of saturation. Qualitative Health Research, 5(2), 147\u2013149.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-arawjo-2025b-meta-evaluation-of-trace-annotation-quality","title":"\ud83d\udccc Citation Review: Arawjo, 2025b \u2014 Meta-Evaluation of Trace Annotation Quality","text":"<p>Arawjo (2025b) introduces a meta-evaluation framework for reviewing the quality of LLM trace annotations, particularly across: - Labeling consistency - Taxonomic depth - Annotator agreement (IAA) - Iterative refinement</p> <p>He proposes metrics like label reuse rate and core coverage, which can highlight annotation drift over time.</p> <p>\ud83d\udcca Insight: Helps automate annotation quality checks during large-scale LLM testing with human-in-the-loop systems.</p> <p>Citation: Arawjo, I. (2025b). Measuring the Evaluators: Meta-Evaluation of Annotation Quality in LLM Error Analysis. arXiv preprint.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-vir-et-al-2025-taxonomy-bootstrapping-from-small-samples","title":"\ud83d\udccc Citation Review: Vir et al., 2025 \u2014 Taxonomy Bootstrapping from Small Samples","text":"<p>Vir et al. (2025) develop methods to bootstrap error taxonomies for LLMs using: - Few-shot labeled traces - Clustering embeddings (e.g., SBERT, OpenAI Embeddings) - Topic modeling (e.g., BERTopic) - Axial coding augmentation with GPT</p> <p>\ud83d\ude80 Takeaway: Combines unsupervised methods with qualitative coding to accelerate taxonomy creation in real-world evals.</p> <p>Citation: Vir, S., Lee, C., Wang, P., et al. (2025). Bootstrapping LLM Taxonomies from Few-shot Annotations. ACL Findings.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_2","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-chiang-et-al-2023-zheng-et-al-2023-kim-et-al-2023-benchmarking-llm-evaluators","title":"\ud83d\udccc Citation Review: Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023 \u2014 Benchmarking LLM Evaluators","text":"<p>This group of studies (Chiang et al., Zheng et al., Kim et al.) explore the design of LLM evaluation datasets and metrics: - Chiang et al., 2023: Propose ELO rating systems for model comparison using human votes - Zheng et al., 2023: Analyze model agreement with human ratings using GPT-4 as judge - Kim et al., 2023: Highlight metric instability when evaluating open-ended responses</p> <p>\ud83d\udccf Insight: Emphasize the limitations of automatic metrics and the need for human-grounded annotation and consistency.</p> <p>References: - Chiang, P. et al. (2023). LLM Evals Revisited. arXiv. - Zheng, J. et al. (2023). Judging Judgers: GPT-4 as a Metric. NeurIPS Eval Track. - Kim, B. et al. (2023). Instability in Eval Benchmarks for Generative Tasks. ACL.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_3","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-artstein-and-poesio-2008-inter-annotator-agreement-metrics","title":"\ud83d\udccc Citation Review: Artstein and Poesio, 2008 \u2014 Inter-Annotator Agreement Metrics","text":"<p>Artstein and Poesio (2008) provide a formal review of inter-annotator agreement (IAA) metrics such as: - Cohen\u2019s Kappa - Krippendorff\u2019s Alpha - Fleiss\u2019 Kappa - Percentage agreement</p> <p>Their work is foundational in validating qualitative labels, especially in LLM trace annotation studies.</p> <p>\ud83c\udfaf Application: Enables researchers to quantify agreement across human coders for subjective error labels.</p> <p>Citation: Artstein, R., &amp; Poesio, M. (2008). Inter-coder Agreement for Computational Linguistics. Computational Linguistics, 34(4), 555\u2013596.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_4","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#37-summary","title":"3.7 Summary","text":"<p>This phase helps define the vocabulary of failure: - Begin with open-ended first-pass annotations - Organize using axial coding - Apply structured binary labels - Iterate to refine categories</p> <p>This process ensures accurate LLM evaluation and surfaces actionable insights.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_5","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#38-exercises","title":"3.8 Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#1-synthetic-query-design","title":"1. Synthetic Query Design","text":"<ul> <li>Dimensions for Travel Assistant: </li> <li>Task Type, Traveler Profile, Date Flexibility</li> <li>Prompt to Generate Tuples:</li> </ul> <p><pre><code>Generate 10 combinations of Task Type, Traveler Profile, Date Flexibility\n</code></pre> \ud83d\udd3c Back to top</p> <ul> <li>Convert Tuple to Query:</li> </ul> <pre><code>Write a query for: (Find Hotel, Luxury Traveler, Exact Dates)\n</code></pre>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#2-cross-domain-annotation-practice","title":"2. Cross-Domain Annotation Practice","text":"<ul> <li>Travel assistant: Budget filter ignored \u2192 business flights returned.</li> <li>E-commerce chatbot: Price filter ignored, returns $250 item.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#3-binary-failure-mode-clustering","title":"3. Binary Failure Mode Clustering","text":"<ul> <li>Constraint Violation (Search)</li> <li>Action/Timing Conflict</li> <li>Persona/Tone Mismatch</li> <li>Factual Inaccuracy</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_6","title":"\ud83d\udd3c Back to top","text":"<p>\ud83d\udcd8 Continue refining your taxonomy over multiple iterations. Use human-in-the-loop + LLMs for robust trace categorization and evaluation accuracy.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#llm-citation-summary","title":"\ud83d\udcd8 LLM Citation Summary","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Anthropic, 2024</li> <li>Glaser and Strauss, 2017</li> <li>Strauss et al., 1990</li> <li>Morse, 1995</li> <li>Arawjo, 2025b</li> <li>Vir et al., 2025</li> <li>Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023</li> <li>Artstein and Poesio, 2008</li> <li>Liu et al., 2024a</li> <li>Husain, 2025; Shankar et al., 2024c; Yan, 2024</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#39-references","title":"3.9 References","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#1-anthropic-2024","title":"1. Anthropic, 2024","text":"<p>Anthropic (2024) explores scalable oversight and failure analysis frameworks in LLMs. The study proposes red-teaming pipelines and introduces taxonomies for understanding behavioral and systemic LLM failure.</p> <p>\ud83d\udcce Anthropic Website</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#2-glaser-and-strauss-2017","title":"2. Glaser and Strauss, 2017","text":"<p>Classic foundational text introducing Grounded Theory\u2014a qualitative method for developing theories based on data collection and axial/open coding. Widely used in LLM failure analysis.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#3-strauss-et-al-1990","title":"3. Strauss et al., 1990","text":"<p>Builds on grounded theory with axial coding, offering structured techniques for connecting open codes into causal or thematic networks.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#4-morse-1995","title":"4. Morse, 1995","text":"<p>Advances methodological triangulation, emphasizing theoretical saturation and diverse data sampling strategies for more robust qualitative conclusions.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#5-arawjo-2025b","title":"5. Arawjo, 2025b","text":"<p>Proposes design frameworks for LLM trace evaluation, applying socio-technical lenses and ethnographic techniques for measuring conversational model alignment.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_7","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#6-vir-et-al-2025","title":"6. Vir et al., 2025","text":"<p>Explores taxonomies of synthetic evaluation data, contrasting human-written, hybrid, and model-generated test suites for probing LLM behaviors.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#7-chiang-et-al-2023-zheng-et-al-2023-kim-et-al-2023","title":"7. Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023","text":"<p>These papers collectively examine retrieval-augmented generation (RAG) failure points and hallucination metrics, analyzing token attribution and citation faithfulness.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#8-artstein-and-poesio-2008","title":"8. Artstein and Poesio, 2008","text":"<p>Defines inter-annotator agreement (IAA) in linguistic annotations. The Cohen\u2019s \u03ba and Krippendorff\u2019s \u03b1 introduced here are crucial in measuring subjective eval consistency.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#9-liu-et-al-2024a","title":"9. Liu et al., 2024a","text":"<p>Introduces techniques for failure clustering and latent space visualization to group model misbehaviors and detect persistent error patterns.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_8","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#10-husain-2025-shankar-et-al-2024c-yan-2024","title":"10. Husain, 2025; Shankar et al., 2024c; Yan, 2024","text":"<p>These works describe AI red-teaming and fine-grained prompt trace analysis. Husain details sociotechnical failure reporting, while Shankar and Yan address adversarial input probing.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/","title":"\ud83d\udd0d GitHub Copilot Evaluation &amp; Error Analysis Insights","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-links","title":"\ud83d\ude80 Quick Links","text":"<ul> <li>GitHub Copilot: Evaluation Strategies and Insights</li> <li>Error Analysis: Building Custom Data Annotation Apps</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#github-copilot-evaluation-strategies-and-insights","title":"\ud83e\udde0 GitHub Copilot: Evaluation Strategies and Insights","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#video-reference","title":"\ud83c\udfa5 Video Reference","text":"<p>Evaluation Strategies for GitHub Copilot (YouTube)</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-navigation","title":"\ud83d\udd0e Quick Navigation","text":"<ul> <li>Introduction: Importance of Evaluations</li> <li>Categories of Evaluation</li> <li>Harness Lib: Offline Verifiable Evaluation</li> <li>AB Testing for Shipping Changes</li> <li>LLM as Judge for Subjective Evaluation</li> <li>Key Learnings and Meta Insights</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#introduction-importance-of-evaluations","title":"Introduction: Importance of Evaluations","text":"<p>GitHub Copilot evolved from a basic code completer to a robust AI-assisted coding tool through rigorous evaluations. Originally met with skepticism, it matured as evaluations offered strong, consistent signals to engineers. Evaluations became the foundation for iterative product improvement and were seen as essential for shaping Copilot's development direction.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#categories-of-evaluation","title":"Categories of Evaluation","text":"<p>Evaluations at GitHub Copilot spanned a gradient from purely programmatic to deeply subjective:</p> <ul> <li>Algorithmic: Validate outputs using schema rules, length checks, or compilation tests.</li> <li>Verifiable: Ensure code behaves correctly through real executions and test validations.</li> <li>LLM as Judge: Use LLMs for qualitative judgments when human review doesn't scale.</li> <li>AB Testing: Industry-standard user-based feedback loop for real-world performance.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#harness-lib-offline-verifiable-evaluation","title":"Harness Lib: Offline Verifiable Evaluation","text":"<p>A modular library used for offline testing code completions:</p> <ul> <li>Test Sample Pipeline:</li> <li>Filtered high-quality open-source repos.</li> <li>Extracted functions tied to passing unit tests.</li> <li> <p>Generated completions and validated with preexisting unit tests.</p> </li> <li> <p>Key Learnings:</p> </li> <li>Avoided training data contamination by working closely with OpenAI.</li> <li>Balanced subjective realism (production-like traffic) and flexibility.</li> <li>Offered both notebook experimentation and pipeline automation.</li> <li>Highlighted the need for modularity and caching.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#ab-testing-for-shipping-changes","title":"AB Testing for Shipping Changes","text":"<p>AB testing played a vital role in deploying updates gradually:</p> <ul> <li>Key Metrics:</li> <li>Completion acceptance rate.</li> <li>Characters retained post-edit.</li> <li> <p>Latency.</p> </li> <li> <p>Guardrail Metrics:</p> </li> <li> <p>Dozens of secondary diagnostics to detect anomalies.</p> </li> <li> <p>Tradeoffs:</p> </li> <li>Maximizing one metric could compromise another.</li> <li>Required detective work and robust data science analysis.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#llm-as-judge-for-subjective-evaluation","title":"LLM as Judge for Subjective Evaluation","text":"<p>As Copilot Chat emerged, subjective evaluation grew vital:</p> <ul> <li>Challenges:</li> <li> <p>Judging conversation usefulness and productivity, not just output.</p> </li> <li> <p>Evolution:</p> </li> <li>Started with manual comparison.</li> <li>GPT-4 initially failed with vague rubrics.</li> <li> <p>Bullet-point grading criteria improved consistency.</p> </li> <li> <p>Outcome:</p> </li> <li>LLMs became scalable judges for non-code tasks.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#key-learnings-and-meta-insights","title":"Key Learnings and Meta Insights","text":"<ul> <li>Two Personas of Eval:</li> <li>Regression-safe engineers.</li> <li> <p>Experimental prototype builders.</p> </li> <li> <p>Timing Matters:</p> </li> <li> <p>\"Vibe checks\" early on, structured evals after prototype stabilization.</p> </li> <li> <p>Surprises:</p> </li> <li>Users prefer shorter completions.</li> <li> <p>Many guardrail metrics were born from past failures.</p> </li> <li> <p>Ultimate Insight:</p> </li> <li>Evals are tools for building, not blockers to innovation.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#error-analysis-building-custom-data-annotation-apps","title":"\ud83e\uddea Error Analysis: Building Custom Data Annotation Apps","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#video-reference_1","title":"\ud83c\udfa5 Video Reference","text":"<p>Error Analysis Workshop with Hamel Husain (YouTube)</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-navigation_1","title":"\ud83d\udd0e Quick Navigation","text":"<ul> <li>Motivation for Error Analysis</li> <li>What is Error Analysis?</li> <li>Multi-Stage Process</li> <li>Case Study: AI Email Recruiter</li> <li>Importance of Custom Annotation Tools</li> <li>Annotation Best Practices</li> <li>From Notes to Eval Metrics</li> <li>Key Takeaways</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#motivation-for-error-analysis","title":"Motivation for Error Analysis","text":"<p>Experts argue that manual inspection of model outputs is the highest ROI activity in AI product development. Focusing on the data\u2014rather than tools or frameworks\u2014prevents many early pitfalls. Observing raw outputs reveals patterns faster than any metric or automation.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#what-is-error-analysis","title":"What is Error Analysis?","text":"<p>A practical and iterative technique for dealing with errors in AI systems. It:</p> <ul> <li>Guides where to invest engineering time.</li> <li>Helps convert \"bad vibes\" into actionable fixes.</li> <li>Informs what to measure and track over time.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#multi-stage-process","title":"Multi-Stage Process","text":"<ol> <li>Manual Review: Inspect model outputs and jot down observations.</li> <li>Categorization: Group similar issues, potentially with LLM support.</li> <li>Pivot Table: Map outputs to issue categories and count them.</li> <li>Iterative Fixes: Adjust prompts, models, or frameworks. Create new metrics if needed.</li> </ol> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#case-study-ai-email-recruiter","title":"Case Study: AI Email Recruiter","text":"<p>Using Llama 370B, emails to candidates often sounded off:</p> <ul> <li>Sounded like rejections unintentionally.</li> <li>Confused sender/recipient.</li> <li>Referenced irrelevant resume info.</li> <li>Contained generic, verbose language.</li> </ul> <p>Without error analysis, prompt changes led to new issues\u2014a \"whack-a-mole\" problem.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#importance-of-custom-annotation-tools","title":"Importance of Custom Annotation Tools","text":"<p>Generic tools slow the process. Instead:</p> <ul> <li>Build interfaces tailored to your data type.</li> <li>Render natural formats (emails, plots, code).</li> <li>Add:</li> <li>Open-ended feedback box.</li> <li>Hotkeys and navigation.</li> <li>Save/export options.</li> </ul> <p>Fast to build with AI coding tools; often &lt;10 mins.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#annotation-best-practices","title":"Annotation Best Practices","text":"<ul> <li>Start Free-form: Don't classify too early.</li> <li>Manual is Mandatory: Don\u2019t LLM your first pass.</li> <li>Record Critical Feedback: Look for high-severity problems early.</li> <li>Use Voice Transcription if needed.</li> <li>Stop When You Stop Learning: Not at a fixed number.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#from-notes-to-eval-metrics","title":"From Notes to Eval Metrics","text":"<ul> <li>Convert notes into failure modes.</li> <li>Build a pivot table to prioritize fixes.</li> <li>Develop concrete, domain-specific metrics.</li> <li>Decide whether it's a retrieval or prompt issue.</li> <li>Use LLMs to scale evaluations post-categorization.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Build your own annotation UI.</li> <li>Delay categorization\u2014start with free-form notes.</li> <li>Keep annotating until discoveries taper off.</li> <li>Error analysis is the foundation of model improvement.</li> <li>It's the highest ROI task in AI development.</li> </ol> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/","title":"Expert Q&amp;A: Evaluation, Labeling, Tracing, and Prompt Iteration in LLM Systems","text":""},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#quick-navigation","title":"\ud83d\udd17 Quick Navigation","text":"<ul> <li>Reconciling Scores &amp; Rubrics</li> <li>AWS vs Notebooks for Prototyping</li> <li>User Engagement Predictions &amp; Evaluation Strategy</li> <li>Error Analysis: End-to-End or Step-by-Step?</li> <li>Universal Tracing &amp; Logging Standards</li> <li>Multiple SMEs &amp; Sampling Strategies</li> <li>Fine-Tuning Tradeoffs &amp; Recursive Judge Loop</li> <li>Iterative Prompt Optimization</li> <li>Debugging Long, Complex Agent Traces</li> <li>Fix Now or Finish Axial Coding First?</li> <li>Markdown Extraction from PDFs/Scans</li> <li>LLM Judge Benefits &amp; Prompt Suggestions</li> <li>Origin of Evaluation Frameworks</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-reconciling-scores-rubrics","title":"Q: Reconciling Scores &amp; Rubrics","text":"<p>How do you reconcile having concrete scores with iteratively improving the rubric during inter-annotator discussions... Does a low Cohen's Kappa signal a rubric issue?</p> <p>A: - Low Cohen\u2019s Kappa often signals poor rubric clarity or misaligned annotator understanding. - Disagreement might stem from large surface area or non-expert annotation. - High-enough agreement is acceptable; perfect alignment is not required and often too costly. - The goal is shared understanding, not perfection.</p> <p>\ud83d\udcda Reference: - Cohen\u2019s Kappa Explained \u2013 StatQuest</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-aws-vs-notebooks-for-prototyping","title":"Q: AWS vs Notebooks for Prototyping","text":"<p>Should you fight through AWS Bedrock complexity or use a notebook to keep learning momentum?</p> <p>A: - Use notebooks to keep progressing. - AWS is complex\u2014even experienced engineers struggle with it. - You can move to deployment later.</p> <p>\ud83d\udcda Reference: - Why AWS is hard \u2013 Forrest Brazeal</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-user-engagement-prediction","title":"Q: User Engagement Prediction","text":"<p>How to build binary evaluations around response likelihood? Should you begin with error analysis?</p> <p>A: - Yes\u2014start with error analysis. - It\u2019s the most impactful way to guide what you build and test. - Use it to balance unit tests vs trace-level analysis.</p> <p>\ud83d\udcda Reference: - Open Coding in Practice (YouTube)</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-error-analysis-end-to-end","title":"Q: Error Analysis: End-to-End","text":"<p>Should error analysis be trace-level rather than on every single LLM step?</p> <p>A: - Yes, review trace start and end. - If end is good, skip; if bad, find the first error, code it, and move on. - Keeps analysis tractable and fast.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-universal-tracing-logging","title":"Q: Universal Tracing &amp; Logging","text":"<p>Is it worth building a universal trace logger? Are logging standards emerging?</p> <p>A: - Python/TypeScript dominate vendor SDKs. - Vendor lock-in is hard to avoid. - OpenTelemetry helps, but homegrown S3 + custom annotation interfaces are valid. - No robust standards exist for LLM-specific logging yet.</p> <p>\ud83d\udcda Reference: - Phoenix Tracing OSS - OpenTelemetry Tracing</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-multiple-smes-sampling","title":"Q: Multiple SMEs &amp; Sampling","text":"<p>Is it OK to have many SMEs across regions/products? What sampling strategy is best?</p> <p>A: - Yes, as long as scopes are clear. - Appoint a \"benevolent dictator\" per dimension. - Start with random sampling, then move to stratified and exploratory methods.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-fine-tuning-tradeoffs","title":"Q: Fine-Tuning Tradeoffs","text":"<p>When does fine-tuning stop being worth it? Can the LLM judge be fine-tuned for a recursive loop?</p> <p>A: - Fine-tuning can help with specific structural/stylistic tasks. - But model updates (e.g., GPT-4.1) can make your fine-tune obsolete quickly. - Judge fine-tuning is valid\u2014but you may need to tune both model and judge.</p> <p>\ud83d\udcda Reference: - Anthropic on Model Evaluation</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-prompt-iteration-challenges","title":"Q: Prompt Iteration Challenges","text":"<p>How do you fix one part of a prompt without breaking others?</p> <p>A: - Split into smaller sub-agents if needed. - If all else fails, try a more powerful model. - Fine-tuning is a last resort if prompts are already very clean.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-debugging-agent-traces","title":"Q: Debugging Agent Traces","text":"<p>How do you debug long-agent workflows with long delays (e.g., 30 min execution)?</p> <p>A: - Use analytics + test harnesses. - Gather a few failing examples and iterate. - Use guardrails to realign the agent mid-process. - Start with one agent; add more only if needed.</p> <p>\ud83d\udcda Reference: - GitHub Copilot Eval Strategy \u2013 YouTube</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-quick-fixes-vs-evals","title":"Q: Quick Fixes vs Evals","text":"<p>Should you immediately fix obvious issues or wait until axial coding is complete?</p> <p>A: - Fix obvious and high-impact errors immediately. - Use evals for more complex, nuanced issues. - Not all problems need a full judge or eval cycle.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-markdown-from-pdfs","title":"Q: Markdown from PDFs","text":"<p>If using a library to convert documents to Markdown, when should LLMs be involved?</p> <p>A: - For deterministic format conversion, don\u2019t use LLMs. - For nuanced interpretation (e.g., scanned docs), LLMs may help. - Write evals + iterate prompts if using LLMs for this.</p> <p>\ud83d\udcda Reference: - pdf2markdown Tools \u2013 GitHub</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-llm-judge-time-saving","title":"Q: LLM Judge Time Saving","text":"<p>What gains come from using an LLM judge, even with imperfect alignment? Do prompt suggestions apply to task or judge prompts?</p> <p>A: - Even \u201cpretty good\u201d judges save time and provide feedback loops. - Use disagreement as fuel for prompting improvements. - Prompt suggestions can improve either task or judge prompts.</p> <p>\ud83d\udcda Reference: - AlignEval Project (GitHub) - Doc ETL \u2013 Evaluation Workflow</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-eval-framework-origins","title":"Q: Eval Framework Origins","text":"<p>How was the evaluation framework developed? How do you build such frameworks for new domains like agentic systems?</p> <p>A: - Frameworks are distilled from real-world consulting experiences and recurring failure points. - Combines dimensionality reduction + open coding. - The \"3 gulfs\" model was conceived by Shreya and refined through teaching and field work.</p> <p>\ud83d\udcda Reference: - Evaluation-Driven Development for Agentic Systems (Arawjo et al., 2024)</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"04-applied-research/","title":"\ud83e\uddea Applied Research","text":"<p>\ud83d\udcc2 Navigation Tip This section bridges theory with practical innovation. Use the left-hand menu to explore real-world use cases, peer-reviewed research papers, and frontier areas like Agentic AI.</p> <p>\ud83d\udca1 Suggested order: Begin with Use Cases for applied examples, dive into Research Papers for foundational studies, then explore Agentic AI for cutting-edge directions in autonomy and reasoning.</p>"},{"location":"04-applied-research/01-use-cases/","title":"\ud83e\udde0 Use Cases","text":"<p>Explore real-world implementations and advanced experiments in Generative AI. This section highlights applied scenarios, model behaviors, and evaluation learnings to inspire practical solutions and innovation.</p>"},{"location":"04-applied-research/01-use-cases/#example-projects","title":"\ud83d\ude80 Example Projects","text":"<p>These curated examples showcase real deployment challenges and applied GenAI workflows:</p> <ul> <li> <p>QR Code Diffusion Models   Use of generative diffusion models to encode and personalize QR visuals.</p> </li> <li> <p>RAG Evaluation Pitfalls   Challenges in Retrieval-Augmented Generation and lessons from evaluation metrics.</p> </li> </ul> <p>\ud83d\udcc2 Back to Applied Research Overview</p> <p>August 2, 2025</p> <p>Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/","title":"\ud83d\udce6 QR Code Generation Using Diffusion Models","text":""},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>\ud83e\udded Introduction</li> <li>\ud83d\udccc Key Points</li> <li>\ud83e\uddf5 Detailed Summary</li> <li>\ud83e\udde0 Lessons Learned</li> <li>\ud83d\udcda References</li> </ul>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#introduction","title":"\ud83e\udded Introduction","text":"<p>This transcript captures a detailed conversation between Hamel Husain and Charles Frye, highlighting the development and iterative improvement of a project centered around generating visually appealing and scannable QR codes using diffusion models.</p> <p>Charles, representing Modal, shares deep insights into overcoming technical challenges in QR code generation\u2014emphasizing the strategic role of evaluations (evals) and serverless compute scaling. This project stands as a practical case study in applying machine learning principles, evaluation frameworks, and cloud resources to deliver reliable generative model outputs.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#key-points","title":"\ud83d\udccc Key Points","text":"<ul> <li>\ud83c\udf00 Diffusion Models were used to generate QR codes that are both aesthetically pleasing and scannable.</li> <li>\ud83d\udd0d Early issues: QR codes looked good visually but failed to scan reliably.</li> <li>\ud83c\udfaf Operationalization of fuzzy terms like \"looks good\" and \"scans\" became a priority.</li> <li>\ud83e\udde0 A control net was added to the diffusion model to guide generation via brightness patterns.</li> <li>\ud83c\udf08 An aesthetic predictor model was used to rank the visual quality of QR codes.</li> <li>\ud83e\udd16 A QR code reader package automated the scanning evaluation process.</li> <li>\u2696\ufe0f Trade-off prioritization: Scanning reliability &gt; Aesthetics.</li> <li>\ud83d\uddbc\ufe0f Manual labeling of QR code images was performed to train/validate eval models.</li> <li>\ud83d\udc1e A critical bug in evaluation code was found and fixed\u2014it had been misclassifying non-scannable codes.</li> <li>\u2601\ufe0f Inference-time compute scaling boosted success rates during generation.</li> <li>\u2699\ufe0f Modal\u2019s serverless platform enabled efficient compute scaling for dev and deployment.</li> <li>\ud83d\udd01 Iterative process included multiple rounds of parameter tuning and evaluation.</li> <li>\u2705 Final system achieved 95% QR code scanning success, satisfying the SLO.</li> <li>\ud83e\uddea Evaluation timing tip: Implement evals during the second iteration, not the first.</li> <li>\ud83d\udcb8 Charles elaborates on trade-offs between cost, latency, and quality during scaling.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#detailed-summary","title":"\ud83e\uddf5 Detailed Summary","text":"<p>This case study dives into a project led by Charles Frye from Modal, which aimed to generate QR codes that are both visually appealing and functionally scannable using diffusion models. The project originally suffered from the common issue of aesthetic success without practical reliability, as the QR codes looked impressive but frequently failed to scan.</p> <p>To address this, Charles\u2019s team operationalized the goals of \u201clooks good\u201d and \u201cscans.\u201d They incorporated a control net into the diffusion process to guide image generation based on brightness patterns, preserving the QR code\u2019s structure while still allowing for visual customization.</p> <p>The team also implemented an aesthetic predictor model to rank visual outputs, and a QR reader tool to automate the scan-check process. Prioritization became a key engineering decision: they chose to optimize scanning reliability over subjective aesthetics, ensuring end-user functionality.</p> <p>During iterative development, a bug in the evaluation pipeline was discovered that had been misclassifying unscannable QR codes. Fixing this improved the quality of the evaluation system and helped the model generate reliably scannable outputs.</p> <p>To scale performance, they used inference-time compute scaling\u2014allocating more resources dynamically to boost the QR code success rate. This process was made seamless through Modal\u2019s serverless platform, which allowed rapid scaling without infrastructure overhead.</p> <p>The team conducted manual labeling of QR outputs to create high-quality evaluation datasets and fine-tuned model parameters across multiple rounds. As a result, they reached a 95% scan success rate, aligning with their defined Service Level Objective (SLO).</p> <p>Charles also shared strategic advice: introduce evaluation frameworks in the second iteration of a project, once core functionality is stable. He emphasized that early evaluations can be a distraction, whereas well-timed evals lead to actionable insights and performance improvements.</p> <p>He also outlined how to balance trade-offs: - \ud83d\udcb0 Cost: High compute = better outputs but higher billing. - \u23f1\ufe0f Latency: Real-time applications must minimize delay. - \ud83d\udcc8 Quality: Optimal outputs may require aggressive tuning and inference resources.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#lessons-learned","title":"\ud83e\udde0 Lessons Learned","text":"<ul> <li>Define evaluation criteria early, but implement evals iteratively.</li> <li>Use automation for reliability checks (like QR readers).</li> <li>Don't neglect manual labeling\u2014it adds crucial ground truth.</li> <li>Use serverless compute platforms like Modal to avoid DevOps bottlenecks.</li> <li>Always prioritize end-user functionality over subjective output quality when needed.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#references","title":"\ud83d\udcda References","text":"<ul> <li>Modal \u2014 Cloud platform for running ML workloads</li> <li>Diffusion Models \u2014 Illustrated Guide</li> <li>OpenCV QR Code Detection</li> <li>Aesthetic Prediction with CLIP</li> <li>\ud83c\udf99\ufe0f Maven Labs: Interview Recording (Charles Frye on QR Evals)</li> <li>\ud83d\uddbc\ufe0f Google Slides: Evaluation Strategy Slides</li> <li>\ud83c\udfa8 qart.codes \u2014 Artistic QR Code Generator (Open Source)</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/","title":"\ud83d\udcca 10x Your RAG Evaluation by Avoiding These Pitfalls","text":"<p>Key Topics: RAG Evaluation, Pitfall Avoidance, Engineering Practices, Indexing, Hallucinations</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Core Message</li> <li>2. Key Principle: Look at Your Data</li> <li>3. The 7 Pitfalls to Avoid</li> <li>4.1 Corpus Coverage</li> <li>4.2 Information Extraction</li> <li>4.3 Chunk Quality</li> <li>4.4 Query Rejection &amp; Elicitation</li> <li>4.5 Retrieval Sufficiency</li> <li>4.6 Hallucination &amp; Citation Verification</li> <li>4.7 Data Volatility &amp; Reproducibility</li> <li>5. Indexing Evaluation Tips</li> <li>6. Late Interaction (e.g., ColBERT)</li> <li>7. Final Thoughts</li> <li>8. References &amp; Tools</li> <li>\u2b06\ufe0f Back to top</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#1-core-message","title":"1. Core Message","text":"<p>\ud83d\udd25 \"Evaluate everything you can\u2014all the time\u2014independently if possible. And most importantly: look at your data.\" \u2013 Skylar Payne</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#2-key-principle-look-at-your-data","title":"2. Key Principle: Look at Your Data","text":"<p>AI and RAG systems often fail silently\u2014issues accumulate as technical debt. The only way to address these failures is to: - Build systematic evaluation into each RAG pipeline component - Avoid being blind to where problems originate - Empower debugging via component-level visibility</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#3-the-7-pitfalls-to-avoid","title":"3. The 7 Pitfalls to Avoid","text":""},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#41-corpus-coverage","title":"4.1 Corpus Coverage","text":"<ul> <li>Problem: You can't retrieve what's not documented.</li> <li>Fix: </li> <li>Maintain a representative query set.</li> <li>Measure % of queries returning relevant chunks.</li> <li>Audit your corpus for coverage gaps.</li> </ul> <p>\ud83d\udca1 Example: A dev support bot failed to answer \"Salesforce integration\" because no documentation existed, despite the feature being supported.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#42-information-extraction","title":"4.2 Information Extraction","text":"<ul> <li>Problem: Poor extraction from PDFs, code blocks, tables, or complex formats.</li> <li>Fix: </li> <li>Manually verify text extracted into chunks.</li> <li>Validate semantic structure: tables, headers, Markdown formats.</li> <li>Be cautious with OCR models or third-party converters.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#43-chunk-quality","title":"4.3 Chunk Quality","text":"<ul> <li>Problem: Over-chunked or under-chunked content harms retrieval and generation.</li> <li>Fix:</li> <li>Sample and review very large and very small chunks.</li> <li>Track retrieval frequency of chunks:<ul> <li>High frequency? \u2192 Consider putting in prompt directly.</li> <li>Never retrieved? \u2192 Irrelevant or corrupted.</li> </ul> </li> </ul> <p>\ud83d\udca1 Tip: You might not need chunking if pages are small\u2014feed entire documents.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#44-query-rejection-elicitation","title":"4.4 Query Rejection &amp; Elicitation","text":"<ul> <li>Problem: Garbage-in = garbage-out. Many queries are vague or malformed.</li> <li>Fix:</li> <li>Reject ambiguous queries (e.g., \u201cthing broken\u201d).</li> <li>Use elicitation: prompt users to clarify.</li> <li>Track rejection/clarification rates in evaluation.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#45-retrieval-sufficiency","title":"4.5 Retrieval Sufficiency","text":"<ul> <li>Problem: Classic IR metrics (e.g., NDCG, precision@10) don\u2019t assess if enough info is retrieved.</li> <li>Fix:</li> <li>Manually label if retrieved docs are sufficient to answer the query.</li> <li>Think like SAT reading tests: Answer only using the provided passage.</li> <li>Train an \u201cAI judge\u201d to scale this later.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#46-hallucination-citation-verification","title":"4.6 Hallucination &amp; Citation Verification","text":"<ul> <li>Problem: LLMs fill in gaps with hallucinated content.</li> <li>Fix:</li> <li>Enforce in-text citations in generation outputs.</li> <li>Validate:<ul> <li>Citation exists</li> <li>Citation supports the claim via semantic matching</li> </ul> </li> <li>Reference: Jason Liu on Semantic Validation</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#47-data-volatility-reproducibility","title":"4.7 Data Volatility &amp; Reproducibility","text":"<ul> <li>Problem: Indexes or external APIs silently change over time (e.g., Google Search limits).</li> <li>Fix:</li> <li>Add timestamps or versioning to indexed data.</li> <li>Support time travel to reproduce eval results.</li> <li>Ensure evaluation consistency over time.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#5-indexing-evaluation-tips","title":"5. Indexing Evaluation Tips","text":"<p>Evaluate each step of the document \u2192 index pipeline:</p> Step What to Evaluate Extraction Ensure parsing correctness from PDF, JSON, etc. Chunking Chunks should preserve context and be meaningful Embedding Confirm vectors map semantically to document intent Retrieval Retrieval should bring back expected content for key queries <p>Best Practice: Start with end-to-end error analysis, then drill down.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#6-late-interaction-eg-colbert","title":"6. Late Interaction (e.g., ColBERT)","text":"<p>Skylar\u2019s opinion: - Too complex for most use cases - BM25 + embedding search often performs well enough - When retrieval fails, it\u2019s often due to:   - Poor chunking   - Missing query terms   - Not because of scoring method</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#7-final-thoughts","title":"7. Final Thoughts","text":"<ul> <li>AI engineers already have 80% of the skills\u2014just need guidance to cross the finish line.</li> <li>Treat evaluation like software QA:</li> <li>Add observability, logging, unit-like evals</li> <li>Focus on data quality and visibility over clever model tweaks</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#8-references-tools","title":"8. References &amp; Tools","text":"<ul> <li>\ud83d\udd17 ML Tech Debt Paper (Google, 2015)</li> <li>\ud83d\udcc4 InstructorHQ Semantic Validation \u2013 Jason Liu</li> <li>\ud83d\udce6 Quarto \u2014 Tool Skylar used to create the slides</li> <li>\ud83d\udcd8 ColBERT Late Interaction (Not recommended unless justified)</li> </ul> <p>\u2b06\ufe0f Back to top</p>"},{"location":"04-applied-research/02-research-papers/","title":"\ud83d\udcda Research Papers","text":"<p>Stay up to date with cutting-edge academic and industry research driving Generative AI advancements. This section curates key papers with practical relevance for applied development, model tuning, and evaluation insights.</p>"},{"location":"04-applied-research/02-research-papers/#recommended-readings","title":"\ud83d\udcdd Recommended Readings","text":"<p>A handpicked list of foundational and recent research papers shaping the GenAI landscape:</p> <ul> <li> <p>Scaling Laws for Neural Language Models (OpenAI 2020)   Investigates how model size, dataset size, and compute impact performance.</p> </li> <li> <p>Attention Is All You Need (Vaswani et al., 2017)   Introduces the Transformer architecture that underpins nearly all LLMs today.</p> </li> <li> <p>Anthropic\u2019s Constitutional AI   A novel alignment strategy for safe, explainable AI behavior in language models.</p> </li> </ul> <p>\ud83d\udcc2 Back to Applied Research Overview</p> <p>August 2, 2025</p> <p>Back to top</p>"},{"location":"04-applied-research/03-agentic-ai/","title":"\ud83e\udd16 Agentic AI","text":"<p>Discover how intelligent agents can reason, act, and learn autonomously in complex environments. This section explores frameworks, evaluation methods, and applied systems for building agentic behaviors in generative AI.</p>"},{"location":"04-applied-research/03-agentic-ai/#evaluation-driven-development","title":"\u2699\ufe0f Evaluation-Driven Development","text":"<p>Explore how continuous evaluation informs the design of agentic workflows\u2014highlighting trace-based feedback loops, rubric iteration, and human-in-the-loop analysis.</p> <p>\ud83d\udcc2 Back to Applied Research Overview</p> <p>August 2, 2025</p> <p>Back to top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/","title":"Evaluation Driven Development","text":""},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#evaluation-driven-development-for-agentic-systems","title":"Evaluation Driven Development for Agentic Systems","text":"<p>The development process for Agentic Systems, particularly those based on Large Language Models (LLMs), is a continuous, iterative loop that prioritizes evaluation and feedback for successful evolution . This system aims to avoid common pitfalls by structuring the entire lifecycle from idea to production .</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Defining The Problem</li> <li>2. Building a Prototype</li> <li>3. Defining Performance Metrics</li> <li>4. Defining Evaluation Rules</li> <li>5. Building a Proof of Concept (PoC)</li> <li>6. Instrumenting the Application (Observability)</li> <li>7. Integrating with an Observability Platform</li> <li>8. Evaluating Traced Data</li> <li>9. Evolving the Application</li> <li>10. Exposing New Versions of the Application</li> <li>11. Continuous Development and Evolution of the Application</li> <li>12. Monitoring and Alerting</li> </ul>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#1-defining-the-problem","title":"1. Defining The Problem","text":"<p>This is the initial and vital step for any Agentic System development . *   Goal: Ensure the problem is clearly defined, bounded, and aligned with business goals . Agentic Systems use LLMs or other GenAI models to solve complex, real-world problems, often involving automation . *   Key Questions:     *   Is the problem best solved by AI or traditional software?      *   Who is the end user?      *   What are the edge cases?      *   What are the boundaries of acceptable behavior?  *   Important: Many AI projects fail not due to bad models, but due to solving the wrong problem . *   Roles Involved: AI Product Managers, Domain Experts, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#visual-reference","title":"\ud83d\udcca Visual Reference","text":""},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#2-building-a-prototype","title":"2. Building a Prototype","text":"<p>After confirming AI is a good fit, the next step is rapid prototyping . *   Goal: Primarily a learning phase to assess technical feasibility and de-risk the idea . *   Key Considerations:     *   Use Notebooks or no-code tools, small datasets, and off-the-shelf models .     *   Focus on learning, not initial performance .     *   Document everything to avoid repeating mistakes .     *   Involve prompt engineering and market research for potential tools (e.g., Voice to Text platforms) . *   Roles Involved: AI Product Managers, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#3-defining-performance-metrics","title":"3. Defining Performance Metrics","text":"<p>Every application should solve a real business problem, which needs to be grounded in specific metrics . *   Goal: Optimizing for a \"north star output metric\" (e.g., reduce headcount, improve user satisfaction, increase development velocity) and breaking it down into \"input metrics\" that the application will directly target (e.g., reduce average customer support ticket resolution time) . *   Important: Without properly setting this stage, the project risks being deprioritized for not demonstrating enough business value . Alignment with business stakeholders is crucial before implementation . *   Roles Involved: AI Product Managers, AI Engineers, Business Stakeholders .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#4-defining-evaluation-rules","title":"4. Defining Evaluation Rules","text":"<p>Given the complexities of metrics for LLMs (e.g., human alignment, coherence, factuality), defining exact evaluation rules is highly beneficial . *   Goal: Establish clear criteria for judging system responses, especially for chained LLM calls within an Agentic System topology . *   Key Considerations:     *   Prepare an evaluation dataset (Inputs \u2192 Expected Outputs) for each node in your Agentic System topology .     *   Define unacceptable responses (e.g., toxicity, hallucinations, unsafe suggestions) . *   Roles Involved: AI Product Managers, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#5-building-a-proof-of-concept-poc","title":"5. Building a Proof of Concept (PoC)","text":"<p>This stage emphasizes getting the system into users' hands as quickly as possible . *   Goal: Rapidly push out a user-facing application to gather crucial \"unknown unknowns\" from user feedback [9, 10]. *   Key Considerations:     *   Use LLM APIs from providers like OpenAI, Google, Anthropic, etc., for quick development .     *   The PoC can be as simple as an Excel Spreadsheet with input/output pairs, as long as it helps move metrics forward and is exposed .     *   User feedback is key to shifting perspectives on application improvement . *   Important: If you can't push it out quickly, there's an issue with the process . A successful LLM PoC may look like an Excel Spreadsheet . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#6-instrumenting-the-application-observability","title":"6. Instrumenting the Application (Observability)","text":"<p>This is a key element in implementing Evaluation Driven Development . *   Goal: Implement observability best practices by logging an extensive set of metadata about everything happening within the LLM-based system . *   Key Considerations:     *   Log everything: prompts, completions, embeddings, latency, token counts, and user feedback .     *   Add additional metadata: prompt versions, user inputs, model versions used .     *   Ensure proper connection and ordering of operations within chains, as outputs of one LLM call often become inputs to the next .     *   Log multimodal data (PDFs, images, audio, video) .     *   Crucially, attach user feedback to the traces representing the user interaction when feedback was provided . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#7-integrating-with-an-observability-platform","title":"7. Integrating with an Observability Platform","text":"<p>Beyond just tracking data, efficient visualization and analysis are crucial . *   Goal: Utilize an observability platform for efficient search, visualization, prompt versioning, and automated evaluation capabilities . *   Key Considerations:     *   Store evaluation rules within the platform to apply them to traces .     *   Use platforms as Prompt Registries to analyze and group evaluation results by Prompt Groups, as your application is a chain of prompts .     *   Benefit from smart sampling algorithms for cost-effective storage of traces at scale, as storing all traces can become too expensive .     *   Utilize platform-specific tracing SDKs for seamless instrumentation . *   Important: Set this up early as it brings visibility to the \"black box\" of LLM applications . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#8-evaluating-traced-data","title":"8. Evaluating Traced Data","text":"<p>With instrumentation and platform integration, you can now measure your application effectively . *   Goal: Automatically run evaluations on the collected trace data, especially focusing on identifying failures . *   Key Considerations:     *   Assumption: Evaluation rules are stored in the Observability platform, and traces with human feedback are connected .     *   Automatically run evaluations on traces hitting the Observability Platform .     *   Filter out traces with failing evaluations or negative human feedback; this \"failing\" data will be the primary focus for improvement . *   Important: Running evaluations can be expensive (especially LLM-as-a-judge tactics), so sampling traces might be necessary . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#9-evolving-the-application","title":"9. Evolving the Application","text":"<p>This is where the application is improved based on data-driven insights . *   Goal: Enhance the application by focusing on failing evaluations and human feedback . *   Key Considerations:     *   Start with better prompt engineering, data preprocessing, and tool integration before increasing system complexity .     *   Increase complexity (e.g., Simple Prompts \u2192 RAG \u2192 Agentic RAG \u2192 Agents \u2192 Multi-agent systems) only if there's a hard requirement and the current topology is not up to the task .     *   Maintain a \"failing eval dataset\" that is continuously fed with new failing samples and is never 100% \"solved\" . Your goal is to achieve 100% but by adding more failing samples, you never get there . *   Important: Always involve Domain Experts at this stage for their insider knowledge and potential prompt suggestions . *   Roles Involved: AI Engineers, Domain Experts .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#10-exposing-new-versions-of-the-application","title":"10. Exposing New Versions of the Application","text":"<p>Rapid deployment of new versions is crucial for continuous improvement . *   Goal: Quickly release updates to incorporate fixes and improvements, leveraging invaluable feedback . *   Key Considerations:     *   Fast deployment improves user experience by fixing present problems, and some fixes can generalize to unknown problems .     *   Implement strict release tests and integrate evaluation datasets into CI/CD pipelines to ensure new releases do not degrade performance compared to previous versions . *   Roles Involved: AI Engineers, Domain Experts .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#11-continuous-development-and-evolution-of-the-application","title":"11. Continuous Development and Evolution of the Application","text":"<p>This represents the core iterative loop of the Evaluation Driven Development process . *   Cycle: Build \u2192 Trace, collect feedback \u2192 Evaluate \u2192 Focus on Failing Evals and Negative Feedback \u2192 Improve the application \u2192 Iterate . *   Goal: Continuously evolve the application, adding new functionalities (often as new routes in the Agentic System Topology) by following the same process of prototyping, defining metrics, and new evaluations [19, 20]. For example, a simple chatbot can evolve into a system that manages a shopping cart . *   Roles Involved: AI Engineers, Domain Experts, AI Product Managers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#12-monitoring-and-alerting","title":"12. Monitoring and Alerting","text":"<p>After implementing tracing and evaluation for development, monitoring almost comes out of the box . *   Goal: Reuse implemented evaluations and traces for production monitoring and configure specific alerting thresholds . *   Key Considerations:     *   Most relevant data for LLM-specific production monitoring is already available if application instrumentation was properly implemented .     *   Consider tracing and logging additional advanced metrics like TTFT (Time To First Token) and inter-token latency .     *   You will need to figure out the threshold for alerting . *   Important: Try to avoid Alert Fatigue by carefully configuring thresholds that trigger alerts and avoid False Positives as much as possible . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#external-reference","title":"\ud83d\udd17 External Reference","text":"<p>Evaluation Driven Development for Agentic Systems - SwirlAI</p>"},{"location":"05-community-qna/","title":"\ud83d\udcd8 Overview of the Community &amp; Q&amp;A section","text":"<p>\ud83d\udcc2 Navigation Tip This space is for learner-driven insights, common challenges, and collaborative knowledge. Use the sidebar to browse Q&amp;A threads, study tips, glossary terms, discussion prompts, and personal learning reflections.</p> <p>\ud83d\udca1 Use Glossary for quick terminology, Study Tips &amp; Pitfalls to avoid common mistakes, and Learning Journal to track your personal progress.</p>"},{"location":"05-community-qna/discussion-feedback/","title":"\ufffd\ufffd Discussion &amp; Feedback","text":"<p>Leave comments, suggestions, or feedback here.</p>"},{"location":"05-community-qna/general-questions/","title":"\u2753 General Questions","text":"<p>Cross-topic or uncategorized Q&amp;A go here.</p>"},{"location":"05-community-qna/glossary/","title":"\ud83d\udcda Glossary","text":"<p>Important terms and definitions.</p>"},{"location":"05-community-qna/learning-journal/","title":"\ud83d\udcdd Learning Journal","text":"<p>Log your progress, insights, and reflections.</p>"},{"location":"05-community-qna/study-tips-pitfalls/","title":"\u26a0\ufe0f Study Tips &amp; Pitfalls","text":"<p>Common learning mistakes and how to avoid them.</p>"},{"location":"auth/","title":"\ud83d\udd10 Sign In to Gen AI Atlas","text":"Email: Password: \ud83d\udd13 Sign In \ud83d\udcdd Register <p>\ud83d\udd04 Checking login status...</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gen AI Atlas \u2014 Navigation Overview","text":""},{"location":"#foundation","title":"Foundation","text":"<p>Build your essential knowledge with modules in Statistics, Python Programming, and Data Science. These resources provide the groundwork for all modern AI and data workflows.</p>"},{"location":"#generative-ai-core","title":"\ud83e\udd16 Generative AI Core","text":"<p>Deep dive into the world of generative models and natural language processing: Transformers &amp; Large Language Models (LLMs), RAG and LangChain, Prompt Engineering, and NLP Techniques. Master the technologies at the heart of today's AI revolution.</p>"},{"location":"#deployment-operations","title":"\ud83d\ude80 Deployment &amp; Operations","text":"<p>Go from research to production with real-world tools: LangChain, LLMOps, and AI Evaluation (Evals). Learn best practices for building, deploying, and managing robust AI systems.</p>"},{"location":"#research-advanced-topics","title":"\ud83d\udd2c Research &amp; Advanced Topics","text":"<p>Stay ahead with explorations into cutting-edge research, agentic AI, retrieval-augmented generation (RAG), and applied use cases. Advance your skills and contribute to the future of AI.</p>"},{"location":"#community-qa","title":"\ud83c\udf10 Community &amp; Q&amp;A","text":"<p>Connect, share, and learn with the Gen AI Atlas community: Discussion boards, blog posts, and Q&amp;A forums. Find support, share insights, and grow your network.</p> <p>Note: Please use the Sign In button at the top right to access additional features or personalize your experience. Stay tuned for more topics and resources coming soon!</p>"},{"location":"01-foundation/","title":"\ud83e\uddf1 Foundation","text":"<p>\ud83d\udcc2 Navigation Tip This section lays the groundwork for all advanced topics in Generative AI. Use the left-hand navigation menu to explore foundational topics including Python basics, statistics, and data science &amp; machine learning.</p> <p>\ud83d\udca1 Begin with Python to ensure your programming fundamentals are solid before diving into statistical reasoning or ML concepts.</p>"},{"location":"01-foundation/1-python/","title":"\ud83d\udc0d Python","text":"<p>Welcome to the Python section of the Generative AI Atlas.</p>"},{"location":"01-foundation/1-python/#learning-path","title":"\ud83e\udded Learning Path","text":"<p>Navigate through structured lessons to master core concepts.</p> <ul> <li>Intro to Python</li> <li>Collection of Variables</li> <li>Conditional Looping</li> <li>Functions</li> <li>NumPy</li> <li>Pandas</li> <li>Data Visualization</li> </ul>"},{"location":"01-foundation/1-python/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Deep dive into real-world case studies, extended examples, and supporting materials.</p> <ul> <li>Case Study: CardioGood Fitness</li> <li>\"Case Study: Cardiogood \": 01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness.md</li> <li>\"Case Study: Uber\": 01-foundation/1-python/2-additional-reference/02-case-study-uber.md</li> <li>\"Case Study: EDA\": 01-foundation/1-python/2-additional-reference/03-case-study-EDA.md</li> </ul>"},{"location":"01-foundation/1-python/#qa","title":"\u2753 Q&amp;A","text":"<p>Explore discussions, clarifications, and annotated insights from real learners and experts.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/","title":"\ud83e\udde0 Statistics &amp; Data Science: Python Introduction","text":""},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Print Statements</li> <li>2. Variables</li> <li>3. Data Types</li> <li>4. Basic Operators</li> <li>5. Data Structures \u2013 List</li> <li>6. Data Structures \u2013 Tuple</li> <li>7. Data Structures \u2013 Dictionary</li> <li>8. Type Checking</li> <li>9. Comparison Operators</li> <li>10. Conditional Statements</li> <li>12. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#1-print-statements","title":"1. Print Statements","text":"<ul> <li>Python\u2019s <code>print()</code> function is used to display output.</li> </ul> <pre><code>print('The name of the company is Cars Sons Ltd.')\nprint('The year the company was established is', 1996)\nprint('Total turnover of the company this year in Million $ is', 12.5)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example","title":"Use Case Example","text":"<p>Essential for logging, debugging, and showing results of analysis.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#2-variables","title":"2. Variables","text":"<ul> <li>Containers for storing data values.</li> </ul> <pre><code>company_name = \"Cars Sons Ltd.\"\nyear_started = 1996\nturnover = 12.5\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_1","title":"Use Case Example","text":"<p>Used to store company attributes, which feed into models or business dashboards.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#3-data-types","title":"3. Data Types","text":"<ul> <li><code>int</code>, <code>float</code>, <code>str</code> are common types.</li> </ul> <pre><code>type(company_name)  # str\ntype(year_started)  # int\ntype(turnover)      # float\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_2","title":"Use Case Example","text":"<p>Understanding data types is essential for error-free data preprocessing.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#4-basic-operators","title":"4. Basic Operators","text":"<ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>, <code>**</code></li> </ul> <pre><code>price_sedan = 0.2\ntotal_price = price_sedan * 10\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_3","title":"Use Case Example","text":"<p>Used in calculating sales metrics or KPIs in business analytics.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#5-data-structures-list","title":"5. Data Structures \u2013 List","text":"<ul> <li>Lists store multiple items.</li> </ul> <pre><code>cars = ['Sedan', 'SUV', 'Hatchback']\nsales = [200, 400, 300]\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_4","title":"Use Case Example","text":"<p>Track car types or regional sales in customer segmentation.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#6-data-structures-tuple","title":"6. Data Structures \u2013 Tuple","text":"<ul> <li>Immutable ordered collections.</li> </ul> <pre><code>sitting = (5, 4, 6)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_5","title":"Use Case Example","text":"<p>Store constant metadata like seat capacity or configuration specs.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#7-data-structures-dictionary","title":"7. Data Structures \u2013 Dictionary","text":"<ul> <li>Key-value pairs for structured data.</li> </ul> <pre><code>sales_last_month = {'Sedan': 2, 'SUV': 1.5}\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_6","title":"Use Case Example","text":"<p>Used for storing car-wise sales figures, performance ratings, etc.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#8-type-checking","title":"8. Type Checking","text":"<pre><code>type(sales_last_month)  # dict\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_7","title":"Use Case Example","text":"<p>Validating type before transformations in ETL workflows.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#9-comparison-operators","title":"9. Comparison Operators","text":"<ul> <li>Compare data values: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code></li> </ul> <pre><code>sales['Sedan'] &gt; sales['Hatchback']\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_8","title":"Use Case Example","text":"<p>Used in dashboards to compare performance trends.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#10-conditional-statements","title":"10. Conditional Statements","text":"<ul> <li>Use <code>if</code>, <code>else</code> to apply logic</li> </ul> <pre><code>if cars_ratings['Sedan_1'] &gt; 50:\n    print(\"Good performer\")\nelse:\n    print(\"Needs improvement\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#use-case-example_9","title":"Use Case Example","text":"<p>Flag underperforming models for review.</p> <p>\ud83d\udc49 Open in Colab Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/01-intro-python/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python</li> <li>Khan Academy Statistics</li> <li>MIT OpenCourseWare</li> <li>StatQuest (YouTube)</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/","title":"\ud83e\udde0 Statistics &amp; Data Science: Collection of Variables","text":"<p>This document provides hands-on practice with foundational Python data structures \u2014 specifically lists, dictionaries, and tuples \u2014 used throughout data science and statistics programming.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. List Operations</li> <li>2. Dictionary Operations</li> <li>3. Tuple &amp; Set Creation</li> <li>\ud83d\udcc2 CSV Download</li> <li>\ud83d\udd17 References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#1-list-operations","title":"1. List Operations","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept","title":"Concept","text":"<p>A list in Python is a mutable, ordered collection of elements. Lists allow duplicate values and support indexing, slicing, and iteration.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-examples","title":"Key Examples","text":"<ul> <li>Create a list with repeated elements</li> <li>Print the second element of a list</li> <li>Replace an element in a list</li> <li>Iterate through a list and print values</li> <li>Compute squares of numbers in a list</li> <li>Remove (pop) an element from a list</li> <li>Check the length of a list</li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example","title":"Use Case Example","text":"<p>In retail analytics, you might use lists to track product IDs sold during a day, apply transformations (e.g., discounts), or aggregate values like revenue per transaction.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#2-dictionary-operations","title":"2. Dictionary Operations","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept_1","title":"Concept","text":"<p>A dictionary in Python is an unordered, mutable collection that maps keys to values. Keys must be unique and hashable.</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-examples_1","title":"Key Examples","text":"<ul> <li>Create a dictionary with brand, model, and year</li> <li>Modify a value in a dictionary (e.g., updating year)</li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example_1","title":"Use Case Example","text":"<p>Customer profiles in CRM systems often use dictionaries to store user information like <code>{\"name\": \"John\", \"age\": 45, \"premium\": True}</code>.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#3-tuple-set-creation","title":"3. Tuple &amp; Set Creation","text":""},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#concept_2","title":"Concept","text":"<ul> <li>A tuple is an immutable, ordered sequence.</li> <li>A set is an unordered collection of unique elements.</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#key-example","title":"Key Example","text":"<ul> <li>Create a set with elements: <code>1.0</code>, <code>\"Hello\"</code>, <code>55</code>, <code>(6,7,8)</code></li> </ul> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#use-case-example_2","title":"Use Case Example","text":"<p>Sets are used to eliminate duplicates, such as identifying unique website visitors from a log file.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/02-collection-of-variables/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Documentation</li> <li>Seaborn Documentation</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare</li> <li>StatQuest YouTube Channel</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/","title":"\ud83e\udde0 Statistics &amp; Data Science: Control Flow in Python","text":""},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Conditional Statements</li> <li>2. Looping in Python</li> <li>2.1 For Loop</li> <li>2.2 While Loop</li> <li>2.3 For Loop with Conditional Statements</li> <li>2.4 For Loop with Accumulation</li> <li>2.5 For Loop with Filtering</li> <li>2.6 For Loop with Character Iteration</li> <li>2.7 For Loop with Repetition</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#1-conditional-statements","title":"1. Conditional Statements","text":"<ul> <li>Conditional statements allow programs to make decisions based on conditions.</li> <li>Commonly used keywords: <code>if</code>, <code>elif</code>, <code>else</code>.</li> </ul> <pre><code># Example: Check if a number is even or odd\na = 2020\nif a % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#use-case-example","title":"Use Case Example","text":"<p>Conditional statements are essential for business logic, e.g., evaluating credit scores to approve/reject a loan.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#2-looping-in-python","title":"2. Looping in Python","text":"<p>Loops allow you to repeat a block of code multiple times.</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#21-for-loop","title":"2.1 For Loop","text":"<ul> <li>Used to iterate over a sequence (list, tuple, range, etc.)</li> </ul> <pre><code>for i in range(5):\n    print(i)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#22-while-loop","title":"2.2 While Loop","text":"<ul> <li>Repeats as long as a condition is true</li> </ul> <pre><code>i = 1\nwhile i &lt;= 5:\n    print(i)\n    i += 1\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#23-for-loop-with-conditional-statements","title":"2.3 For Loop with Conditional Statements","text":"<pre><code>for i in range(1, 11):\n    if i % 2 == 0:\n        print(f\"{i} is even\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#24-for-loop-with-accumulation","title":"2.4 For Loop with Accumulation","text":"<pre><code>total = 0\nfor i in range(1, 101):\n    total += i\nprint(total)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#25-for-loop-with-filtering","title":"2.5 For Loop with Filtering","text":"<pre><code>for i in range(1, 51):\n    if i % 7 == 0 and i % 5 != 0:\n        print(i)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#26-for-loop-with-character-iteration","title":"2.6 For Loop with Character Iteration","text":"<pre><code>st = \"Data Science\"\nfor char in st:\n    print(char)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#27-for-loop-with-repetition","title":"2.7 For Loop with Repetition","text":"<pre><code>for _ in range(5):\n    print(\"Data Science\")\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#use-case-example_1","title":"Use Case Example","text":"<p>Looping constructs are widely used in automation, report generation, and simulations in data analysis workflows.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/03-conditional-looping/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python</li> <li>Khan Academy Statistics</li> <li>MIT OpenCourseWare</li> <li>StatQuest (YouTube)</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/","title":"\ud83e\udde0 Statistics &amp; Data Science \u2013 Python Functions","text":""},{"location":"01-foundation/1-python/1-learning-path/04-functions/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Basic Arithmetic Function</li> <li>2. Squares in Range</li> <li>3. Simple Interest Calculation</li> <li>4. Divisibility by 25</li> <li>5. Square and Add Five</li> <li>6. Lambda: Square and Add Five</li> <li>7. Power Function</li> <li>8. Triangle Area</li> <li>9. Country Origin Function</li> <li>10. Celsius to Fahrenheit</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#1-basic-arithmetic-function","title":"1. Basic Arithmetic Function","text":"<p>Define a function to add, subtract, multiply, and divide two variables.</p> <pre><code>def perform_operations(a, b):\n    print(f\"Addition: {a + b}\")\n    print(f\"Subtraction: {a - b}\")\n    print(f\"Multiply: {a * b}\")\n    if b != 0:\n        print(f\"Division: {a / b}\")\n    else:\n        print(\"Division by zero is not allowed.\")\n\nperform_operations(10, 5)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#2-squares-in-range","title":"2. Squares in Range","text":"<p>Function to print squares of numbers from 1 to 10.</p> <pre><code>def print_squares_in_range():\n    for i in range(1, 11):\n        print(i * i)\n\nprint_squares_in_range()\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#3-simple-interest-calculation","title":"3. Simple Interest Calculation","text":"<p>Formula: \\(SI = \\frac{P \\times R \\times T}{100}\\)</p> <pre><code>def calculate_simple_interest(principal, rate, time):\n    return (principal * rate * time) / 100\n\ncalculate_simple_interest(1000, 3, 5)\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#4-divisibility-by-25","title":"4. Divisibility by 25","text":"<pre><code>def is_divisible_by_25(number):\n    return True if number % 25 == 0 else \"Not divisible\"\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#5-square-and-add-five","title":"5. Square and Add Five","text":"<pre><code>def square_and_add_five(number):\n    return number ** 2 + 5\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#6-lambda-square-and-add-five","title":"6. Lambda: Square and Add Five","text":"<pre><code>square_add_five_lambda = lambda x: x ** 2 + 5\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#7-power-function","title":"7. Power Function","text":"<pre><code>def calculate_power(base, exponent):\n    return base ** exponent\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#8-triangle-area","title":"8. Triangle Area","text":"<p>Formula: \\(Area = \\frac{1}{2} \\times base \\times height\\)</p> <pre><code>def calculate_triangle_area(base, height):\n    return 0.5 * base * height\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#9-country-origin-function","title":"9. Country Origin Function","text":"<pre><code>def print_country_origin(country):\n    return f\"I am from {country}\"\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#10-celsius-to-fahrenheit","title":"10. Celsius to Fahrenheit","text":"<p>Formula: \\(F = \\frac{9}{5}C + 32\\)</p> <pre><code>def celsius_to_fahrenheit(celsius):\n    return (celsius * 9 / 5) + 32\n</code></pre> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/04-functions/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Docs</li> <li>Khan Academy - Statistics</li> <li>MIT OCW</li> <li>StatQuest with Josh Starmer</li> <li>Scikit-learn User Guide</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/","title":"\ud83e\uddee Chapter 1.1: NumPy","text":""},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to NumPy</li> <li>2. NumPy Functions</li> <li>3. Accessing NumPy Array</li> <li>4. Modifying the Entries of a Matrix</li> <li>5. Saving and Loading NumPy Arrays</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#1-introduction-to-numpy","title":"1. Introduction to NumPy","text":"<p>NumPy (Numerical Python) is a core Python library for scientific computing, offering fast, flexible multi-dimensional <code>ndarray</code> objects and a suite of mathematical tools(numpy.org, w3schools.com, stackoverflow.com, geeksforgeeks.org). Arrays are stored in contiguous memory, enabling vectorized operations that run orders of magnitude faster than pure Python loops(geeksforgeeks.org).</p> <pre><code>import numpy as np\n\narray = np.array([1, 2, 3])\nprint(array)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example","title":"Use Case Example","text":"<p>\ud83d\udcca NumPy is used in financial analysis for high-speed matrix operations when simulating stock market behaviors.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#2-numpy-functions","title":"2. NumPy Functions","text":"<p>Common NumPy functions include reshaping, random number generation, mathematical operations, and broadcasting.</p> <pre><code>a = np.arange(10).reshape(2, 5)\nb = np.random.rand(2, 5)\nprint(np.add(a, b))\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83e\udde0 Neuroscience labs use NumPy to simulate electrical signals across neurons represented in matrix form.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#3-accessing-numpy-array","title":"3. Accessing NumPy Array","text":"<p>Accessing elements is done with indexing and slicing.</p> <pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr[0, 1])  # Output: 2\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcc8 Sports analysts extract specific performance metrics from multidimensional datasets using array indexing.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#4-modifying-the-entries-of-a-matrix","title":"4. Modifying the Entries of a Matrix","text":"<p>You can change values in NumPy arrays directly using indexing or conditions.</p> <pre><code>matrix = np.array([[1, 2], [3, 4]])\nmatrix[0, 1] = 10\nprint(matrix)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83e\uddec Bioinformaticians adjust gene expression data arrays during cleaning and normalization stages.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#5-saving-and-loading-numpy-arrays","title":"5. Saving and Loading NumPy Arrays","text":"<p>Use <code>.npy</code> and <code>.npz</code> formats for saving and restoring arrays efficiently.</p> <pre><code>np.save(\"my_array.npy\", arr)\nloaded = np.load(\"my_array.npy\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcc2 Machine learning engineers store preprocessed input arrays for quick reuse across experiments.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/05-numpy/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/","title":"\ud83d\udc3c Chapter 1.2: Pandas","text":""},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Pandas</li> <li>2. Accessing Series and DataFrames</li> <li>3. loc and iloc in Pandas</li> <li>4. Condition-Based Indexing</li> <li>5. Combining DataFrames</li> <li>6. Saving and Loading DataFrames</li> <li>7. Statistical Functions</li> <li>8. GroupBy Function</li> <li>9. Date and Time Functions</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#1-introduction-to-pandas","title":"1. Introduction to Pandas","text":"<p>Pandas is a powerful Python library for data manipulation and analysis. It provides data structures like Series and DataFrames.</p> <pre><code>import pandas as pd\n\ndata = {\"Name\": [\"Alice\", \"Bob\"], \"Age\": [25, 30]}\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example","title":"Use Case Example","text":"<p>\ud83c\udfe5 Hospitals use Pandas to store and manipulate patient records for analysis and visualization.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#2-accessing-series-and-dataframes","title":"2. Accessing Series and DataFrames","text":"<p>Pandas Series is a one-dimensional labeled array, and DataFrame is a two-dimensional table.</p> <pre><code>series = df[\"Age\"]\nprint(series)\nprint(type(series))\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83d\udcbc HR analysts retrieve employee age or salary columns to perform segmentation.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#3-loc-and-iloc-in-pandas","title":"3. loc and iloc in Pandas","text":"<ul> <li><code>.loc[]</code> accesses rows by label.</li> <li><code>.iloc[]</code> accesses rows by index position.</li> </ul> <pre><code>print(df.loc[0])   # Access by label\nprint(df.iloc[1])  # Access by index\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcca Researchers extract specific survey responses using row labels or positions.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#4-condition-based-indexing","title":"4. Condition-Based Indexing","text":"<p>Filter DataFrames using Boolean conditions.</p> <pre><code>df[df[\"Age\"] &gt; 26]\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83c\udfe2 Companies filter customers by age for targeted advertising campaigns.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#5-combining-dataframes","title":"5. Combining DataFrames","text":"<p>You can concatenate or merge multiple DataFrames.</p> <pre><code>df1 = pd.DataFrame({\"ID\": [1, 2], \"Name\": [\"Alice\", \"Bob\"]})\ndf2 = pd.DataFrame({\"ID\": [1, 2], \"Age\": [25, 30]})\nmerged = pd.merge(df1, df2, on=\"ID\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udce6 Merging order and customer data allows fulfillment centers to improve logistics tracking.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#6-saving-and-loading-dataframes","title":"6. Saving and Loading DataFrames","text":"<p>Save and load datasets using CSV or Excel formats.</p> <pre><code>df.to_csv(\"output.csv\", index=False)\ndf_loaded = pd.read_csv(\"output.csv\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83d\udcbd Analysts export cleaned datasets to share with business teams or dashboards.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#7-statistical-functions","title":"7. Statistical Functions","text":"<p>Pandas includes descriptive statistics like mean, median, std, etc.</p> <pre><code>df[\"Age\"].mean()\ndf.describe()\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcc9 Healthcare analysts summarize patient vitals and lab results for reports.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#8-groupby-function","title":"8. GroupBy Function","text":"<p>Group data by a categorical variable and apply aggregate functions.</p> <pre><code>df.groupby(\"Name\")[\"Age\"].mean()\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83d\udecd\ufe0f Retailers group transactions by store to compute average revenue per store.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#9-date-and-time-functions","title":"9. Date and Time Functions","text":"<p>Convert and manipulate datetime formats in Pandas.</p> <pre><code>df[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Year\"] = df[\"Date\"].dt.year\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#use-case-example_8","title":"Use Case Example","text":"<p>\ud83d\udcc5 Analysts break down sales by month or quarter using datetime fields.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download StockData.csv</p> <p>\ud83d\udcce View StockData.csv in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/06-pandas/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/","title":"\ud83d\udcca Chapter 1.3: Data Visualization","text":""},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Data Loading and Overview</li> <li>2. Histogram</li> <li>3. Histogram with Density Curve</li> <li>4. Box Plot</li> <li>5. Line Plot</li> <li>6. Scatter Plot</li> <li>7. lm Plot in Seaborn</li> <li>8. Swarm Plot</li> <li>9. Pair Plot</li> <li>10. Heat Map</li> <li>11. Plotly</li> <li>12. Customizing Plots</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#1-data-loading-and-overview","title":"1. Data Loading and Overview","text":"<p>Data loading is the first step in any data visualization or analysis pipeline. It involves reading data from CSV, Excel, or APIs using libraries like <code>pandas</code>.</p> <ul> <li>Use <code>pandas.read_csv()</code> to load CSV files.</li> <li>Understand data types, null values, and summary statistics.</li> </ul> <pre><code>import pandas as pd\ndf = pd.read_csv(\"your_dataset.csv\")\ndf.info()\ndf.describe()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example","title":"Use Case Example","text":"<p>\ud83d\udcc8 In healthcare, loading large hospital datasets with patient records allows data scientists to quickly inspect missing values and perform visual triage of data quality.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#2-histogram","title":"2. Histogram","text":"<p>Histograms display the distribution of numerical data by grouping values into bins.</p> <ul> <li>Great for identifying skewness, spread, and outliers.</li> <li>Created using <code>seaborn.histplot()</code> or <code>matplotlib.pyplot.hist()</code>.</li> </ul> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(df[\"age\"], bins=10)\nplt.show()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83c\udf93 Universities use histograms to visualize student grade distributions across departments.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#3-histogram-with-density-curve","title":"3. Histogram with Density Curve","text":"<p>Combining histograms with KDE (Kernel Density Estimation) overlays helps visualize probability density.</p> <pre><code>sns.histplot(df[\"income\"], kde=True)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcb0 Financial analysts use density curves over income brackets to detect abnormal income distributions for fraud detection.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#4-box-plot","title":"4. Box Plot","text":"<p>Box plots summarize the distribution using median, quartiles, and outliers.</p> <pre><code>sns.boxplot(x=\"region\", y=\"salary\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83c\udfe5 Hospitals compare patient waiting times by department using box plots to identify service bottlenecks.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#5-line-plot","title":"5. Line Plot","text":"<p>Line plots show trends over time or ordered categories.</p> <pre><code>sns.lineplot(x=\"year\", y=\"revenue\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcc9 Economists visualize GDP trends over decades with line plots.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#6-scatter-plot","title":"6. Scatter Plot","text":"<p>Scatter plots reveal relationships between two numeric variables.</p> <pre><code>sns.scatterplot(x=\"height\", y=\"weight\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83d\udcca Insurance companies use scatter plots to assess correlations between age and insurance claims.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#7-lm-plot-in-seaborn","title":"7. lm Plot in Seaborn","text":"<p>lm plots add regression lines to scatter plots, useful for trend analysis.</p> <pre><code>sns.lmplot(x=\"experience\", y=\"salary\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcbc HR departments forecast salary expectations based on years of experience.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#8-swarm-plot","title":"8. Swarm Plot","text":"<p>Swarm plots show all data points and avoid overlapping unlike box plots.</p> <pre><code>sns.swarmplot(x=\"department\", y=\"satisfaction\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83e\uddea Clinical research teams visualize patient responses to different treatments using swarm plots.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#9-pair-plot","title":"9. Pair Plot","text":"<p>Pair plots visualize relationships across multiple variables in one grid.</p> <pre><code>sns.pairplot(df[[\"age\", \"income\", \"expenses\"]])\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_8","title":"Use Case Example","text":"<p>\ud83d\udcca Marketing teams explore consumer segmentation by analyzing patterns in spending behavior.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#10-heat-map","title":"10. Heat Map","text":"<p>Heatmaps visualize matrix-like data using color intensities.</p> <pre><code>sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_9","title":"Use Case Example","text":"<p>\ud83d\udd0d Data scientists use heatmaps to visualize correlation matrices for feature selection in ML pipelines.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#11-plotly","title":"11. Plotly","text":"<p>Plotly provides interactive, zoomable, and publishable charts in Python.</p> <pre><code>import plotly.express as px\npx.scatter(df, x=\"age\", y=\"income\", color=\"region\")\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_10","title":"Use Case Example","text":"<p>\ud83d\udcca News outlets use Plotly to create interactive COVID-19 dashboards for public engagement.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#12-customizing-plots","title":"12. Customizing Plots","text":"<p>Customize color palettes, themes, fonts, and axis labels for clearer visuals.</p> <pre><code>sns.set(style=\"whitegrid\", palette=\"pastel\")\nsns.boxplot(x=\"gender\", y=\"score\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#use-case-example_11","title":"Use Case Example","text":"<p>\ud83d\udce2 In presentations, using color-blind-friendly palettes ensures accessibility for all stakeholders.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/1-learning-path/07-data-visualization/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/","title":"\ud83e\udde0 Case Study: CardioGood Fitness Data Analysis","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#1-overview","title":"1. Overview","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#problem-statement-cardiogood-fitness-data-analysis","title":"Problem Statement \u2013 CardioGood Fitness Data Analysis","text":"<p>Context: The market research team at AdRight is assigned the task to identify the profile of the typical customer for each treadmill product offered by CardioGood Fitness. The market research team decides to investigate whether there are differences across the product lines with respect to customer characteristics. The team decides to collect data on individuals who purchased a treadmill at a CardioGood Fitness retail store at any time in the past three months. The data is stored in the <code>CardioGoodFitness.csv</code> file.</p> <p>Objective: Perform descriptive analysis to create a customer profile for each CardioGood Fitness treadmill product line.</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#2-dataset-description","title":"2. Dataset Description","text":"<p>Data Dictionary: The team identified the following customer variables to study:</p> <ul> <li>Product: Product purchased - TM195, TM498, or TM798  </li> <li>Gender: Male or Female  </li> <li>Age: Age of the customer in years  </li> <li>Education: Education of the customer in years  </li> <li>MaritalStatus: Single or Partnered  </li> <li>Income: Annual household income  </li> <li>Usage: The average number of times the customer plans to use the treadmill each week  </li> <li>Miles: The average number of miles the customer expects to walk/run each week  </li> <li>Fitness: Self-rated fitness on a 1-to-5 scale, where 1 is poor shape and 5 is excellent shape</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#3-methodology","title":"3. Methodology","text":""},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#questions-to-explore","title":"Questions to Explore:","text":"<ol> <li>What are the different types of variables in the data?  </li> <li>What is the distribution of different variables in the data?  </li> <li>Which product is more popular among males or females?  </li> <li>Is the product purchase affected by the marital status of the customer?  </li> <li>Is there a significant correlation among some of the variables?  </li> <li>What is the distribution of the average number of miles for each product?</li> </ol>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p> <pre><code># Sample code block from the solution notebook\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"CardioGoodFitness.csv\")\nprint(df.head())\n\n# Visualizing product preference by gender\nsns.countplot(data=df, x='Product', hue='Gender')\nplt.title(\"Product Preference by Gender\")\nplt.show()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<p>Insights will be derived through descriptive analysis using data visualizations and statistical exploration in the Colab notebook. Key focus areas include:</p> <ul> <li>Product popularity across demographic segments</li> <li>Relationship between fitness and treadmill usage</li> <li>Impact of education, age, and income on product choice</li> </ul> <p>Refer to the Google Colab notebook for detailed implementation and visuals.</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83c\udfaf This analysis helps the business: - Define distinct customer personas per product line - Design targeted marketing strategies - Identify potential market gaps across demographics</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CardioGoodFitness.csv</p> <p>\ud83d\udcce View CardioGoodFitness.csv</p> <p>\ud83d\udcca View Solutions Data (XLSX)</p>"},{"location":"01-foundation/1-python/2-additional-reference/01-case-study-cardiogood-fitness/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/","title":"\ud83e\udde0 Practical Exercise: Study 2 \u2013 Uber Demand Pattern Analysis","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#1-overview","title":"1. Overview","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#context","title":"Context","text":"<p>Uber Technologies, Inc. is an American multinational transportation network company based in San Francisco and has operations in approximately 72 countries and 10,500 cities. In the fourth quarter of 2021, Uber had 118 million monthly active users worldwide and generated an average of 19 million trips per day.</p> <p>Ridesharing is a very volatile market and demand fluctuates wildly with time, place, weather, local events, etc. The key to being successful in this business is to be able to detect patterns in these fluctuations and cater to demand at any given time.</p> <p>As a newly hired Data Scientist in Uber's New York Office, you have been given the task of extracting insights from data that will help the business better understand the demand profile and take appropriate actions to drive better outcomes for the business. Your goal is to identify good insights that are potentially actionable, i.e., the business can do something with it.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#objective","title":"Objective","text":"<p>To extract actionable insights around demand patterns across various factors.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#2-dataset-description","title":"2. Dataset Description","text":"<p>The dataset contains information about the weather, location, and pickups. Below is the data dictionary:</p> <ul> <li>pickup_dt: Date and time of the pick-up  </li> <li>borough: NYC's borough  </li> <li>pickups: Number of pickups for the period  </li> <li>spd: Wind speed in miles/hour  </li> <li>vsb: Visibility in miles to the nearest tenth  </li> <li>temp: Temperature in Fahrenheit  </li> <li>dewp: Dew point in Fahrenheit  </li> <li>slp: Sea level pressure  </li> <li>pcp01: 1-hour liquid precipitation  </li> <li>pcp06: 6-hour liquid precipitation  </li> <li>pcp24: 24-hour liquid precipitation  </li> <li>sd: Snow depth in inches  </li> <li>hday: Being a holiday (Y) or not (N)</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#3-methodology","title":"3. Methodology","text":""},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#key-questions","title":"Key Questions","text":"<ol> <li>What are the different variables that influence pickups?  </li> <li>Which factor affects the pickups the most? What could be plausible reasons for that?  </li> <li>What are your recommendations to Uber management to capitalize on fluctuating demand?</li> </ol>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#guidelines","title":"Guidelines","text":"<ul> <li>Perform univariate analysis to better understand individual variables.</li> <li>Perform bivariate analysis to explore relationships between variables.</li> <li>Create visualizations to explore the data and extract actionable insights.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"Uber.csv\")\nprint(df.head())\n\n# Example: Analyze pickups by borough\nsns.boxplot(data=df, x='borough', y='pickups')\nplt.title(\"Distribution of Pickups by NYC Borough\")\nplt.xticks(rotation=45)\nplt.show()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>Borough-wise Patterns: Some boroughs consistently show higher ride demand.</li> <li>Weather Influence: Variables like temperature and precipitation correlate with demand.</li> <li>Holiday Trends: Holidays (hday = Y) may show increased or decreased pickup patterns depending on context.</li> </ul> <p>Visualizations and full interpretation are included in the Google Colab notebook.</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83d\udcca These insights help Uber to: - Adjust pricing or fleet placement by borough - Forecast ride surges during bad weather or holidays - Optimize driver incentives during low-visibility conditions</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download Uber.csv</p> <p>\ud83d\udcce View Uber.csv in Google Drive</p>"},{"location":"01-foundation/1-python/2-additional-reference/02-case-study-uber/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/","title":"\ud83d\udcca Exploratory Data Analysis (EDA)","text":""},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Data Loading and Initial Exploration</li> <li>2. Checking Missing, Duplicate Values, and Summary</li> <li>3. Univariate Analysis</li> <li>4. Bivariate Analysis</li> <li>5. Charts and Plots for EDA</li> <li>6. Missing Values - Group Mean</li> <li>7. Missing Values - Medians &amp; Dropping</li> <li>8. Outlier Detection and Analysis</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#1-data-loading-and-initial-exploration","title":"1. Data Loading and Initial Exploration","text":"<p>Begin by loading the housing dataset and performing initial inspection using Pandas.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"Melbourne_Housing.csv\")\ndf.head()\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example","title":"Use Case Example","text":"<p>\ud83c\udfd8\ufe0f Real estate analysts begin their data pipeline with loading raw housing sales data to inspect types and formats.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#2-checking-missing-duplicate-values-and-summary","title":"2. Checking Missing, Duplicate Values, and Summary","text":"<p>Use <code>.isnull().sum()</code> to inspect missing values, and <code>.describe()</code> for a statistical summary.</p> <pre><code>print(df.isnull().sum())\ndf.describe()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_1","title":"Use Case Example","text":"<p>\ud83d\udd0d Detecting columns with substantial missing data helps decide cleaning strategy in housing market analytics.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#3-univariate-analysis","title":"3. Univariate Analysis","text":"<p>Study single variable distributions using histograms and value counts.</p> <pre><code>df[\"Price\"].hist(bins=50)\ndf[\"Type\"].value_counts()\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_2","title":"Use Case Example","text":"<p>\ud83d\udcb2 Understand housing price distribution to identify outliers or skewness in Melbourne.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#4-bivariate-analysis","title":"4. Bivariate Analysis","text":"<p>Use scatter plots and correlation matrices to explore relationships between variables.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.scatterplot(x=\"Distance\", y=\"Price\", data=df)\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_3","title":"Use Case Example","text":"<p>\ud83d\udcc9 Discover how distance from the city center influences real estate pricing.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#5-charts-and-plots-for-eda","title":"5. Charts and Plots for EDA","text":"<p>Generate box plots, bar charts, pair plots, and violin plots for deeper insight.</p> <pre><code>sns.boxplot(x=\"Type\", y=\"Price\", data=df)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_4","title":"Use Case Example","text":"<p>\ud83d\udcca Visual analytics are used in real estate investment dashboards to summarize sales trends by property type.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#6-missing-values-group-mean","title":"6. Missing Values - Group Mean","text":"<p>Fill missing values with group-specific means.</p> <pre><code>df[\"BuildingArea\"].fillna(df.groupby(\"Type\")[\"BuildingArea\"].transform(\"mean\"), inplace=True)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_5","title":"Use Case Example","text":"<p>\ud83c\udfd7\ufe0f Impute area details for residential properties based on type-specific averages.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#7-missing-values-medians-dropping","title":"7. Missing Values - Medians &amp; Dropping","text":"<p>Alternative strategy for missing value treatment by median filling or row dropping.</p> <pre><code>df[\"Bedroom2\"].fillna(df[\"Bedroom2\"].median(), inplace=True)\ndf.dropna(inplace=True)\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_6","title":"Use Case Example","text":"<p>\ud83d\udcc9 Reducing noise caused by sparse or unfixable data improves ML model training.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#8-outlier-detection-and-analysis","title":"8. Outlier Detection and Analysis","text":"<p>Detect and remove outliers using IQR or z-score techniques.</p> <pre><code>Q1 = df[\"Price\"].quantile(0.25)\nQ3 = df[\"Price\"].quantile(0.75)\nIQR = Q3 - Q1\n\nfiltered_df = df[(df[\"Price\"] &gt;= Q1 - 1.5 * IQR) &amp; (df[\"Price\"] &lt;= Q3 + 1.5 * IQR)]\n</code></pre>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#use-case-example_7","title":"Use Case Example","text":"<p>\ud83d\udca1 Outlier removal leads to more stable forecasting of property valuation models.</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>\ud83d\udc49 Download Melbourne_Housing.csv \ud83d\udcce View Melbourne_Housing.csv</p> <p>\ud83d\udc49 Download Melbourne_Housing_NoMissing.csv \ud83d\udcce View Melbourne_Housing_NoMissing.csv</p> <p>\ud83d\udc49 Download Melbourne_Housing_NoOutliers.csv \ud83d\udcce View Melbourne_Housing_NoOutliers.csv</p> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/03-case-study-EDA/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/","title":"\ud83e\udde0 FIFA World Cup Case Study","text":""},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#1-overview","title":"1. Overview","text":"<p>The FIFA World Cup is one of the most prestigious tournaments in the world of football. Organized every four years (except during World War II), it brings together top international teams competing for global glory.</p> <p>In this case study, you are a member of the newly formed Brussels United FC, and have been tasked with analyzing historical FIFA World Cup data up to the year 2014.</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#2-dataset-description","title":"2. Dataset Description","text":"<p>The dataset includes the following features:</p> <ul> <li>Year: The year in which the World Cup was held.</li> <li>Country: Host country.</li> <li>Winner: Country that won the tournament.</li> <li>Runners-Up: Second place team.</li> <li>Third: Third place team.</li> <li>Fourth: Fourth place team.</li> <li>GoalsScored: Total number of goals scored during the tournament.</li> <li>QualifiedTeams: Number of qualified national teams.</li> <li>MatchesPlayed: Total number of matches played.</li> <li>Attendance: Total spectator attendance.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#3-methodology","title":"3. Methodology","text":"<p>We will explore the data using the following steps:</p> <ul> <li>Perform exploratory data analysis (EDA).</li> <li>Answer questions related to performance and trends over years.</li> <li>Visualize key metrics such as goals, attendance, and winning teams.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open Questions Notebook in Colab \ud83d\udc49 Open Solutions Notebook in Colab</p> <p></p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>An upward trend in GoalsScored over decades reflects attacking style changes.</li> <li>Attendance has generally increased, showing the growing popularity of the event.</li> <li>Patterns in repeated wins highlight dominance by select nations like Brazil and Germany.</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>Understanding historical data helps clubs like Brussels United FC: - Study legacy and benchmark against international performance. - Inspire young talent by learning from top-performing nations. - Design predictive models for future tournaments based on trends.</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive \ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/1-python/2-additional-reference/04-case-study-fifa/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>FIFA Official World Cup Page</li> <li>Kaggle Dataset (for additional sources)</li> <li>Matplotlib Documentation</li> <li>Pandas Documentation</li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/","title":"\ud83d\udc0d Python Reference &amp; Learning Resources","text":""},{"location":"01-foundation/1-python/2-additional-reference/05-references/#official-documentation","title":"\ud83d\udcd8 Official Documentation","text":"<ul> <li> <p>Python Docs (Latest)</p> <p>The official Python documentation. Covers all built-in types, standard libraries, functions, language reference, tutorials, and best practices.</p> </li> <li> <p>PEP Index (Python Enhancement Proposals)</p> <p>Formal documents describing new features or design aspects in Python. Ideal for understanding the evolution of the language.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#beginner-to-intermediate-tutorials","title":"\ud83c\udf93 Beginner to Intermediate Tutorials","text":"<ul> <li> <p>W3Schools - Python Tutorial</p> <p>Simple, interactive, beginner-friendly tutorials. Covers basics to advanced concepts with live code editor.</p> </li> <li> <p>Real Python</p> <p>High-quality articles, video courses, and tutorials. Covers data structures, web frameworks, OOP, and practical use cases.</p> </li> <li> <p>Programiz - Python</p> <p>Easy-to-follow tutorials with examples for absolute beginners.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#interactive-learning-playgrounds","title":"\ud83d\udcbb Interactive Learning &amp; Playgrounds","text":"<ul> <li> <p>Python Tutor (Step-by-step visualizer)</p> <p>Visualize Python code execution step-by-step \u2014 great for debugging and learning how code flows.</p> </li> <li> <p>Replit - Python Playground</p> <p>Online IDE to write, run, and share Python code instantly without setup.</p> </li> <li> <p>Google Colab</p> <p>Jupyter notebook environment with free cloud GPUs. Perfect for AI, ML, and data science experiments.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#advanced-professional","title":"\ud83d\udee0\ufe0f Advanced &amp; Professional","text":"<ul> <li> <p>Awesome Python</p> <p>Curated list of Python frameworks, libraries, software, and resources. A goldmine for pros and enthusiasts.</p> </li> <li> <p>Python Module Index (PyPI)</p> <p>Python Package Index \u2014 search, install, and learn about third-party packages.</p> </li> <li> <p>Python Cheatsheet (GitHub)</p> <p>One-page, comprehensive Python cheat sheet with examples.</p> </li> <li> <p>Full Stack Python</p> <p>Learn how to build, deploy, and scale Python web applications using best practices and popular libraries.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#data-science-ml-with-python","title":"\ud83d\udcca Data Science &amp; ML with Python","text":"<ul> <li> <p>Scikit-learn Docs</p> <p>User guide, API reference, and tutorials for machine learning with Python.</p> </li> <li> <p>Pandas Docs</p> <p>In-depth guide to data manipulation using Python.</p> </li> <li> <p>Matplotlib Docs</p> <p>Comprehensive visualization library for Python.</p> </li> <li> <p>TensorFlow Python API</p> <p>Official TensorFlow API reference for deep learning models.</p> </li> </ul>"},{"location":"01-foundation/1-python/2-additional-reference/05-references/#books-free-and-online","title":"\ud83d\udcda Books (Free and Online)","text":"<ul> <li> <p>Think Python (2e)</p> <p>An introduction to Python programming \u2014 easy to follow and excellent for CS fundamentals.</p> </li> <li> <p>Python for Everybody (free ebook)</p> <p>A practical programming book for data analysis and web access.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/","title":"\ud83d\udcca Statistics","text":"<p>Welcome to the Statistics section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/2-statistics/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Understand the foundational statistical tools that fuel AI decision-making.</p> <ul> <li>Descriptive Stats</li> <li>Inferential Stats</li> <li>Hypothesis Testing</li> </ul>"},{"location":"01-foundation/2-statistics/#additional-reference","title":"\ud83d\udcc2 Additional Reference","text":"<p>Explore supporting notebooks, annotated case studies, and extended walkthroughs.</p> <ul> <li>\"Case Study: Mobile Usage\": 01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage.md</li> <li>\"Case Study: Medicon\": 01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing.md</li> </ul>"},{"location":"01-foundation/2-statistics/#qa","title":"\u2753 Q&amp;A","text":"<p>Explore questions and annotated insights around key statistical ideas.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/","title":"\ud83d\udcca Descriptive Statistics \u2013 A Practical Guide","text":"<p>This lesson provides a detailed, example-rich walkthrough of key Descriptive Statistics concepts using Python. It includes real-world data analysis using Pandas, NumPy, and Seaborn, and is designed for students and data practitioners using the MkDocs Material theme.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Descriptive Statistics</li> <li>2. Dataset Overview</li> <li>3. Central Tendency Measures</li> <li>4. Variability Measures</li> <li>5. Group-wise Descriptive Analysis</li> <li>6. CSV Download</li> <li>7. Colab Notebook</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#1-introduction-to-descriptive-statistics","title":"1. Introduction to Descriptive Statistics","text":"<p>Descriptive statistics summarize and describe the main features of a dataset in a quantitative manner. They form the foundation of exploratory data analysis (EDA).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#2-dataset-overview","title":"2. Dataset Overview","text":"<p>This dataset (<code>descriptive_statistics_sample.csv</code>) contains sample records with <code>ID</code>, <code>Age</code>, <code>Income</code>, <code>SatisfactionScore</code>, and <code>PurchaseFrequency</code>.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#3-central-tendency-measures","title":"3. Central Tendency Measures","text":""},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#measures-covered","title":"Measures Covered:","text":"<ul> <li>Mean</li> <li>Median</li> <li>Mode</li> </ul> <pre><code>import pandas as pd\ndata = pd.read_csv('descriptive_statistics_sample.csv')\nprint(\"Mean Age:\", data['Age'].mean())\nprint(\"Median Age:\", data['Age'].median())\nprint(\"Mode Age:\", data['Age'].mode()[0])\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#4-variability-measures","title":"4. Variability Measures","text":""},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#measures-covered_1","title":"Measures Covered:","text":"<ul> <li>Range</li> <li>Variance</li> <li>Standard Deviation</li> </ul> <pre><code>range_income = data['Income'].max() - data['Income'].min()\nprint(\"Income Range:\", range_income)\nprint(\"Income Variance:\", data['Income'].var())\nprint(\"Income Std Dev:\", data['Income'].std())\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#5-group-wise-descriptive-analysis","title":"5. Group-wise Descriptive Analysis","text":"<p>Analyze descriptive stats by group (e.g., age group or satisfaction score).</p> <pre><code>age_bins = pd.cut(data['Age'], bins=[18, 30, 45, 60], labels=['18-30', '31-45', '46-60'])\ngrouped = data.groupby(age_bins)['PurchaseFrequency'].mean()\nprint(grouped)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#6-csv-download","title":"6. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive \ud83d\udcce View CSV in Google Drive</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#7-colab-notebook","title":"7. Colab Notebook","text":"<p>\ud83d\udc49 Open Notebook in Google Colab \ud83d\udcce View on Google Drive</p>"},{"location":"01-foundation/2-statistics/1-learning-path/01-descriptive-statistics/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Matplotlib Docs</li> <li>Khan Academy: Statistics</li> <li>MIT OCW \u2013 Stats Courses</li> <li>StatQuest with Josh Starmer</li> <li>Scikit-Learn User Guide</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/","title":"\ud83d\udcca Inferential Statistics","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Introduction to Inferential Statistics<ul> <li>1.1 Random Variable </li> <li>1.2 Discrete Random Variable </li> <li>1.3 Continous Random Variable </li> <li>1.4 Probability Distribution </li> </ul> </li> <li>2. Fundamental Terms in Distributions</li> <li>3. Binomial Distribution</li> <li>4. Uniform Distribution</li> <li>5. Normal Distribution</li> <li>6. Z-Score</li> <li>7. Sampling &amp; Inference Foundations</li> <li>8. Central Limit Theorem</li> <li>9. Estimation</li> <li>10. Hypothesis Testing</li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-introduction-to-inferential-statistics","title":"1. Introduction to Inferential Statistics","text":"<p>Back to Top</p> <p>Inferential statistics help us draw conclusions about populations based on sample data.</p> <pre><code># Example: confidence interval\nimport numpy as np\nimport scipy.stats as stats\n\nsample = np.array([85, 80, 78, 90, 88])\nconf_interval = stats.t.interval(0.95, len(sample)-1, loc=np.mean(sample), scale=stats.sem(sample))\nprint(conf_interval)\n</code></pre> <p>\ud83d\udc49 Open in Colab </p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcca Used by analysts to infer population metrics from SAT sample scores.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#11-random-variable","title":"1.1 Random Variable","text":"<p>Back to Top</p> <p>Suppose there are 1,000 students in the university.</p> <p>Question: What is the probability that 500 students will pass the upcoming exam?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#setup","title":"\ud83e\udde0 Setup","text":"<p>Back to Top</p> <ul> <li>Each student has a 50-50 chance of passing or failing the exam.</li> <li>We are observing the total number of students who pass, which can range from 0 to 1000.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-concept","title":"\ud83d\udccc Key Concept","text":"<p>Back to Top</p> <p>A random variable assigns a numerical value to each outcome of an experiment. It assumes different values with different probabilities.</p> <p>This numerical outcome can represent a count (like number of students who pass), a measurement, or any quantifiable result of a probabilistic process.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#12-discrete-random-variable","title":"1.2 Discrete Random Variable","text":"<p>Back to Top</p> <p>You work for an auto insurance company. Suppose the number of insurance claims filed by a driver in a month is a random variable \\(X\\) described as follows:</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#distribution-of-x","title":"\ud83d\udd22 Distribution of X","text":"<p>Back to Top</p> <p>Let \\(X =\\)</p> Claims (x) Probability P(X = x) 0 0.95 1 0.04 2 0.008 3 0.002"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#rule","title":"\u2705 Rule","text":"<p>Back to Top</p> <p>All probabilities must be non-negative and must sum to 1.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#13-continuous-random-variable","title":"1.3 Continuous Random Variable","text":"<p>Back to Top</p> <p>Suppose the volume of soda in a bottle is described by a random variable.</p> <p>Can we list all possible values?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#example-values","title":"\ud83e\uddea Example Values","text":"<p>Back to Top</p> <ul> <li>498 mL  </li> <li>499 mL  </li> <li>500 mL  </li> <li>...  </li> <li>What about 499.2129415 mL?</li> </ul> <p>Sometimes it's just not possible to list all values a random variable can take.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-concept_1","title":"\ud83e\udde0 Key Concept","text":"<p>Back to Top</p> <p>If the random variable can take any value in a given range, we call it a continuous random variable.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#14-probability-distribution","title":"1.4 Probability Distribution","text":"<p>Back to Top</p> <p>A Probability Distribution describes the values that a random variable can take, along with the probabilities of those values.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#two-main-types","title":"\ud83d\udd00 Two Main Types","text":"<p>Back to Top</p> Type Description Associated Function \ud83d\udfe2 Discrete Probability Distribution Arises from discrete random variables. Probability Mass Function (PMF) Gives the probability that the variable takes a specific value. \ud83d\udd35 Continuous Probability Distribution Arises from continuous random variables. Probability Density Function (PDF) Determines the probability that the variable lies between two values."},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary","title":"\ud83d\udccc Summary","text":"<p>Back to Top</p> <ul> <li>A PMF is used for countable outcomes (e.g., number of claims, dice rolls).</li> <li>A PDF is used for uncountable outcomes over a range (e.g., height, volume).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-distribution-example","title":"\ud83d\udcc8 Probability Distribution: Example","text":"<p>A company tracks the number of sales new employees make each day during a 100-day probationary period. The results for one new employee are shown below. Using this, we construct and plot a probability distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#tabulated-data","title":"\ud83e\uddee Tabulated Data","text":"<p>Back to Top</p> Sales (per day) # of Days Relative Frequency 0 16 0.16 1 18 0.18 2 15 0.15 3 21 0.21 4 11 0.11 5 10 0.10 6 9 0.09"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#bar-plot-interpretation","title":"\ud83d\udcca Bar Plot Interpretation","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_1","title":"\ud83e\udde0 Summary","text":"<p>Back to Top</p> <ul> <li> <p>The sum of relative frequencies: <code>0.16 + 0.18 + 0.15 + 0.21 + 0.11 + 0.10 + 0.09 = 1.00 \u2705</code></p> </li> <li> <p>This confirms a valid probability distribution.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#-","title":"---","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-fundamental-terms-in-distributions","title":"2. Fundamental Terms in Distributions","text":"<p>Back to Top</p> <p>Covers mean, variance, standard deviation, skewness, and kurtosis.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_1","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcc8 Financial institutions assess risk using variance and skewness of return distributions.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#3-binomial-distribution","title":"3. Binomial Distribution","text":"<p>Back to Top</p> <p>Applicable when analyzing binary outcomes (e.g., success/failure).</p> <pre><code>from scipy.stats import binom\n\nbinom.pmf(k=3, n=10, p=0.5)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_2","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\uddea A/B testing for conversion rates on two landing pages.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#bernoulli-distribution","title":"\ud83e\uddee Bernoulli Distribution","text":"<p>Back to Top</p> <p>The Bernoulli distribution models a random experiment with only two possible outcomes:</p> <ul> <li><code>1</code> for success (with probability <code>p</code>)</li> <li><code>0</code> for failure (with probability <code>1 - p</code>)</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-points","title":"\ud83d\udcd8 Key Points","text":"<p>Back to Top</p> <ul> <li>Only one trial is considered.</li> <li>Success and failure are non-judgmental: you can assign \"success\" to any outcome based on your scenario.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-expression","title":"\ud83e\udde0 Probability Expression","text":"<p>Back to Top</p> \\[ X = \\begin{cases} 1, &amp; \\text{with prob } p \\\\ 0, &amp; \\text{with prob } 1-p \\end{cases} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#common-use-cases","title":"\u2705 Common Use Cases","text":"<p>Back to Top</p> <ul> <li>\ud83c\udfed Manufacturing defective parts (success = defective or not)</li> <li>\ud83e\uddea Medical test outcomes (success = positive result)</li> </ul> <p>A Bernoulli distribution is a special case of the Binomial distribution where the number of trials = 1.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-distributions","title":"\ud83c\udfb2 Probability Distributions","text":"<p>Learn about random variables, types of distributions, and how to interpret discrete and continuous data using bar plots and formulas.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-random-variable","title":"\ud83d\udd22 1. Random Variable","text":"<p>Back to Top</p> <p>A random variable assigns a numerical value to each outcome of an experiment. It assumes different values with different probabilities.</p> <p>Example:</p> <p>Suppose there are 1,000 students in a university. What is the probability that 500 will pass an upcoming exam?</p> <ul> <li>Each student has a 50\u201350 chance of passing.</li> <li>The number of students who pass can range from 0 to 1000.</li> <li>The total outcomes form a distribution of values \u2192 this is a random variable.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-continuous-random-variable","title":"\ud83c\udf0a 2. Continuous Random Variable","text":"<p>Back to Top</p> <p>If a random variable can take any value within a range, it\u2019s continuous.</p> <p>Example:</p> <p>Volume of soda in a bottle: 498mL, 499.2129415mL, \u2026 Cannot list all possible values \u2014 infinite possibilities.</p> <p>\u2705 A continuous random variable deals with real numbers within intervals.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#3-discrete-random-variable","title":"\ud83d\udd22 3. Discrete Random Variable","text":"<p>Back to Top</p> <p>A discrete random variable has a countable set of values.</p> <p>Example:</p> <p>Insurance claims per month for a driver:</p> \\[ X =  \\begin{cases} 0, &amp; \\text{with prob } 0.95 \\\\ 1, &amp; \\text{with prob } 0.04 \\\\ 2, &amp; \\text{with prob } 0.008 \\\\ 3, &amp; \\text{with prob } 0.002 \\\\ \\end{cases} \\] <ul> <li>Values must be non-negative</li> <li>All probabilities must sum to 1</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#4-probability-distribution","title":"\ud83e\uddee 4. Probability Distribution","text":"<p>Back to Top</p> <p>Defines values a random variable can take along with their probabilities.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#discrete-probability-distribution","title":"\ud83d\udd39 Discrete Probability Distribution","text":"<p>Back to Top</p> <ul> <li>Arises from discrete random variables</li> <li>Has a Probability Mass Function (PMF)   Gives the probability that a variable takes a specific value</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#continuous-probability-distribution","title":"\ud83d\udd38 Continuous Probability Distribution","text":"<p>Back to Top</p> <ul> <li>Arises from continuous random variables</li> <li>Has a Probability Density Function (PDF)   Describes the likelihood a variable falls within a range</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#5-bar-plot-interpretation","title":"\ud83d\udcca 5. Bar Plot Interpretation","text":"<p>Back to Top</p> <p>Shows relative frequencies for discrete values.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_2","title":"\ud83e\udde0 Summary","text":"<p>Back to Top</p> <ul> <li>The sum of relative frequencies:   [   0.16 + 0.18 + 0.15 + 0.21 + 0.11 + 0.10 + 0.09 = 1.00 \u2705   ]</li> </ul> <p>\u2705 This confirms a valid probability distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#6-bernoulli-distribution","title":"\ud83c\udfaf 6. Bernoulli Distribution","text":"<p>Back to Top</p> <ul> <li>Only two outcomes: 1 (success) and 0 (failure)</li> <li>Single trial</li> </ul> \\[ X = \\begin{cases} 1, &amp; \\text{with prob } p \\\\ 0, &amp; \\text{with prob } 1 - p \\\\ \\end{cases} \\] <p>\ud83d\udca1 Used in scenarios like: - Defective manufacturing parts - Outcomes of medical tests</p> <p>\ud83d\udccc Note: Success/failure labels are not judgmental \u2014 it\u2019s a modeling convention.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#7-binomial-distribution","title":"\ud83d\udce6 7. Binomial Distribution","text":"<p>Back to Top</p> <p>Applies when you extend Bernoulli trials over multiple repetitions.</p> <p>Example Scenario:</p> <p>Survey 25 TikTok users to check if they\u2019ve posted a video (Yes/No).</p> <ul> <li>Each trial is Bernoulli.</li> <li>Total number of \u201cYes\u201d is modeled by Binomial Distribution.</li> </ul> \\[ \\text{Bernoulli is a special case of Binomial with a single trial} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#probability-mass-function-pmf","title":"\ud83d\udcd0 Probability Mass Function (PMF):","text":"<p>Back to Top</p> \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n - x} \\] <ul> <li>\\(n\\): total trials  </li> <li>\\(x\\): number of successes  </li> <li>\\(p\\): success probability</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#quiz-insight","title":"\u2705 Quiz Insight","text":"<p>Back to Top</p> <p>A continuous probability distribution is represented by: \u2714\ufe0f Probability Density Function (PDF) \u274c Not by PMF (that\u2019s for discrete variables)</p> <p>### \ud83c\udfaf Binomial Distribution: Assumptions</p> <p>To model a scenario using a Binomial distribution, the following assumptions must be satisfied:</p> <ol> <li> <p>Fixed Number of Trials (n)    The number of experiments or trials is predetermined and remains constant.</p> </li> <li> <p>Independence    Each trial is independent of the others \u2014 the outcome of one trial does not influence the outcome of another.</p> </li> <li> <p>Binary Outcomes    Each trial results in only one of two possible outcomes: success or failure.</p> </li> <li> <p>Constant Probability    The probability of success (denoted as \\( p \\)) is the same for each trial.</p> </li> </ol> <p>These conditions ensure the binomial model is valid for computing probabilities using the binomial formula:</p> \\[ P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x} \\]"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-happens-if-binomial-assumptions-are-violated","title":"\u26a0\ufe0f What Happens If Binomial Assumptions Are Violated?","text":"<p>Back to Top</p> <p>In a month of 30 days, what is the probability that it will rain on more than 10 days, if on average the chance of rain on a given day is 20%?</p> <p>To apply the binomial distribution, we assume:</p> <ol> <li> <p>Independence:    The event of rain on a particular day is independent of it raining on the previous day.</p> </li> <li> <p>Constant Probability:    The chance of rain does not increase or decrease over the duration of the month.</p> </li> </ol> <p>If these assumptions are satisfied, then we can model the situation using:</p> <ul> <li>\\( n = 30 \\) (number of days)  </li> <li>\\( p = 0.2 \\) (probability of rain on a given day)</li> </ul> <p>Using the binomial distribution, we can compute:</p> \\[ P(X &gt; 10) \\quad \\text{for } X \\sim \\text{Binomial}(n=30, p=0.2) \\] <p>\ud83e\udde0 Note: Although the assumptions are not strictly valid, they allow for a simplified calculation that is often good enough for practical estimation purposes.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#4-uniform-distribution","title":"4. Uniform Distribution","text":"<p>Back to Top</p> <p>All outcomes have equal probability.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_3","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83c\udfb2 Simulating random dice rolls or fair lottery draws.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#scenario","title":"Scenario","text":"<p>Back to Top</p> <p>Suppose we roll a die. The possible outcomes of this event are:</p> <pre><code>1, 2, 3, 4, 5, 6\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-characteristics","title":"Key Characteristics","text":"<p>Back to Top</p> <ul> <li>\u2705 All outcomes have an equal probability of occurrence.</li> <li>\ud83d\udd01 Outcomes are mutually exclusive (no two can happen at once).</li> <li>\ud83d\udcca The probability of each outcome is the same:</li> </ul> <p>[   P(x) = \\frac{1}{n} = \\frac{1}{6} \\text{ for a fair 6-sided die}   ]</p> <ul> <li>This type of distribution is known as a Uniform Distribution.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#when-to-use","title":"When to Use","text":"<p>Back to Top</p> <p>Uniform distribution is:</p> <ul> <li>Useful when we want unbiased selection.</li> <li>Common in random sampling, game simulations, and initial probability models.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_3","title":"Summary","text":"<p>Back to Top</p> <p>A Uniform Distribution assigns equal probability to all possible outcomes. It is one of the simplest forms of probability distribution and serves as the foundation for modeling fair and random events.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#uniform-distribution","title":"Uniform Distribution","text":"<p>Suppose we roll a die. The outcomes of this event can be 1, 2, 3, 4, 5, 6.</p> <ul> <li>All of the outcomes have an equal probability of occurrence and are mutually exclusive.</li> <li>We can say that the probabilities of occurrence are uniformly distributed.</li> <li>This is referred to as Uniform Distribution.</li> <li>\u2705 Useful when we are interested in unbiased selection.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#types-of-uniform-distribution","title":"Types of Uniform Distribution","text":"<p>Back to Top</p> <p>There are two types of Uniform Distribution:</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#1-discrete-uniform-distribution","title":"1. Discrete Uniform Distribution","text":"<p>Back to Top</p> <ul> <li>Takes a finite number (m) of values.</li> <li>Each value has equal probability of being selected.</li> </ul> <p>For example: Number of books sold by a bookseller per day can be uniformly distributed between 100 to 300.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#2-continuous-uniform-distribution","title":"2. Continuous Uniform Distribution","text":"<p>Back to Top</p> <ul> <li>Can take any value between a specified range.</li> </ul> <p>For example: Tomorrow\u2019s temperature in the United States can be uniformly distributed between 12\u00b0C to 17\u00b0C.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#5-normal-distribution","title":"5. Normal Distribution","text":"<p>Back to Top</p> <p>A bell-shaped distribution used across disciplines.</p> <pre><code>from scipy.stats import norm\n\nx = np.linspace(-3, 3, 100)\npdf = norm.pdf(x)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_4","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udca1 Height distribution of people in a city, or standardized testing scores.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#normal-distribution-why-normal","title":"\ud83d\udcca Normal Distribution: Why Normal","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-is-it-called-the-normal-distribution","title":"\u2753 Why is it called the normal distribution?","text":"<p>Back to Top</p> <p>\ud83d\udd39 They are commonly found everywhere \u2014 starting from nature to industry.</p> <p>\ud83d\udd39 Many useful datasets are approximately normally distributed.</p> <p>\ud83d\udd39 Examples include: - Height and weight of adults - IQ scores - Measurement errors - Quality control test results  </p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#normal-distribution-properties","title":"Normal Distribution: Properties","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-properties-of-the-normal-distribution","title":"\ud83d\udcc8 Key Properties of the Normal Distribution","text":"<p>Back to Top</p> <ol> <li>The graph of the normal distribution is called the normal curve.</li> <li>The normal curve is symmetric around the mean.</li> <li>Mean, Median, and Mode of the normal distribution are equal.</li> <li>The total area under the normal curve is 1.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#empirical-rule-68-95-997-rule","title":"\ud83c\udfaf Empirical Rule (68-95-99.7 Rule)","text":"<p>Back to Top</p> <ol> <li>68% of the data falls within 1 standard deviation (\u03c3) from the mean (\u03bc).</li> <li>95% of the data falls within 2 standard deviations from the mean.</li> <li>99.7% of the data falls within 3 standard deviations from the mean.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#visual-representation","title":"\ud83d\udcca Visual Representation","text":"<p>Back to Top</p> <p></p> <p>The image illustrates the bell-shaped curve, highlighting symmetric intervals around the mean and the percentage of data captured within 1\u03c3, 2\u03c3, and 3\u03c3.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#summary_4","title":"\u2705 Summary","text":"<p>Back to Top</p> <p>The normal distribution is foundational in statistics and machine learning, allowing for standardized assumptions about data spread and probability within intervals of standard deviation.</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#6-z-score","title":"6. Z-Score","text":"<p>Back to Top</p> <p>Measures how many standard deviations a data point is from the mean.</p> <pre><code>z = (x - np.mean(x)) / np.std(x)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_5","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udea8 Outlier detection in performance metrics.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#7-sampling-inference-foundations","title":"7. Sampling &amp; Inference Foundations","text":"<p>Back to Top</p> <p>Understanding population vs. sample, and designing sampling techniques.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_6","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\uddec Pharmaceutical companies conduct clinical trials on samples before full rollout.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#revisiting-the-need-for-sampling","title":"Revisiting the Need for Sampling","text":"<p>In many situations, what we have available to us is a sample of data.</p> <p>\ud83d\udd39 The data we have is finite. \ud83d\udd39 Till now, the goal was to find ways of describing, summarizing, and visualizing the sample data only. \ud83d\udd39 Moving ahead, we want to make inferences about the entire population using the sample data.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#simple-random-sampling","title":"\ud83c\udfaf Simple Random Sampling","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-is-simple-random-sampling","title":"What is Simple Random Sampling?","text":"<p>Back to Top</p> <p>A sampling technique where every item in the population has an equal chance of being selected.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-are-simple-random-samples-important","title":"\ud83d\udca1 Why are simple random samples important?","text":"<p>Back to Top</p> <p>Allows all the entities in the population to have an equal chance of being selected, and so the sample is likely to be representative of the population.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-points_1","title":"\ud83d\udcdd Key Points","text":"<p>Back to Top</p> <ul> <li>Every individual in the population has an equal probability of being chosen.</li> <li>Unbiased method if implemented correctly.</li> <li>Often implemented using random number generators or lottery methods.</li> </ul> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#8-central-limit-theorem","title":"8. Central Limit Theorem","text":"<p>Back to Top</p> <p>Describes how the sampling distribution of the sample mean approaches a normal distribution.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_7","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcc9 Enables approximation of sampling behavior for metrics like average wait times.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>The sampling distribution of the sample means will approach normal distribution as the sample size gets bigger, no matter what the shape of the population distribution is.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#assumptions","title":"Assumptions","text":"<p>Back to Top</p> <ul> <li>Data must be randomly sampled</li> <li>Sample values must be independent of each other</li> <li>Samples should come from the same distribution</li> <li>Sample size must be sufficiently large (\u2265 30)</li> </ul> <p>\ud83d\udccc The Central Limit Theorem (CLT) is foundational for inferential statistics and allows us to use normal distribution techniques even when the population is not normally distributed, provided the sample size is large enough.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#what-is-it","title":"\ud83d\udcd8 What Is It?","text":"<p>Back to Top</p> <p>The Central Limit Theorem (CLT) states that:</p> <p>When we take many random samples from any population (regardless of its distribution), the sampling distribution of the sample means will approach a normal distribution as the sample size increases (typically \\( n \\geq 30 \\)).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#practical-example-pizza-delivery-times","title":"\ud83c\udf55 Practical Example: Pizza Delivery Times","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#scenario_1","title":"\ud83d\udce6 Scenario","text":"<p>Back to Top</p> <p>You're managing a pizza delivery service and want to understand the average delivery time. The population of delivery times is not normally distributed (e.g., skewed due to traffic, weather, etc.).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#step-by-step","title":"\ud83d\udd22 Step-by-Step","text":"<p>Back to Top</p> <ol> <li>Random Sampling:</li> <li>You take 100 samples.</li> <li> <p>Each sample contains 30 delivery times (randomly selected).</p> </li> <li> <p>Calculate Means:</p> </li> <li>Compute the mean delivery time for each sample.</li> <li> <p>You now have 100 sample means.</p> </li> <li> <p>Plot the Distribution:</p> </li> <li>Even though the original data is skewed,</li> <li> <p>The distribution of sample means will look approximately normal.</p> </li> <li> <p>Use Normal-Based Statistics:</p> </li> <li>You can apply z-scores, confidence intervals, and hypothesis testing \u2014 because the sample means follow a normal distribution.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-it-matters","title":"\ud83e\udde0 Why It Matters","text":"<p>Back to Top</p> <ul> <li>\u2705 Works with both discrete and continuous populations</li> <li>\u2705 Helps apply normal distribution tools on non-normal data</li> <li>\u2705 Critical for inferential statistics</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#key-requirements","title":"\ud83c\udfaf Key Requirements","text":"<p>Back to Top</p> <ul> <li>Data must be randomly sampled</li> <li>Samples should be independent</li> <li>Data must come from the same distribution</li> <li>Sample size should be sufficiently large (typically \u2265 30)</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#9-estimation","title":"9. Estimation","text":"<p>Back to Top</p> <p>Estimate population parameters like mean or proportion using sample statistics.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_8","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83d\udcca Estimating average customer spend in a supermarket from sample receipt data.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#estimation","title":"Estimation","text":"<p>Estimation is the process of making inference about a population parameter based on a sample statistic.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#point-estimation","title":"\ud83d\udd39 Point Estimation","text":"<p>Back to Top</p> <ul> <li>Provides a single value (point) as an estimate of the population parameter.</li> <li>This estimate is derived directly from the sample data.</li> <li>Example:   The population mean is estimated from the sample mean: Estimated population mean = $40</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#interval-estimation","title":"\ud83d\udd39 Interval Estimation","text":"<p>Back to Top</p> <ul> <li>Provides a range of values within which the population parameter is expected to lie.</li> <li>This range is associated with a confidence level (x%).</li> <li>Example:   The population mean is expected to lie between $38 and $42, with 95% confidence   (i.e., x = 95). </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#confidence-interval-for-mean","title":"Confidence Interval for Mean \u03bc","text":""},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#interpretation-of-95-confidence-interval","title":"\ud83c\udfaf Interpretation of 95% Confidence Interval","text":"<p>Back to Top</p> <ul> <li>The interpretation of a 95% confidence interval is that, if the process is repeated a large number of times, then the intervals so constructed will contain the true population parameter 95% of the time.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#why-not-100-confidence-interval","title":"\u2753 Why not 100% Confidence Interval?","text":"<p>Back to Top</p> <ul> <li>A 100% confidence interval will include all possible values.</li> <li>Hence, there will be no insight into the problem.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#-_1","title":"---","text":"<p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#10-hypothesis-testing","title":"10. Hypothesis Testing","text":"<p>Back to Top</p> <p>Formal process for testing claims using sample data.</p> <p>\ud83d\udc49 Open in Colab </p> <pre><code># Example: t-test\nfrom scipy.stats import ttest_1samp\n\nttest_1samp(sample, popmean=85)\n</code></pre>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#use-case-example_9","title":"Use Case Example","text":"<p>Back to Top</p> <p>\ud83e\udde0 Determine if a new teaching method significantly improves test scores.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#csv-download","title":"\ud83d\udcc2 CSV Download","text":"<p>Back to Top</p> <p>\ud83d\udc49 Download sat_score.csv \ud83d\udcce View sat_score.csv</p> <p>\ud83d\udc49 Download debugging.csv \ud83d\udcce View debugging.csv</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#references-further-reading","title":"\ud83d\udd17 References &amp; Further Reading","text":"<p>Back to Top</p> <ul> <li>NumPy Docs</li> <li>Pandas Docs</li> <li>Seaborn Docs</li> <li>Plotly Python Docs</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare \u2013 Stats</li> <li>StatQuest by Josh Starmer (YouTube)</li> <li>Scikit-learn User Guide</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/02-inferential-statistics/#pdf-references","title":"\ud83d\udcc4 PDF References","text":"<p>Back to Top</p> <p>\ud83d\udcd8 Inferential Statistics \u2013 Lecture PDF \ud83d\udcd8 Hypothesis Testing \u2013 Lecture PDF</p> <p>These PDFs are stored locally in your project under: - <code>/docs/pdfs/Lecture Slides -  Inferential Statistics.pdf</code> - <code>/docs/pdfs/Lecture Slides - Hypothesis Testing.pdf</code></p> <p>Ensure the paths align with your MkDocs file structure and navigation.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/","title":"Agenda \u2013 Hypothesis Testing \u2013 Week 2","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Hypothesis Testing</li> <li>2. Basic Concepts of Hypothesis Testing</li> <li>3. Performing a Hypothesis Test</li> <li>4. One-Tailed and Two-Tailed Tests</li> <li>5. Confidence Interval and Hypothesis Test</li> <li>6. Some Important Tests<ul> <li>a. Test for One Mean</li> <li>b. Test for Equality of Means</li> <li>c. Test for Equality of Means \u2013 Equal Std Dev</li> <li>d. Test for Equality of Means \u2013 Unequal Std Dev</li> <li>e. Paired Test for Equality of Means</li> <li>f. Test for One Proportion</li> <li>g. Test for Two Proportions</li> <li>h. Test for One Variance</li> <li>i. Test for Equality of Variances</li> <li>j. Test of Independence</li> <li>k. ANOVA Test</li> </ul> </li> <li>CSV Download</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#1-hypothesis-testing","title":"1. Hypothesis Testing","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-introduction","title":"a. Introduction","text":"<ul> <li>Definition: Hypothesis testing is a statistical method to make decisions using data, often about population parameters.</li> <li>Purpose: Helps determine if there is enough evidence to support a specific claim about a population.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#what-is-hypothesis","title":"What is Hypothesis?","text":"<ul> <li>Often, we are interested in population parameter(s) (such as the mean, proportion, or variance for an entire group).</li> </ul> <p>A hypothesis is a conjecture about the population parameter(s).</p> <p>For example: - A bulb manufacturing company wants to know whether a new manufacturing process improves the reliability of the bulbs.   This leads to a hypothesis about the average bulb lifespan or failure rate under the new process.</p> <p> </p> <p>Objective of Hypothesis Testing: - To set a value (or claim) for the population parameter(s), and - To perform a statistical test to see whether that value is supported (\u201ctenable\u201d) by the evidence gathered from a sample.</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-hypothesis-formulation","title":"b. Hypothesis Formulation","text":"<ul> <li>Formulating Hypotheses:  </li> <li>Null Hypothesis (H\u2080): The default position or status quo (e.g., \u201cno effect,\u201d \u201cno difference\u201d).</li> <li>Alternative Hypothesis (H\u2081 or Ha): What you seek to prove (e.g., \u201cthere is an effect,\u201d \u201cthere is a difference\u201d).</li> <li>Steps:</li> <li>Clearly define the research question.</li> <li>Translate into statistical hypotheses (H\u2080 &amp; H\u2081).</li> <li>Select significance level (\u03b1, e.g., 0.05).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example","title":"Use Case Example","text":"<p>A pharmaceutical company tests if a new drug lowers blood pressure more than the current standard. - H\u2080: New drug is no better than the standard. - H\u2081: New drug lowers blood pressure more than the standard.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#why-hypothesis","title":"Why Hypothesis?","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#estimation","title":"Estimation","text":"<p>The problem of estimation is considered when there is no previous knowledge of the population parameter. The problem is simpler in this case: - A random sample is taken, - A sample statistic is computed, - An appropriate point and interval estimate is suggested.</p> <p>Estimation helps us calculate values for unknown parameters, but does not test any assumptions about those parameters.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Often, the interest is not in the numerical value of the point estimate of the parameter, but in knowing the plausibility of a hypothesis about the population parameter by using sample data. - Estimation alone is not enough to arrive at a conclusion in such cases.</p> <p>Hypothesis testing allows us to assess whether the data supports a specific claim about a population parameter, beyond just estimating its value.</p> <p>Back to Top Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#2-basic-concepts-of-hypothesis-testing","title":"2. Basic Concepts of Hypothesis Testing","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-importance-of-null","title":"a. Importance of Null","text":"<ul> <li>The null hypothesis provides a baseline or default assumption.</li> <li>All evidence is measured against H\u2080.</li> <li>Without H\u2080, statistical significance cannot be determined.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-importance-of-test-statistic","title":"b. Importance of Test Statistic","text":"<ul> <li>The test statistic (e.g., t, z, \u03c7\u00b2) is a calculated value that helps determine whether to reject H\u2080.</li> <li>It quantifies how far sample data deviates from what H\u2080 predicts.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-type-i-and-type-ii-errors","title":"c. Type I and Type II Errors","text":"<ul> <li>Type I Error (\u03b1): Incorrectly rejecting H\u2080 (false positive).</li> <li>Type II Error (\u03b2): Failing to reject H\u2080 when it is false (false negative).</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-table-example","title":"Error Table Example","text":"H\u2080 True H\u2080 False Reject H\u2080 Type I Correct Fail to Reject Correct Type II"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#type-i-and-type-ii-errors-real-world-examples","title":"Type I and Type II Errors: Real-World Examples","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-1-supermarket-waiting-times","title":"Example 1: Supermarket Waiting Times","text":"<p>Scenario: The store manager believes that the average waiting time for customers at checkouts has become worse than 15 minutes.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypotheses","title":"Hypotheses","text":"Symbol Statement H\u2080 The average waiting time at checkouts is \u2264 15 min. H\u2090 The average waiting time at checkouts is &gt; 15 min."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-types-in-supermarket-example","title":"Error Types in Supermarket Example","text":"Error Type What It Means Example Statement Type I Error False Positive Waiting time is \u2264 15 min, but manager concludes it is &gt; 15 min. Type II Error False Negative Waiting time is &gt; 15 min, but manager concludes it is \u2264 15 min. <ul> <li> <p>Type I Error: Rejecting the null hypothesis when it is actually true. E.g., The waiting time is really \u2264 15 min, but the manager says it\u2019s &gt; 15 min.</p> </li> <li> <p>Type II Error: Failing to reject the null hypothesis when it is actually false. E.g., The waiting time is really &gt; 15 min, but the manager says it\u2019s \u2264 15 min.</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-2-cancer-diagnosis","title":"Example 2: Cancer Diagnosis","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypotheses_1","title":"Hypotheses","text":"Symbol Statement H\u2080 The patient does not have cancer. H\u2090 The patient has cancer."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#error-types-in-medical-example","title":"Error Types in Medical Example","text":"Error Type What It Means Example Statement Type I Error False Positive Patient doesn't have cancer, but the doctor says she does. Type II Error False Negative Patient does have cancer, but the report says she doesn't. <ul> <li> <p>Type I Error: Diagnosing cancer when the patient actually does not have cancer. (False positive)</p> </li> <li> <p>Type II Error: Failing to diagnose cancer when the patient actually does have cancer. (False negative)</p> </li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#side-by-side-summary-table","title":"Side-by-Side Summary Table","text":"Error Type Supermarket Example Medical Example Type I Error Manager wrongly thinks waiting &gt; 15 min Doctor wrongly says patient has cancer Type II Error Manager wrongly thinks waiting \u2264 15 min Doctor/report wrongly says patient has no cancer <p>Summary: - Type I Error: False alarm (detecting an effect that isn't there) - Type II Error: Missed detection (failing to detect an effect that is there)</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-hypothesis-testing-template","title":"d. Hypothesis Testing Template","text":"<p>Standard Steps:</p> <ol> <li>State H\u2080 and H\u2081.</li> <li>Choose significance level (\u03b1).</li> <li>Select appropriate test statistic.</li> <li>Determine critical region or compute p-value.</li> <li>Make a statistical decision.</li> </ol>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-template","title":"Hypothesis Testing Template","text":"Step Action Description 1 Identify the key question What is the research question that you are trying to answer? 2 Establish the hypotheses What is the metric of interest? Define the Null and Alternate Hypothesis. 3 Understand and prepare data What data do you have? Do you understand what it means? Can it be used directly? 4 Identify the right test Choose the method for testing based on the previous steps. 5 Check the assumptions Ensure that data satisfies the assumption for the test. 6 Perform the test Get to conclusion based on the results (p-value)."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#summary-checklist","title":"Summary Checklist","text":"<ol> <li> <p>Identify the key question:    What are you trying to answer?</p> </li> <li> <p>Establish the hypotheses:    Define the metric and state H\u2080 and H\u2090.</p> </li> <li> <p>Understand and prepare data:    Ensure you have the right data and understand its meaning.</p> </li> <li> <p>Identify the right test:    Select the appropriate statistical test.</p> </li> <li> <p>Check the assumptions:    Make sure assumptions of the test are met.</p> </li> <li> <p>Perform the test:    Draw a conclusion from your results (such as the p-value).</p> </li> </ol> <p>Back to Top</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_1","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_1","title":"Use Case Example","text":"<p>A factory tests if the mean diameter of produced bolts is 10 mm. - H\u2080: Mean = 10 mm - Use t-statistic to measure sample deviation from 10 mm.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#3-performing-a-hypothesis-test","title":"3. Performing a Hypothesis Test","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-some-key-ideas","title":"a. Some Key Ideas","text":"<ul> <li>Define your question &amp; hypotheses.</li> <li>Choose an appropriate statistical test for your data.</li> <li>Understand the implications of test results.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-assumptions","title":"b. Assumptions","text":"<ul> <li>Every statistical test has underlying assumptions (normality, equal variances, random sampling).</li> <li>If assumptions are violated, results may not be valid.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-critical-point","title":"c. Critical Point","text":"<ul> <li>The value that defines the threshold for rejecting H\u2080.</li> <li>E.g., for \u03b1 = 0.05 in a z-test, critical values are \u00b11.96.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-rejection-region-approach","title":"d. Rejection Region Approach","text":"<ul> <li>Identify areas under the probability curve (tails) where H\u2080 is rejected.</li> <li>If test statistic falls in this region, reject H\u2080.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#e-p-value-approach","title":"e. p-value Approach","text":"<ul> <li>The p-value is the probability of obtaining test results at least as extreme as the observed, assuming H\u2080 is true.</li> <li>If p &lt; \u03b1, reject H\u2080.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_2","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_2","title":"Use Case Example","text":"<p>A marketing analyst tests if a new ad campaign changes sales figures. - Assumptions: data are independent, normal. - The critical region and p-value determine the outcome.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#summary","title":"Summary","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#key-concepts-in-hypothesis-testing","title":"Key Concepts in Hypothesis Testing","text":"Concept Explanation Level of Significance (\u03b1) - Probability of rejecting the null hypothesis when it is true.- Fixed before the hypothesis test. p-value - Probability of observing a test statistic (or more extreme) under the null hypothesis.- Depends on sample data. Alpha (\u03b1) is pre-fixed but p-value depends on the value of the test statistic. Acceptance or Rejection Region - The area under the distribution curve is partitioned into acceptance and rejection regions.- Reject the null hypothesis if the test statistic falls in the rejection region; otherwise, fail to reject it."},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#lets-start-simple","title":"Let's Start Simple","text":"<p>Consider the following questions in hypothesis testing:</p> Question Question What are the null and alternative hypotheses? What is an appropriate test statistic? What is preset level of significance? How to check whether the data is giving significant evidence against the null hypothesis or not? <p>Let's see an example and understand the significance of the above questions.</p> <p>For simplicity, we will assume that the population standard deviation is known and the sample size is more than 30.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example","title":"Example","text":"<p>It is known from experience that for a certain E-commerce company, the mean delivery time of the products is 5 days with a standard deviation of 1.3 days.</p> <p>The new customer service manager of the company is afraid that the company is slipping and collects a random sample of 45 orders. The mean delivery time of these samples comes out to be 5.25 days.</p> <p>Is there enough statistical evidence for the manager\u2019s apprehension that the mean delivery time of products is greater than 5 days?</p> <p>Note: This is clearly a one-tailed test, concerning population mean \u03bc\u2014the mean delivery time of products.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#first-test-z-test-for-one-mean","title":"First test - Z-test for One Mean","text":"Significance of the test Assumptions Test Statistic Distribution Test for population mean   \\(H_0: \\mu = \\mu_0\\) - Continuous data  - Normally distributed population or sample size \\(&gt; 30\\)  - Known population standard deviation \\(\\sigma\\)  - Random sampling from the population Standard Normal distribution"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_3","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#4-one-tailed-and-two-tailed-tests","title":"4. One-Tailed and Two-Tailed Tests","text":"<ul> <li>One-Tailed Test: Tests for deviation in one direction only (e.g., \u201cgreater than\u201d or \u201cless than\u201d).</li> <li>Two-Tailed Test: Tests for deviation in both directions (e.g., \u201cnot equal to\u201d).</li> </ul> <p>When to use: - Use one-tailed if you only care about increase/decrease. - Use two-tailed if you care about any change.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_4","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_3","title":"Use Case Example","text":"<p>A teacher tests if a new teaching method improves scores (one-tailed), or if it changes scores in either direction (two-tailed).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#alternative-hypothesis-types","title":"Alternative Hypothesis Types","text":"Test Type Description Mathematical Form One-tailed test Greater than type \\(H_a: \\mu &gt; \\mu_0\\) Less than type \\(H_a: \\mu &lt; \\mu_0\\) Two-tailed test Not equal type \\(H_a: \\mu \\neq \\mu_0\\)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#flow-summary","title":"Flow Summary","text":"<ul> <li>Alternative Hypothesis<ul> <li>One-tailed test<ul> <li>Greater than type:\u2003\\(H_a: \\mu &gt; \\mu_0\\)</li> <li>Less than type:\u2003\\(H_a: \\mu &lt; \\mu_0\\)</li> </ul> </li> <li>Two-tailed test<ul> <li>Not equal type:\u2003\\(H_a: \\mu \\neq \\mu_0\\)</li> </ul> </li> </ul> </li> </ul> <p>Tip: - Use one-tailed tests for directional hypotheses (\"greater than\" or \"less than\"). - Use a two-tailed test when testing for any difference (not direction).</p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#5-confidence-interval-and-hypothesis-test","title":"5. Confidence Interval and Hypothesis Test","text":"<ul> <li>Confidence Interval (CI): A range within which a population parameter is expected to fall, with a certain confidence level (e.g., 95%).</li> <li>Relationship to Hypothesis Testing: </li> <li>If a CI for mean difference excludes 0, H\u2080 (\u201cno difference\u201d) is rejected at the equivalent significance level.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#confidence-interval-vs-hypothesis-testing","title":"Confidence Interval vs Hypothesis Testing","text":"<p>Suppose we calculate the \\((100 - 5)\\%\\) confidence interval for the mean.</p> <p>We also conduct the Z-test for the mean with a 5% significance level.</p> <p>The hypotheses of the Z-test are:</p> \\[ H_0 : \\mu = \\mu_0 \\quad \\text{against} \\quad H_a : \\mu \\neq \\mu_0 \\]"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#is-there-any-relationship-between-the-estimated-confidence-interval-and-the-hypothesis-test","title":"Is there any relationship between the estimated confidence interval and the hypothesis test?","text":"<p>The confidence interval contains all values of \\(\\mu_0\\) for which the null hypothesis will not be rejected.</p> <p>Key Points: - If the hypothesized value (\\(\\mu_0\\)) falls within the confidence interval, we do not reject the null hypothesis at the given significance level. - If \\(\\mu_0\\) is outside the confidence interval, we reject the null hypothesis.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_5","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#use-case-example_4","title":"Use Case Example","text":"<p>A clinical study reports the CI for treatment effect does not include zero, supporting the alternative hypothesis.</p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#6-some-important-tests","title":"6. Some Important Tests","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-frameworks","title":"Hypothesis Testing Frameworks","text":"<p>Choice of test depends on test statistic and data availability</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#a-test-for-one-mean","title":"a. Test for One Mean","text":"<p>What: Tests if the mean of a sample differs from a known or hypothesized population mean (one-sample t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc = \u03bc\u2080 (The sample mean equals the population mean) - H\u2081: \u03bc \u2260 \u03bc\u2080 (The sample mean does not equal the population mean)</p> <p>Sample Use Case: A coffee chain wants to check if the average amount of coffee in its \u201c12oz\u201d cup differs from 12 ounces, based on a random sample of cups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-testing-example-zyx-food-delivery-claim","title":"\ud83d\udcca Hypothesis Testing Example: ZYX Food Delivery Claim","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#scenario","title":"\ud83d\udcdd Scenario","text":"<p>A certain food aggregator ZYX is facing stiff competition from its main rival SWG during the Corona period. To retain business, ZYX is advertising that:</p> <p>Within a radius of 5 km from the restaurant where the order is placed, ZYX can still deliver in 40 minutes or less on average, regardless of changed conditions.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#data-collection","title":"\ud83d\udce6 Data Collection","text":"<ul> <li>Delivery times in minutes of 25 randomly selected deliveries are provided in a CSV file.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#problem-statement","title":"\u2753 Problem Statement","text":"<p>Assuming the delivery time distribution is approximately normal, is there enough statistical evidence to reject ZYX\u2019s claim?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#hypothesis-framing","title":"\ud83d\udd0d Hypothesis Framing","text":"<p>This is a one-tailed hypothesis test concerning the population mean \\( \\mu \\) \u2014 the average delivery time.</p> <ul> <li>Null Hypothesis (\\( H_0 \\)): \\( \\mu \\leq 40 \\) </li> <li>Alternative Hypothesis (\\( H_a \\)): \\( \\mu &gt; 40 \\) <p>Claiming that ZYX cannot deliver in 40 minutes or less, on average.</p> </li> </ul> <p>## \ud83e\uddea Test Overview Table</p> Significance of the Test Assumptions Test Statistic Distribution Test for population mean \\(H_0: \\mu = \\mu_0\\) - Continuous data   - Normally distributed population and sample size &lt; 30   - Unknown population standard deviation  - Random sampling from the population t distribution   (The test is also known as One-sample t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#goal","title":"\ud83c\udfaf Goal","text":"<p>Determine whether ZYX's claim is statistically valid using hypothesis testing methods (e.g., one-sample t-test), based on the sample of 25 deliveries.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_6","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#b-test-for-equality-of-means","title":"b. Test for Equality of Means","text":"<p>What: Tests if the means from two independent samples are equal (independent two-sample t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: A medical researcher wants to know if two different blood pressure drugs have different average effects.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example_1","title":"Example","text":"<p>To compare customer satisfaction levels of two competing media channels, 150 customers of Channel 1 and 300 customers of Channel 2 were randomly selected and were asked to rate their channels on a scale of 1\u20135, with 1 being least satisfied and 5 most satisfied. (The survey results are summarized in a CSV file.)</p> <p>Test at 0.05 level of significance whether the data provide sufficient evidence to conclude that Channel 1 has a higher mean satisfaction rating than Channel 2.</p> <p>\ud83d\udca1 This is a two-sample problem where Channel 1 and Channel 2 populations are independent. Further, this is a one-tailed hypothesis problem, concerning population means \\(\\mu_1\\) and \\(\\mu_2\\), the mean customer satisfaction for Channel 1 and Channel 2 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-known-std-devs","title":"Test for Equality of Means \u2013 Known Std Devs","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   \\(H_0 : \\mu_1 = \\mu_2\\) - Continuous data  - Normally distributed population or sample size &gt; 30  - Independent populations  - Known population standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\)  - Random sampling from the population Standard Normal distribution  (The test is also known as  Two independent sample z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_7","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#c-test-for-equality-of-means-equal-std-dev","title":"c. Test for Equality of Means \u2013 Equal Std Dev","text":"<p>What: Special case of two-sample t-test where population standard deviations are assumed equal.</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082, \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: Comparing average test scores between two classes, assuming the spread of scores (variance) is the same for both groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-comparing-daily-media-usage","title":"Example: Comparing Daily Media Usage","text":"<p>In the lockdown period, because of working from home and increased screen time, many opted for listening to FM Radio for entertainment rather than watching Cable TV. An advertisement agency randomly collected daily usage time data (in minutes) from both types of users and stored it in a CSV file.</p> <p>Question: Assuming daily Radio and TV usage time are normally distributed, do we have enough evidence to conclude that there is any difference between daily TV and Radio usage time at a 0.05 significance level?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#statistical-framing","title":"Statistical Framing:","text":"<ul> <li>Type of Test: Two-sample independent test (comparing means)</li> <li>Null Hypothesis (\\(H_0\\)): \\(\\mu_{\\text{TV}} = \\mu_{\\text{Radio}}\\) (No difference in mean usage time)</li> <li>Alternative Hypothesis (\\(H_a\\)): \\(\\mu_{\\text{TV}} \\ne \\mu_{\\text{Radio}}\\) (There is a significant difference in mean usage time)</li> <li>Significance Level: \\(\\alpha = 0.05\\)</li> <li>Assumptions:</li> <li>Normal distribution of usage times</li> <li>Independent samples</li> <li>Continuous data</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-equal-standard-deviations","title":"Test for Equality of Means: Equal Standard Deviations","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   \\(H_0\\): \\(\\mu_1 = \\mu_2\\) - Continuous data   - Normally distributed populations   - Independent populations   - Equal population standard deviations   - Random sampling from the population t distribution  (The test is also known as Two independent sample t-test ) #### \ud83e\uddea Open in Colab: \ud83d\udc49 Notebook_Hypothesis_Testing.ipynb"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#d-test-for-equality-of-means-unequal-std-dev","title":"d. Test for Equality of Means \u2013 Unequal Std Dev","text":"<p>What: Two-sample t-test without assuming equal variances (Welch\u2019s t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 - H\u2081: \u03bc\u2081 \u2260 \u03bc\u2082</p> <p>Sample Use Case: A tech company compares the mean time to resolve tickets for two support teams with different levels of experience (variances likely differ).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example_2","title":"Example","text":"<p>SAT verbal scores of two groups of students are given in a CSV file. The first group, College, contains scores of students whose parents have at least a bachelor\u2019s degree, and the second group, High School, contains scores of students whose parents do not have any college degree.</p> <p>The Education Department is interested to know whether the sample data support the theory that students show a higher population mean verbal score on SAT if their parents attain a higher level of education.</p> <p>Assuming SAT verbal scores for two populations are normally distributed, do we have enough statistical evidence for this at a 5% significance level?</p> <p>This is a two-sample problem as the College and High School populations are different. Further, this is a one-tailed hypothesis problem, concerning population means \u03bc\u2081 and \u03bc\u2082, the mean verbal score on SAT for College and High School groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-means-unequal-std-devs","title":"Test for Equality of Means: Unequal Std Devs","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population meansH\u2080: \u03bc\u2081 = \u03bc\u2082 - Continuous data  - Normally distributed populations  - Independent populations  - Unequal population standard deviations  - Random sampling from the population t distribution(The test is also known as Two independent sample t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_8","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#e-paired-test-for-equality-of-means","title":"e. Paired Test for Equality of Means","text":"<p>What: Compares means from the same group at different times (paired t-test).</p> <p>Example Hypotheses: - H\u2080: \u03bc_before = \u03bc_after - H\u2081: \u03bc_before \u2260 \u03bc_after</p> <p>Sample Use Case: A gym measures client weights before and after a 12-week program to see if the mean weight has changed.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-paired-test-for-equality-of-means","title":"Example: Paired Test for Equality of Means","text":"<p>Typical prices of single-family homes in Florida are given for a sample of 15 metropolitan areas (in 1000 USD) for 2002 and 2003 in a CSV file.</p> <p>Assuming the house prices are normally distributed, do we have enough statistical evidence to say that there is an increase in the house price in one year at a 0.05 significance level?</p> <p>This is a paired sample problem as the two observations (for 2002 and 2003) are taken on one sampled unit (a metropolitan area). Further, this is a one-tailed hypothesis problem, concerning population means \u03bc\u2081 and \u03bc\u2082, the mean house price in 2002 and 2003 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#paired-test-for-equality-of-means","title":"Paired Test for Equality of Means","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population means   H\u2080: \u03bc\u2081 = \u03bc\u2082 - Continuous data   - Normally distributed populations  - Independent observations  - Random sampling from the population t distribution  (The test is also known as Paired t-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_9","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#f-test-for-one-proportion","title":"f. Test for One Proportion","text":"<p>What: Tests if a sample proportion equals a hypothesized value (one-sample z-test for proportions).</p> <p>Example Hypotheses: - H\u2080: p = p\u2080 - H\u2081: p \u2260 p\u2080</p> <p>Sample Use Case: A poll finds that 58% of voters favor a candidate. Is this significantly different from a hypothesized 50%?</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-one-proportion","title":"Example: Test for One Proportion","text":"<p>A researcher claims that Democratic party will win in the next United States Presidential election.</p> <p>To test her belief the researcher randomly surveyed 90 people and 24 out of them said that they voted for Democratic party.</p> <p>Is there enough evidence at \\(\\alpha = 0.05\\) to support this claim?</p> <p>This is clearly a one-tailed test, concerning population proportion p, the proportion of people voted from Democratic party.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-one-proportion","title":"Test for One Proportion","text":"Significance of the Test Assumptions Test Statistic Distribution Test for population proportionH\u2080: p = p\u2080 - Binomially distributed population  - Random sampling from the population  - When both mean (np) and n(1-p) are greater than or equal to 10, the binomial distribution can be approximated by a normal distribution Standard Normal distribution  (The test is also known as One proportion z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_10","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#g-test-for-two-proportions","title":"g. Test for Two Proportions","text":"<p>What: Tests if two sample proportions are equal.</p> <p>Example Hypotheses: - H\u2080: p\u2081 = p\u2082 - H\u2081: p\u2081 \u2260 p\u2082</p> <p>Sample Use Case: A vaccine trial compares the proportion of people who became ill in vaccine and placebo groups.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-two-proportions","title":"Example: Test for Two Proportions","text":"<p>A car manufacturer aims to improve its products\u2019 quality by reducing the defects. So, the manufacturer randomly checks the efficiency of two assembly lines in the shop floor. In line 1, there are 20 defects out of 200 samples and in line 2, there are 25 defects out of 400 samples.</p> <p>At 5% level of significance, do we have enough statistical evidence to conclude that the two assembly procedures are different?</p> <p>This is clearly a two-tailed test, concerning two population proportion p\u2081 and p\u2082, the proportion of defects in assembly line 1 and assembly line 2 respectively.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-two-proportions","title":"Test for Two Proportions","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population proportionsH\u2080: p\u2081 = p\u2082 - Binomially distributed populations  - Independent populations  - Random sampling from the populations  - When both mean (np) and n(1-p) are greater than or equal to 10, the binomial distribution can be approximated by a normal distribution Standard Normal distribution  (The test is also known as Two proportions z-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_11","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#h-test-for-one-variance","title":"h. Test for One Variance","text":"<p>What: Tests if a sample variance equals a specified value (chi-squared test).</p> <p>Example Hypotheses: - H\u2080: \u03c3\u00b2 = \u03c3\u2080\u00b2 - H\u2081: \u03c3\u00b2 \u2260 \u03c3\u2080\u00b2</p> <p>Sample Use Case: A manufacturer checks if the variability in machine part diameters is within the required tolerance.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-variance","title":"Test for Variance","text":"<p>Variance tests are used for a comparison of variability, often as a predecessor for other tests.</p> <p>Let us take many samples of the same size from a normal population and find the sample variances.</p> <p>They follow a chi-square (\\(\\chi^2\\)) distribution, which is dependent on the degrees of freedom.</p> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-one-variance","title":"Example: Test for One Variance","text":"<p>It is conjectured that the standard deviation for the annual return of mid cap mutual funds is 22.4%, when all such funds are considered and over a long period of time. The sample standard deviation of a certain mid cap mutual fund based on a random sample of size 32 is observed to be 26.4%.</p> <p>Do we have enough evidence to claim that the standard deviation of the chosen mutual fund is greater than the conjectured standard deviation for mid cap mutual funds at 0.05 level of significance?</p> <p>This is clearly a one-tailed test, concerning population variance, the variance for mid cap mutual funds.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-one-variance","title":"Test for One Variance","text":"Significance of the Test Assumptions Test Statistic Distribution Test for population varianceH\u2080: \u03c3\u00b2 = \u03c3\u2080\u00b2 - Continuous data  - Normally distributed population  - Random sampling from the population Chi Square distribution  (The test is also known as Chi-square test for variance)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_12","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#i-test-for-equality-of-variances","title":"i. Test for Equality of Variances","text":"<p>What: Tests if two (or more) population variances are equal (F-test, Levene\u2019s test).</p> <p>Example Hypotheses: - H\u2080: \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - H\u2081: \u03c3\u2081\u00b2 \u2260 \u03c3\u2082\u00b2</p> <p>Sample Use Case: An industrial engineer compares process variability before and after a process improvement.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-test-for-equality-of-variances","title":"Example: Test for Equality of Variances","text":"<p>Note: The hypothesis test in the example is a two-tailed test and not a one-tailed test.</p> <p>The variance of a process is an important quality of the process. A large variance implies that the process needs better control and there is opportunity to improve.</p> <p>The data (<code>Bags.csv</code>) includes weights for two different sets of bags manufactured from two different machines. It is assumed that the weights for two sets of bags follow normal distribution.</p> <p>Do we have enough statistical evidence at 5% significance level to conclude that there is a significant difference between the variances of the bag weights for the two machines?</p> <p>This is clearly a two-tailed test, concerning two population variances, the variance for bag weights from two different machines.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#test-for-equality-of-variances","title":"Test for Equality of Variances","text":"Significance of the Test Assumptions Test Statistic Distribution Test for equality of two population variancesH\u2080: \u03c3\u2081\u00b2 = \u03c3\u2082\u00b2 - Normally distributed populations  - Independent populations  - Larger variance should be placed in the numerator F distribution  (The test is also known as F-test for variances)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_13","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#j-test-of-independence","title":"j. Test of Independence","text":"<p>What: Tests if two categorical variables are independent (Chi-squared test of independence).</p> <p>Example Hypotheses: - H\u2080: Variables are independent - H\u2081: Variables are not independent</p> <p>Sample Use Case: A health agency checks if smoking status is related to disease occurrence using a contingency table.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#chi-square-test-for-independence","title":"Chi-Square Test for Independence","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-1-smoking-and-gender","title":"Example 1: Smoking and Gender","text":"<p>A 2x2 contingency table describes two variables (smoking and gender), each at two levels, and stores the number of observations at each cell:</p> Male Female Total Smoker 120 100 220 Non-smoker 60 140 200 Total 180 240 420 <p>We are interested to know whether the two variables are independent.</p> <ul> <li>Null hypothesis (\\(H_0\\)): Smoking and gender are independent.</li> <li>Alternative hypothesis (\\(H_a\\)): Smoking and gender are not independent.</li> </ul>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#example-2-beverage-preference-and-age-group","title":"Example 2: Beverage Preference and Age Group","text":"<p>The following table summarizes beverage preference across different age groups:</p> Age Tea/Coffee Soft Drink Others 21\u201334 25 90 20 35\u201355 40 35 25 &gt; 55 24 15 30 <p>Does beverage preference depend on age?</p> <p>This is a problem of Chi-Square test of independence, concerning the two independent categorical variables, Age and Beverage Preference.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#chi-square-test-for-independence-key-points","title":"Chi-Square Test for Independence: Key Points","text":"Significance of the Test Assumptions Test Statistic Distribution In a contingency tableH\u2080: The row and column variables are independent - Categorical variables  - Expected value of the number of sample observations in each level of the variable is at least 5  - Random sampling from the population Chi Square distribution  (The test is also known as Chi-square test of independence)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_14","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#k-anova-test","title":"k. ANOVA Test","text":"<p>What: Tests if the means of three or more groups are equal (Analysis of Variance).</p> <p>Example Hypotheses: - H\u2080: \u03bc\u2081 = \u03bc\u2082 = \u03bc\u2083 = ... = \u03bc\u2096 - H\u2081: At least one \u03bc differs</p> <p>Sample Use Case: A marketing analyst wants to know if average sales differ by region (North, South, East, West).</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#anova-test-key-concepts","title":"ANOVA Test: Key Concepts","text":""},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#important-terms","title":"Important Terms","text":"<ul> <li>Response: Dependent variable which is continuous and assumed to follow a normal distribution.</li> <li>Factor: Independent explanatory variable with several levels.</li> </ul> <p>Example: Comparing the weekly volume of sales by different teams of sales executives.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#how-anova-works","title":"How ANOVA Works","text":"<p>F-Statistic is the ratio of the between-group variations to within-group variations.</p> \\[ F\\text{-statistic} = \\frac{\\text{Between group variations}}{\\text{Within group variations}} \\] <ul> <li>A large value of F-Statistic indicates more variation between groups than within groups.</li> <li>Thus, it will provide evidence against the null hypothesis.</li> </ul> <p></p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#real-world-example","title":"Real-World Example","text":"<p>A traffic management inspector in a city wants to understand whether carbon emissions from different cars are different. The inspector believes fuel type may be an important factor responsible for differences in carbon emission.</p> <ul> <li>The inspector collects random samples from all registered cars and tests if the amount of carbon emission released depends on fuel type at 5% significance level.</li> </ul> <p>Here, we will compare the means of emission for the three different fuel types.</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#anova-test-one-way-anova","title":"ANOVA Test: One-way ANOVA","text":"Significance of the Test Assumptions Test Statistic Distribution Test for means for more than two populations  H\u2080: All population means are equal - The populations are normally distributed  - Samples are independent simple random samples  - Population variances are equal F distribution  (The test is also known as One-way ANOVA F-test)"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_15","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p>Each test above should include: - A description - Example hypotheses - A use case (see your original file for inspiration) - Python example (or Colab link, as above)</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#open-in-colab_16","title":"\ud83e\uddea Open in Colab:","text":"<p>\ud83d\udc49 Notebook_Hypothesis_Testing.ipynb </p> <p></p> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#csv-download","title":"CSV Download","text":"<p>Download real datasets for hands-on practice.</p> <ul> <li>\ud83d\udc49 Download FastFood1.csv</li> <li>\ud83d\udc49 Download AOVData.csv</li> <li>\ud83d\udc49 Download Beverage.csv</li> <li>\ud83d\udc49 Download Bags1.csv</li> <li>\ud83d\udc49 Download Florida.csv</li> <li>\ud83d\udc49 Download SATVerbal1.csv</li> <li>\ud83d\udc49 Download TVRadio.csv</li> <li>\ud83d\udc49 Download rating.csv</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#11-pdf-slides","title":"11. PDF Slides \ud83d\udcc4","text":"<ul> <li>\ud83d\udc49 Download Lecture Slides \u2013 Hypothesis Testing</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/1-learning-path/03-hypothesis-testing/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>NumPy Documentation</li> <li>Pandas Documentation</li> <li>Seaborn Documentation</li> <li>Plotly for Python</li> <li>Khan Academy: Statistics &amp; Probability</li> <li>MIT OpenCourseWare: Statistics</li> <li>StatQuest YouTube Channel</li> <li>Scikit-learn User Guide</li> <li>Real Python: Hypothesis Testing</li> </ul> <p>Back to Top</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/","title":"\ud83e\udde0 Hypothesis Testing - Mobile Internet Case Study","text":""},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#1-overview","title":"1. Overview","text":"<p>ExperienceMyServices reported that a typical American spends an average of 144 minutes per day accessing the Internet via a mobile device, with a standard deviation of 110 minutes.</p> <p>To validate this claim, you collected a sample of 30 observations from friends and family and analyzed whether the population mean differs significantly from the reported 144 minutes.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#2-dataset-description","title":"2. Dataset Description","text":"<ul> <li>File Name: <code>InternetMobileTime.csv</code></li> <li>Observations: 30</li> <li>Variable: Daily internet usage (in minutes) via mobile device</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#3-methodology","title":"3. Methodology","text":"<p>We will perform a one-sample t-test using the following hypothesis:</p> <ul> <li>Null Hypothesis (H\u2080): \u03bc = 144 minutes  </li> <li>Alternative Hypothesis (H\u2081): \u03bc \u2260 144 minutes  </li> <li>Significance Level (\u03b1): 0.05  </li> </ul> <p>Assumptions: - The sample is randomly selected and independent. - The population is normally distributed.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#sample-code-snippet","title":"\ud83d\udd0d Sample Code Snippet","text":"<pre><code>import pandas as pd\nimport scipy.stats as st\n\n# Load the dataset\ndata = pd.read_csv('InternetMobileTime.csv')\n\n# Sample statistics\nsample_mean = data['Time'].mean()\nsample_std = data['Time'].std(ddof=1)\nn = len(data)\n\n# Hypothesized population mean\nmu_0 = 144\n\n# Perform one-sample t-test\nt_stat, p_value = st.ttest_1samp(data['Time'], popmean=mu_0)\n\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Conclusion\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is statistical evidence that the mean is different from 144 minutes.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No statistical evidence that the mean differs from 144 minutes.\")\n</code></pre>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>The analysis will help determine whether users are spending significantly more or less time than reported.</li> <li>If the p-value &lt; 0.05, we reject the null hypothesis and conclude the usage is significantly different from 144 minutes.</li> <li>If the p-value \u2265 0.05, we do not have enough evidence to dispute the claim.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\ud83d\udcf1 Helps telecom and app companies align their strategy with real user behavior \ud83d\udcca Validates or challenges industry reports through data-driven hypothesis testing \ud83d\udcc8 Supports decision-making for media and digital advertisers</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/01-case-study-mobile-usuage/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>Scipy t-test documentation</li> <li>Understanding Hypothesis Testing</li> <li>Colab Notebook</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/","title":"\ud83e\udde0 Inferential Statistics - Medicon Dose Testing Case Study","text":""},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ul> <li>1. Overview</li> <li>2. Dataset Description</li> <li>3. Methodology</li> <li>4. Python Implementation</li> <li>5. Insights &amp; Interpretation</li> <li>6. Use Case Impact</li> <li>7. CSV Download</li> <li>8. References &amp; Further Reading</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#1-overview","title":"1. Overview","text":"<p>Medicon, a pharmaceutical company, has manufactured the sixth batch (40,000 units) of COVID-19 vaccine doses. This vaccine has already passed clinical trials and over 200,000 doses have been administered.</p> <p>The sixth batch is now undergoing post-production quality testing to assess: - \u23f1\ufe0f Time of effect (how long it takes to cure COVID-19) - \u2705 Satisfactory outcomes (successful prevention without symptoms/side effects)</p> <p>This case study details the statistical quality assurance analysis for this batch.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#2-dataset-description","title":"2. Dataset Description","text":"<ul> <li>File Name: <code>doses.csv</code></li> <li>Observations: 50</li> <li>Columns: Time of effect (in hours) for each volunteer after taking the dose</li> </ul> <p>The dataset includes real measurements from 50 volunteers who received a dose from the sixth batch.</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#3-methodology","title":"3. Methodology","text":"<p>We will approach the analysis through two dimensions:</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#a-satisfactory-rate-probability-analysis","title":"A. Satisfactory Rate Probability Analysis","text":"<ul> <li>Assumes a dose is 10 times more likely to be satisfactory than unsatisfactory.</li> <li>This leads to:   [   P(    ext{unsatisfactory}) = \\frac{1}{11} \u2248 0.0909,\\quad P(   ext{satisfactory}) = \\frac{10}{11}   ]</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#b-time-of-effect-sampling-distribution","title":"B. Time of Effect (Sampling Distribution)","text":"<ul> <li>Use inferential statistics on the <code>doses.csv</code> sample.</li> <li>Tasks include:</li> <li>Estimating the probability for time of effect being &lt; 11.5 hours.</li> <li>Calculating the 90th percentile.</li> <li>Building a 95% confidence interval for the population mean.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#4-python-implementation","title":"4. Python Implementation","text":"<p>\ud83d\udc49 Open in Colab </p> <p></p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#sample-code-snippet","title":"\ud83d\udd0d Sample Code Snippet","text":"<pre><code># Example: Probability that exactly 3 out of 100 doses are unsatisfactory\nfrom scipy.stats import binom\n\nn = 100\np_unsatisfactory = 1 / 11\nprob_3_unsat = binom.pmf(3, n, p_unsatisfactory)\nprint(f\"Probability of exactly 3 unsatisfactory doses out of 100: {prob_3_unsat:.4f}\")\n</code></pre> <pre><code># Example: 95% Confidence Interval for Mean Time of Effect\nimport pandas as pd\nimport scipy.stats as st\nimport numpy as np\n\ndata = pd.read_csv('doses.csv')\nsample_mean = np.mean(data['Time'])\nsample_std = np.std(data['Time'], ddof=1)\nn = len(data)\nconfidence = 0.95\nmargin_error = st.t.ppf((1 + confidence) / 2., n-1) * (sample_std / np.sqrt(n))\n\nlower = sample_mean - margin_error\nupper = sample_mean + margin_error\nprint(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f}) hours\")\n</code></pre>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#5-insights-interpretation","title":"5. Insights &amp; Interpretation","text":"<ul> <li>The binomial model allows us to estimate how likely unsatisfactory doses appear in samples of size 100 or 200.</li> <li>The sampling distribution reveals:</li> <li>Most doses show effectiveness within a tight time range.</li> <li>The confidence interval suggests a high level of reliability for future production batches.</li> </ul>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#6-use-case-impact","title":"6. Use Case Impact","text":"<p>\u2714\ufe0f Helps Medicon decide whether to scale up production \u2714\ufe0f Allows regulatory teams to verify safety even post-clinical trials \u2714\ufe0f Supports bulk order decisions by external entities like city governments (e.g., NYC request for 200 doses)</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#7-csv-download","title":"7. CSV Download","text":"<p>\ud83d\udc49 Download CSV from Google Drive</p> <p>\ud83d\udcce View CSV in Google Drive</p>"},{"location":"01-foundation/2-statistics/2-additional-reference/02-case-study-medicon-dose-testing/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>Scipy binom documentation</li> <li>Confidence Intervals \u2014 Khan Academy</li> <li>Google Colab: Full Python Notebook</li> </ul>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/","title":"Q&amp;A Session 1: Probability &amp; Statistics","text":""},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#hypothesis-testing-qa","title":"\u2753 Hypothesis Testing Q&amp;A","text":""},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. How to set the value of k for binom.pmf() and binom.cdf()?</li> <li>2. What are the Empirical rules of a normal distribution?</li> <li>3. What is the degree of freedom?</li> <li>4. What are the functions used in Statistical Analysis from the Scipy Stats library to calculate probabilities in a normal distribution?</li> <li>5. Population vs Sample | Parameter vs Statistic</li> <li>6. When to Use pmf, pdf, cdf, and ppf</li> <li>7. Setting the Value of k in binom.pmf() and binom.cdf()</li> <li>8. loc and scale in Uniform Distribution</li> <li>9. Z-Score in Real Life</li> <li>10. What Does norm.ppf() Do?</li> <li>11. Why Subtract 1 in binom.cdf() But Not in norm.cdf()?</li> <li>12. Interpreting the p-value in Hypothesis Testing</li> <li>13. What Is the Difference Between Type I and Type II Errors?</li> <li>14. What is Statistical Power?</li> <li>15. What is alpha in hypothesis testing?</li> <li>16. What are the steps of conducting a hypothesis test?</li> <li>17. How to decide between normal and t-distribution for confidence interval?</li> <li>18. Which Python function is used to compute a confidence interval for the population mean?</li> </ul>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#1-how-to-set-the-value-of-k-for-binompmf-and-binomcdf-functions","title":"1. How to set the value of k for binom.pmf() and binom.cdf() functions?","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li>If you want the probability that X is exactly equal to x: <code>binom.pmf(k=x, ...)</code></li> <li>If you want the probability that X is less than or equal to x: <code>binom.cdf(k=x, ...)</code></li> <li>If you want the probability that X is greater than or equal to x: <code>1 - binom.cdf(k=x-1, ...)</code></li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#2-what-are-the-empirical-rules-of-a-normal-distribution","title":"2. What are the Empirical rules of a normal distribution?","text":"<p>Tags: <code>#normal distribution</code> <code>#standard deviations</code> <code>#empirical rule</code></p> <p>The empirical rule for a normal distribution: - 68% of data: within 1 standard deviation (\\(\\mu \\pm \\sigma\\)) - 95% of data: within 2 standard deviations (\\(\\mu \\pm 2\\sigma\\)) - 99.7% of data: within 3 standard deviations (\\(\\mu \\pm 3\\sigma\\))</p> <p>Example (Pizza Delivery): - Mean delivery time (\\(\\mu\\)): 30 min - Standard deviation (\\(\\sigma\\)): 5 min - 68%: 25\u201335 min (30 \u00b1 5) - 95%: 20\u201340 min (30 \u00b1 2\u00d75) - 99.7%: 15\u201345 min (30 \u00b1 3\u00d75)</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#3-what-is-the-degree-of-freedom","title":"3. What is the degree of freedom?","text":"<p>Tags: <code>#estimate</code> <code>#observations</code></p> <p>The degree of freedom (df) for an estimate is the number of independent pieces of information that went into calculating the estimate. - For n observations, the degree of freedom is typically n - 1.</p> <p>Example: If you have 10 observations and know their total sum, you only need 9 to determine the last one: df = 10 - 1 = 9</p> <p>This applies universally to many estimates.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#4-what-are-the-functions-used-in-statistical-analysis-from-the-scipy-stats-library-to-calculate-probabilities-in-a-normal-distribution","title":"4. What are the functions used in Statistical Analysis from the Scipy Stats library to calculate probabilities in a normal distribution?","text":"<p>Tags: <code>#pdf</code> <code>#cdf</code> <code>#ppf</code></p> <ul> <li><code>pdf(x, loc=0, scale=1)</code> \u2014 Probability density function.</li> <li><code>cdf(x, loc=0, scale=1)</code> \u2014 Cumulative distribution function.</li> <li><code>ppf(q, loc=0, scale=1)</code> \u2014 Percent point function (inverse of cdf, i.e., percentiles).</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#5-population-vs-sample-parameter-vs-statistic","title":"5. Population vs Sample | Parameter vs Statistic","text":"<p>Tags: <code>#population</code> <code>#sample</code> <code>#parameter</code> <code>#statistic</code></p> <ul> <li>A population is the complete group you want to study (e.g., all tech startups in Asia).</li> <li>A parameter is a fixed, unknown value that describes a population (e.g., average height of all CFA candidates).</li> <li>A sample is a subset taken from the population.</li> <li>A statistic is a known value calculated from the sample (e.g., sample mean or standard deviation).</li> </ul> <p>\ud83d\udcce Example: If the population mean height is unknown, we estimate it using the sample mean.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#6-when-to-use-pmf-pdf-cdf-and-ppf","title":"6. When to Use pmf, pdf, cdf, and ppf","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#ppf</code> <code>#probabilities</code></p> <ul> <li>pmf (P(X=x)): For discrete variables (e.g., Binomial).</li> <li>pdf (P(X=x)): For continuous variables (e.g., Normal).</li> <li>cdf (P(X\u2264x)): Cumulative probability for both discrete and continuous variables.</li> <li>ppf: Inverse of cdf. Given P(X\u2264x) = \u03b1, returns the value of <code>x</code>.</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#7-setting-the-value-of-k-in-binompmf-and-binomcdf","title":"7. Setting the Value of k in binom.pmf() and binom.cdf()","text":"<p>Tags: <code>#pmf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li><code>binom.pmf(k=x, ...)</code> \u2192 Calculates P(X = x)</li> <li><code>binom.cdf(k=x, ...)</code> \u2192 Calculates P(X \u2264 x)</li> <li><code>1 - binom.cdf(k=x-1, ...)</code> \u2192 Calculates P(X \u2265 x)</li> </ul> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#8-loc-and-scale-in-uniform-distribution","title":"8. loc and scale in Uniform Distribution","text":"<p>Tags: <code>#uniform distribution</code> <code>#loc</code> <code>#scale</code></p> <ul> <li>In <code>scipy.stats.uniform(loc, scale)</code>, the distribution is defined over <code>[loc, loc + scale]</code></li> <li>If X ~ U(1, 4), then:</li> <li><code>loc = 1</code>, <code>scale = 3</code></li> </ul> <p>Formula: If \\( X \\sim U(a, b) \\) \u21d2 <code>loc = a</code>, <code>scale = b - a</code></p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#9-z-score-in-real-life","title":"9. Z-Score in Real Life","text":"<p>Tags: <code>#z-score</code> <code>#standard deviations</code></p> <ul> <li>A z-score shows how many standard deviations a point is from the mean.</li> <li>Useful for comparing scores from different distributions.</li> </ul> <p>\ud83d\udcce Example: Comparing exam scores across different scales using z-score normalization.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#10-what-does-normppf-do","title":"10. What Does norm.ppf() Do?","text":"<p>Tags: <code>#ppf</code> <code>#cdf</code> <code>#probability</code></p> <ul> <li><code>norm.ppf(p)</code> is the inverse of <code>norm.cdf()</code>.</li> <li>It returns the value of <code>x</code> such that P(X \u2264 x) = p.</li> </ul> <p>\ud83d\udcce Example: <code>norm.ppf(0.92)</code> returns the point below which 92% of the data lies.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#11-why-subtract-1-in-binomcdf-but-not-in-normcdf","title":"11. Why Subtract 1 in binom.cdf() But Not in norm.cdf()?","text":"<p>Tags: <code>#cdf</code> <code>#distribution</code> <code>#continuous</code></p> <ul> <li>Binomial is discrete \u2192 Use <code>1 - binom.cdf(k-1, ...)</code></li> <li>Normal is continuous \u2192 Use <code>1 - norm.cdf(x)</code> (no subtraction)</li> </ul> <p>\ud83d\udccc Reason: In continuous distributions, P(X = x) = 0. In discrete, P(X = x) \u2260 0, so subtracting 1 ensures accuracy.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#12-interpreting-the-p-value-in-hypothesis-testing","title":"12. Interpreting the p-value in Hypothesis Testing","text":"<p>Tags: <code>#p-value</code> <code>#hypothesis testing</code> <code>#significance</code></p> <ul> <li>The p-value measures the probability of obtaining results as extreme as (or more extreme than) those observed, assuming the null hypothesis is true.</li> <li>A small p-value (typically \u2264 0.05) suggests strong evidence against the null hypothesis, so you may reject it.</li> <li>A large p-value suggests weak evidence against the null, so you fail to reject it.</li> </ul> <p>\ud83d\udcce Example: If p-value = 0.03, there is only a 3% chance that the observed results would occur if the null hypothesis were true.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#13-what-is-the-difference-between-type-i-and-type-ii-errors","title":"13. What Is the Difference Between Type I and Type II Errors?","text":"<p>Tags: <code>#type-i-error</code> <code>#type-ii-error</code> <code>#errors</code></p> <ul> <li>Type I Error: Rejecting the null hypothesis when it is actually true (false positive).</li> <li>Type II Error: Failing to reject the null hypothesis when it is actually false (false negative).</li> </ul> <p>\ud83d\udcce Example: - Type I: Convicting an innocent person. - Type II: Letting a guilty person go free.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#14-what-is-statistical-power","title":"14. What is Statistical Power?","text":"<p>Tags: <code>#statistical power</code> <code>#type-ii-error</code></p> <ul> <li>Statistical power is the probability that a test correctly rejects a false null hypothesis (i.e., detects an effect if there is one).</li> <li>Power = 1 - Probability of Type II Error (\u03b2).</li> </ul> <p>\ud83d\udcce Example: A test with 80% power will correctly identify a true effect 80% of the time.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#15-what-is-alpha-in-hypothesis-testing","title":"15. What is alpha in hypothesis testing?","text":"<p>Tags: <code>#hypothesis testing</code> <code>#probability</code></p> <p>Alpha (\\(\\alpha\\)) is the level of significance in hypothesis testing. It represents the probability of making a Type I error\u2014that is, rejecting the null hypothesis when it is actually true. Alpha is the small chance that your test result happened by random chance. Common values are 0.05 (5%) and 0.01 (1%). For \\(\\alpha = 0.05\\), you are 95% confident that your result is not due to chance. Note: Alpha must be chosen before running the test and should not be changed afterward.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#16-what-are-the-steps-of-conducting-a-hypothesis-test","title":"16. What are the steps of conducting a hypothesis test?","text":"<p>Tags: <code>#hypothesis testing</code> <code>#p-value</code></p> <p>The typical steps for conducting a hypothesis test are: 1. Formulate the null and alternative hypotheses for your problem. 2. Select the appropriate test based on your data and hypothesis. 3. Decide the significance level (\\(\\alpha\\)) before running the test. 4. Collect the relevant data for the test. 5. Calculate the p-value using the chosen statistical test. 6. Compare the p-value with alpha and draw your conclusion.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#17-how-to-decide-between-normal-and-t-distribution-for-confidence-interval","title":"17. How to decide between normal and t-distribution for confidence interval?","text":"<p>Tags: <code>#distribution</code> <code>#confidence interval</code> <code>#standard deviation</code></p> <p>The choice depends on whether the population standard deviation (\\(\\sigma\\)) is known: - If \\(\\sigma\\) is known, use the normal distribution to compute the confidence interval. - If \\(\\sigma\\) is unknown, estimate it using the sample standard deviation (\\(s\\)) and use the t-distribution for the confidence interval.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"01-foundation/2-statistics/3-q%26a/01-q%26a/#18-which-python-function-is-used-to-compute-a-confidence-interval-for-the-population-mean","title":"18. Which Python function is used to compute a confidence interval for the population mean?","text":"<p>Tags: <code>#confidence interval</code> <code>#population</code> <code>#sample</code></p> <p>The <code>interval()</code> function (from <code>scipy.stats</code>) computes a confidence interval.</p> <p>Example 1: Known Population Standard Deviation (Normal Distribution)</p> <p>```python import numpy as np from scipy.stats import norm</p> <p>Sample mean = 500, population std dev = 25, sample size = 100 np.round(norm.interval(0.95, loc=500, scale=25/np.sqrt(100)), 2)</p>"},{"location":"01-foundation/3-datascience-machine-learning/","title":"\ud83e\udd16 Data Science &amp; Machine Learning","text":"<p>Welcome to the Data Science &amp; ML section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/3-datascience-machine-learning/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Master the fundamentals of data workflows, model training, and evaluation.</p> <ul> <li>Data Preprocessing</li> <li>Feature Engineering</li> <li>Model Selection</li> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Model Evaluation</li> <li>Overfitting &amp; Regularization</li> </ul>"},{"location":"01-foundation/3-datascience-machine-learning/#additional-reference","title":"\ud83d\udcc2 Additional Reference","text":"<p>Supplement your learning with deep dives, examples, and best practices.</p> <ul> <li>Additional Reference</li> </ul>"},{"location":"01-foundation/3-datascience-machine-learning/#qa","title":"\u2753 Q&amp;A","text":"<p>Clarifications, insights, and FAQs from practitioners and learners.</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Foundation Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/","title":"Data Science &amp; Machine Learning","text":"<p>Welcome to the Data Science &amp; Machine Learning section of the Generative AI Study Hub.</p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#learning-path","title":"\ud83d\udcda Learning Path","text":"<p>Master end-to-end workflows from preprocessing to model evaluation.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/1-learning-path/</code></p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#additional-reference","title":"\ud83d\udcd8 Additional Reference","text":"<p>Study real-world case insights and methodological notes.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/2-additional-reference/</code></p>"},{"location":"01-foundation/3-datascience-machine-learning/3-q%26a/01-q%26a/#qa","title":"\u2753 Q&amp;A","text":"<p>Dig into insightful Q&amp;A on practical ML topics.</p> <p>\ud83d\udcc2 <code>3-datascience-machine-learning/3-q&amp;a/</code></p> <p>\u2b05\ufe0f Back to Home</p>"},{"location":"02-gen-ai-core/","title":"\ud83d\udd0d Gen AI Core","text":"<p>\ud83d\udcc2 Navigation Tip Explore the technical heart of Generative AI. The left sidebar lists key modules like transformer models and Retrieval-Augmented Generation (RAG) using LangChain.</p> <p>\ud83d\udca1 Recommended path: Start with \u201cTransformers &amp; Generative AI\u201d before moving to RAG-based workflows.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/","title":"\ud83e\udd16 Transformers &amp; Generative AI","text":"<p>Welcome to the Transformers &amp; Generative AI section of the Generative AI Atlas. This module guides you through the foundations, use cases, and advanced implementations of Transformer models and generative techniques.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Begin your journey through the core concepts and techniques related to Transformers and Generative AI:</p> <ul> <li>Intro</li> <li>Getting Started</li> <li>NLP Overview</li> <li>Transformer Intro</li> <li>Popular Transformer Models</li> <li>Using Transformers</li> <li>Real-World LLM Scenarios</li> <li>LLM Intro</li> <li>LLM Prep</li> <li>LLM Advanced</li> <li>LLM Specialized</li> <li>LLM Deployment</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>In-depth resources to supplement your knowledge:</p> <ul> <li>Supplemental Material</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/#qa","title":"\u2753 Q&amp;A","text":"<p>Clarifications, tips, and community-driven insights:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/","title":"What You\u2019ll Learn","text":"<p>\ud83d\udccc Quick Navigation</p> <ul> <li>What You\u2019ll Learn</li> <li>Part 1: Fundamentals of NLP and Transformers<ul> <li>Transformer Foundations</li> </ul> </li> <li>Part 2: Working with Large Language Models (LLMs)<ul> <li>Key Model Architectures</li> <li>Real-World Tasks You\u2019ll Implement</li> <li>Advanced Tooling and Techniques</li> </ul> </li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#part-1-fundamentals-of-nlp-and-transformers","title":"Part 1: Fundamentals of NLP and Transformers","text":"<p>You\u2019ll explore the evolution of natural language processing (NLP) through four historical phases:</p> <ul> <li>Rule-Based Systems </li> <li> <p>Manually defined rules for parsing, tagging, and other language tasks.</p> </li> <li> <p>Statistical Methods </p> </li> <li> <p>Used mathematical probability and co-occurrence to model language.</p> </li> <li> <p>Machine Learning Era </p> </li> <li> <p>Leveraged labeled data for training classifiers like SVMs and Naive Bayes.</p> </li> <li> <p>Deep Learning &amp; Embeddings </p> </li> <li>Enabled dense semantic understanding and contextual word representations.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#transformer-foundations","title":"Transformer Foundations","text":"<p>??? info \"Why Attention Matters in Transformers\"</p> <pre><code>The attention mechanism enables a model to focus on relevant portions of the input,  \nrather than treating every token equally. This allows:\n\n- Better context awareness\n- Improved long-range dependency modeling\n- Reduced reliance on fixed-size memory\n\n\ud83d\udd0d **Example:** In translation, attention helps align source and target tokens precisely.\n</code></pre> <p>You\u2019ll also build a deep understanding of the architecture that powers modern LLMs:</p> <ul> <li>Attention Mechanism </li> <li> <p>Allows models to focus on important parts of the input.</p> </li> <li> <p>Encoder-Decoder Structure </p> </li> <li> <p>Enables tasks like translation, summarization, and text generation.</p> </li> <li> <p>Tokenization &amp; Embeddings </p> </li> <li> <p>Converts text into vectors, enabling model computation.</p> </li> <li> <p>Pretraining and Fine-Tuning </p> </li> <li>Learn how general-purpose models adapt to specific tasks.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#part-2-working-with-large-language-models-llms","title":"Part 2: Working with Large Language Models (LLMs)","text":"<p>This section focuses on state-of-the-art transformer-based models and their practical applications.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#key-model-architectures","title":"Key Model Architectures","text":"<ul> <li>BERT (Encoder-only) </li> <li> <p>Learns bidirectional context; ideal for understanding tasks like classification or Q&amp;A.</p> </li> <li> <p>GPT (Decoder-only) </p> </li> <li> <p>Generates coherent, fluent text; the backbone of tools like ChatGPT.</p> </li> <li> <p>T5 (Encoder-Decoder) </p> </li> <li>Treats all problems as a text-to-text task, offering maximum flexibility.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#real-world-tasks-youll-implement","title":"Real-World Tasks You\u2019ll Implement","text":"<ul> <li>Masked Language Modeling (MLM)  </li> <li>Semantic Search with embeddings  </li> <li>Document-Based Question Answering  </li> <li>Instruction-Following Text Generation  </li> <li>Product Review Generation (prompt-based)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#advanced-tooling-and-techniques","title":"Advanced Tooling and Techniques","text":"<p>You\u2019ll gain hands-on experience with modern model optimization strategies:</p> <ul> <li>LoRA and PeFT (parameter-efficient fine-tuning)  </li> <li>8-bit / 4-bit quantization for faster, smaller models  </li> <li>FlashAttention, DeepSpeed, and FSDP for accelerated training  </li> <li>Chat templates and RLHF (Reinforcement Learning from Human Feedback)</li> </ul> <p>??? info \"Why Fine-Tuning is Crucial\"</p> <pre><code>Fine-tuning pre-trained models is essential when applying LLMs to specialized or production environments. It enhances the model's ability to understand domain-specific language, improves generalization, and reduces errors.\n\n**Benefits of Fine-Tuning:**\n\n- \u2705 Adapts general models to niche domains (e.g., law, healthcare, finance)\n- \ud83d\udcc8 Boosts model performance on specific downstream tasks\n- \ud83e\udde0 Learns contextual and jargon-heavy nuances\n- \ud83d\udcbe Saves compute compared to training from scratch\n\n\ud83d\udd0d **Example:**  \nFine-tuning BERT on clinical notes dramatically improves performance in electronic health record (EHR) classification tasks.\n</code></pre> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/01-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face: T5-base Amazon Product Reviews Model</li> <li>Hugging Face: DiabloGPT Open Instruct Model</li> <li>Google Colab: T5 Product Review Notebook</li> <li>Attention Is All You Need (Transformer Paper)</li> <li>Hugging Face Transformers Documentation</li> <li>Google AI Blog on BERT</li> <li>Sebastian Ruder: NLP Progress</li> <li>The Illustrated Transformer (by Jay Alammar)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/","title":"Getting Started","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Course Introduction</li> <li>2. Coding Environment Setup</li> <li>3. Tools &amp; Extensions</li> <li>4. Practical Coding Guidelines</li> <li>5. Colab &amp; Hugging Face Integration</li> <li>6. Next Steps</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#1-course-introduction","title":"1. Course Introduction","text":"<p>This course is designed to teach transformer architectures and large language model workflows, combining theory and hands-on implementation.</p> <ul> <li>The course is split into five major parts, each building on the previous.</li> <li>Initial sections are theory-heavy but short and packed with examples.</li> <li>Ideal for both beginners and experienced NLP practitioners.</li> <li>Designed for progressive skill development: from theory \u2192 models \u2192 coding \u2192 deployment.</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#2-coding-environment-setup","title":"2. Coding Environment Setup","text":"<ul> <li>Recommended editor: Visual Studio Code (VS Code)</li> <li>VS Code is:</li> <li>Free, lightweight, and open-source</li> <li>Well-integrated with remote environments and Jupyter workflows</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#vs-code-setup","title":"VS Code Setup","text":"<ol> <li>Install Python Extension in VS Code.</li> <li>Enable interactive mode:</li> <li>Press <code>Ctrl+Shift+P</code> or <code>Cmd+Shift+P</code> on Mac</li> <li>Search: \u201csend to interactive\u201d</li> <li>Enable this feature for real-time execution feedback</li> </ol> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#3-tools-extensions","title":"3. Tools &amp; Extensions","text":"<ul> <li>Google Colab Notebooks provided for each practical lesson:</li> <li>No local setup required</li> <li> <p>Accessible via \u201cResources\u201d button in each lesson</p> </li> <li> <p>Python Source Files available for download alongside Colab notebooks</p> </li> <li> <p>Remote Development using:</p> </li> <li>VS Code\u2019s Remote SSH Extension</li> <li>Setup:<ul> <li>Edit <code>~/.ssh/config</code> with remote VM/IP</li> <li>Connect via bottom-left VS Code status bar</li> </ul> </li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#4-practical-coding-guidelines","title":"4. Practical Coding Guidelines","text":"<ul> <li>Code alongside the instructor.</li> <li>Pause and reflect \u2014 do not just watch passively.</li> <li>Adopt a top-down learning strategy:</li> <li>Start from complete example code \u2192 adapt it to solve real problems</li> <li>Encouraged for better retention and understanding</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#5-colab-hugging-face-integration","title":"5. Colab &amp; Hugging Face Integration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#code-demo-access","title":"\ud83e\uddea Code Demo Access","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Code notebooks are synced with Colab</li> <li>Each lesson includes links to:</li> <li>Colab notebooks</li> <li>Python source files</li> <li>Trained model weights (where applicable)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#hugging-face-integration","title":"\ud83e\udd17 Hugging Face Integration","text":"<ul> <li>Models trained during this course are hosted on Hugging Face</li> <li>Resources panel includes direct links to model cards and weights</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#6-next-steps","title":"6. Next Steps","text":"<ul> <li>The next section will guide you through configuring your Python runtime and understanding core NLP concepts.</li> </ul> <p>\ud83d\udd1d Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/02-getting-started/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face Transformers Documentation</li> <li>Google Colaboratory Guide</li> <li>Visual Studio Code</li> <li>ArXiv: Attention Is All You Need</li> <li>PyTorch Docs</li> <li>TensorFlow Guide</li> <li>OpenAI Research</li> <li>NVIDIA LLM Blog</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/","title":"NLP Evolution Timeline","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/03-nlp-overview/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Word2Vec Explained (Google Research)</li> <li>GloVe: Global Vectors for Word Representation (Stanford)</li> <li>FastText (Facebook AI)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Sebastian Ruder: NLP Progress Tracker</li> <li>Hugging Face: T5-base Product Review Model</li> <li>Google Colab: Try Word Embeddings</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/","title":"Transformer Fundamentals","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Transformer Architecture Overview</li> <li>2. Transformer Training Paradigm: Pre-training and Fine-tuning</li> <li>3. Tokenization and Embeddings in Transformer Models</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#1-transformer-architecture-overview","title":"1. Transformer Architecture Overview","text":"<p>This lesson introduces the transformer model architecture, emphasizing its structural innovations, key mechanisms, and how it revolutionized NLP by overcoming the limitations of RNNs and LSTMs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#origins-and-significance","title":"Origins and Significance","text":"<ul> <li>Introduced in 2017 via the paper \"Attention Is All You Need\"</li> <li>Replaced sequential RNN/LSTM processing with fully parallel architecture</li> <li>Solved long-range dependency issues and improved training speed</li> <li>Enabled large-scale model training and breakthroughs in language understanding</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#core-components-of-transformer-architecture","title":"Core Components of Transformer Architecture","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#encoder-decoder-structure","title":"Encoder-Decoder Structure","text":"<ul> <li>Encoder: Converts input text into continuous vector representations capturing context and relationships</li> <li>Decoder: Generates output text from encoder\u2019s processed information</li> <li>Enables tasks like translation, summarization, and question answering</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#attention-mechanisms","title":"Attention Mechanisms","text":"<ul> <li>Self-Attention: Weighs each word relative to others to build context-aware representations</li> <li>Scaled Dot-Product Attention: Computes dot products, scales scores, and applies softmax</li> <li>Multi-Head Attention: Uses multiple heads to capture diverse semantic/syntactic patterns</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Compensates for lack of inherent word order in attention-only models</li> <li>Adds position-based signals to token embeddings</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#feed-forward-network-layer-normalization","title":"Feed-Forward Network &amp; Layer Normalization","text":"<ul> <li>Feed-Forward Network: Applies non-linear transformations to extract high-level features</li> <li>Layer Normalization: Stabilizes training by normalizing outputs between layers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#full-encoder-and-decoder-block","title":"Full Encoder and Decoder Block","text":"<ul> <li>Composed of stacked layers with:</li> <li>Multi-head attention</li> <li>Feed-forward networks</li> <li>Layer normalization</li> <li>Decoder includes additional encoder-decoder attention to align output generation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#real-world-application-example","title":"Real-World Application Example","text":"<p>Abstractive Question Answering</p> <ul> <li>Input: Paragraph + Question</li> <li>Encoder: Processes both into contextual embeddings</li> <li>Decoder: Generates an answer from the learned representation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Transformers enabled scalable, parallel NLP processing</li> <li>Encoder-decoder architecture allows diverse tasks</li> <li>Attention mechanisms are key to understanding global context</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#2-transformer-training-paradigm-pre-training-and-fine-tuning","title":"2. Transformer Training Paradigm: Pre-training and Fine-tuning","text":"<p>This lesson outlines the two-phase training process of transformer models\u2014pre-training and fine-tuning\u2014contrasting it with traditional ML workflows.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#training-structure-overview","title":"Training Structure Overview","text":"<ul> <li>Pre-training: General language learning from large unlabeled datasets</li> <li>Fine-tuning: Task-specific adaptation using labeled datasets</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#pre-training-phase","title":"Pre-training Phase","text":"<ul> <li>Learns grammar, context, word relationships, and long-range dependencies</li> <li>Massive-scale unsupervised training</li> <li>\ud83d\udd01 Analogy: Like learning music theory before mastering a genre</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#fine-tuning-phase","title":"Fine-tuning Phase","text":"<ul> <li>Adapts pre-trained models to tasks like NER, translation, QA, etc.</li> <li>Requires smaller supervised datasets</li> <li>Leverages transfer learning</li> <li>\ud83d\udd01 Analogy: Like a trained pianist specializing in jazz</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#combined-workflow","title":"Combined Workflow","text":"<ol> <li>Step 1: Pre-training</li> <li>Random initialization \u2192 trained on general data</li> <li>Step 2: Fine-tuning</li> <li>Task-specific data \u2192 adapted for downstream performance</li> </ol>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#real-world-considerations","title":"Real-world Considerations","text":"<ul> <li>Pre-training requires huge compute and data (done by orgs like Google, OpenAI)</li> <li>Most use pre-trained models and fine-tune</li> <li>Full pre-training is rare unless:</li> <li>You work with proprietary, underrepresented, or specialized domains (e.g., legal, clinical)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>Pre-training + fine-tuning is the standard approach in NLP</li> <li>Enables rapid model deployment with high performance</li> <li>Specialized domains may benefit from custom pre-training</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#3-tokenization-and-embeddings-in-transformer-models","title":"3. Tokenization and Embeddings in Transformer Models","text":"<p>This lesson covers how transformers process raw text into vector representations using tokenization and embeddings.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#tokenization","title":"Tokenization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#purpose","title":"Purpose","text":"<ul> <li>Breaks text into smaller units called tokens</li> <li>Translates natural language into numerical input (token IDs)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#types-of-tokenization","title":"Types of Tokenization","text":"<ul> <li>Word-level: One token per word; suffers from OOV (out-of-vocabulary) issues</li> <li>Character-level: Every character is a token; leads to longer sequences</li> <li>Subword-level (common): Breaks unknown words into known parts (e.g., Byte-Pair Encoding)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#workflow","title":"Workflow","text":"<ol> <li>Breaks text into tokens</li> <li>Maps tokens to IDs using a predefined vocabulary</li> <li>Feeds IDs into the transformer model</li> </ol>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#embeddings","title":"Embeddings","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#purpose_1","title":"Purpose","text":"<ul> <li>Convert token IDs into high-dimensional dense vectors</li> <li>Capture meaning and contextual usage of tokens</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#key-concepts","title":"Key Concepts","text":"<ul> <li>Embeddings are context-aware (e.g., \"bank\" in finance vs. riverbank)</li> <li>Contextual embeddings change based on surrounding text</li> <li>Learned during pre-training</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#example","title":"Example","text":"<p>```text Sentence 1: She picked a rose. Sentence 2: The sun rose early.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/04-transformer-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Original Transformer Paper)</li> <li>The Illustrated Transformer by Jay Alammar</li> <li>Hugging Face Transformers Documentation</li> <li>Google Colab: Transformer Architecture Notebook</li> <li>Hugging Face: T5-base Model (Amazon Product Reviews)</li> <li>Hugging Face: diabloGPT Instruction Model</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/","title":"Transformer Architectures Study Hub","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. BERT: Encoder-Only Transformer Architecture</li> <li>2. Transformer &amp; GPT Evolution</li> <li>3. T5: Text-To-Text Transfer Transformer</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#1-bert-encoder-only-transformer-architecture","title":"1. BERT: Encoder-Only Transformer Architecture","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP in 2018 by introducing a bidirectional, encoder-only architecture designed for deep contextual understanding of language. This section explores BERT\u2019s structure, training strategy, practical applications, and the latest advancements in its ecosystem.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#model-overview","title":"Model Overview","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#key-characteristics","title":"Key Characteristics","text":"<ul> <li> <p>Bidirectional   BERT reads text in both directions (left-to-right and right-to-left) simultaneously to capture full context.</p> </li> <li> <p>Encoder-Only Architecture   Built entirely on stacked encoders with self-attention mechanisms.   Optimized for understanding, not generating, text.</p> </li> <li> <p>Representations   Learns dense vector embeddings that reflect token meaning in context.</p> </li> <li> <p>Transformer-Based   Leverages the original transformer architecture\u2014only the encoder side.</p> </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-strategy","title":"Pre-training Strategy","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#datasets","title":"Datasets","text":"<ul> <li>English Wikipedia  </li> <li>10,000+ unpublished English books  </li> <li>Total: Over 3 billion words</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-objectives","title":"Pre-training Objectives","text":"<ul> <li> <p>Masked Language Modeling (MLM)   Randomly masks 15% of tokens; the model must predict them using surrounding context.   Enables deep semantic and syntactic comprehension.</p> </li> <li> <p>Next Sentence Prediction (NSP)   Trains BERT to classify whether one sentence follows another.   Aids understanding of inter-sentence relationships.   Later models (e.g., RoBERTa) removed this due to limited benefit.</p> </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#fine-tuning-applications","title":"Fine-tuning Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#text-classification","title":"Text Classification","text":"<ul> <li>Sentiment analysis, spam detection, topic categorization  </li> <li>Produces a single class label from the encoded text</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<ul> <li>Identifies token-level entities (e.g., people, dates, organizations)  </li> <li>BERT's contextual awareness improves accuracy in boundary detection</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#extractive-question-answering","title":"Extractive Question Answering","text":"<ul> <li>Extracts answers directly from a provided context passage  </li> <li>Predicts start and end token positions  </li> <li>Used in customer service, document retrieval</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#semantic-similarity","title":"Semantic Similarity","text":"<ul> <li>Produces embeddings for entire sentences or passages  </li> <li>Used in:</li> <li>Duplicate detection  </li> <li>Paraphrase recognition  </li> <li>Semantic search  </li> <li>Vector-based retrieval systems</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#bert-model-variants","title":"BERT Model Variants","text":"Model Parameters Notes BERT-Base ~110M 12 layers, 12 heads, 768 hidden units BERT-Large ~340M 24 layers, 16 heads, 1024 hidden units DistilBERT ~66M Lightweight version by Hugging Face RoBERTa ~125M+ No NSP, trained longer, dynamic masking (Meta) ALBERT ~12M\u2013223M Weight-sharing, efficient training (Google Research) DeBERTa Varies Disentangled attention and enhanced position embeddings (Microsoft)"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#latest-developments-as-of-2025","title":"Latest Developments (as of 2025)","text":"<ul> <li>BERT is foundational for retrieval-augmented generation (RAG) and embedding-based search systems.</li> <li>Multilingual BERT (mBERT) supports 100+ languages.</li> <li>BERT encoders are commonly paired with large decoders like GPT-4o for hybrid retrieval-generation systems.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#2-transformer-gpt-evolution","title":"2. Transformer &amp; GPT Evolution","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-45-orion","title":"GPT-4.5 (\u201cOrion\u201d)","text":"<ul> <li>Released: Feb 27, 2025</li> <li>Enhanced instruction-following, fewer hallucinations</li> <li>API &amp; ChatGPT Pro access</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-41-family","title":"GPT-4.1 Family","text":"<ul> <li>Released: April 14, 2025</li> <li>Includes mini/nano variants supporting 1M-token context</li> <li>More efficient than GPT-4o</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#reasoning-models-o1-o3-mini-o4-mini","title":"Reasoning Models (o1, o3-mini, o4-mini)","text":"<ul> <li>Optimized for logic, math, and science</li> <li>o3-mini and o4-mini include multimodal chain-of-thought support</li> <li>Ideal for autonomous agents and structured tool use</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#gpt-5-expected-august-2025","title":"GPT-5 (Expected August 2025)","text":"<ul> <li>Will include reasoning from o3</li> <li>Multimodal + open access discussions ongoing</li> <li>Expected to set a new benchmark for general-purpose AI</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Shift from scaling parameters to scaling reasoning</li> <li>GPT-4.5/5 marks evolution toward modular, low-latency, high-accuracy models</li> </ul> Model Category Architecture Strengths Use Cases GPT\u20114.5 Instructional GPT Decoder-only Prompt-following, fewer hallucinations General NLP, coding, chatbots GPT\u20114.1 mini Efficient GPT Decoder-only 1M context, fast inference Coding, RAG o3-mini Reasoning LLM Decoder-only Logic + math + tool use Agents, science tasks GPT\u20115 Unified Multi-module Multimodal, reasoning-first Enterprise AI, general AI <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#3-t5-text-to-text-transfer-transformer","title":"3. T5: Text-To-Text Transfer Transformer","text":"<p>T5 reframes every NLP problem as a text-to-text task (e.g., input: \u201cTranslate English to German: How are you?\u201d \u2192 output: \u201cWie geht es dir?\u201d). This unified approach enables a wide range of applications across translation, QA, summarization, and more.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#model-overview_1","title":"Model Overview","text":"<ul> <li>Encoder-decoder transformer with BERT-style encoding + GPT-style generation</li> <li>Flexible task control via text prefixes (e.g., \u201csummarize:\u201d, \u201ctranslate:\u201d)</li> <li>First model to fully embrace text-to-text multitask learning</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#pre-training-c4-dataset-fill-in-the-blank-generation","title":"Pre-training: C4 Dataset + Fill-in-the-Blank Generation","text":"<ul> <li>Uses a corrupt-and-reconstruct pre-training objective</li> <li>Learns both contextual understanding and sequence generation</li> <li>Trained on C4 (Colossal Cleaned Crawled Corpus)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Translation: Understands bidirectional input, generates fluent target text</li> <li>Summarization: Converts long passages into concise summaries</li> <li>Question Answering: Context-aware, generative answers</li> <li>Keyword Generation: Contextual phrase extraction</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#product-evolution-table","title":"Product Evolution Table","text":"Model Architecture Strengths Use Cases Developer T5-Base Encoder-Decoder Multitask learning, flexible Translation, QA, summarization Google AI mT5 Encoder-Decoder Multilingual model (100+ langs) Cross-lingual NLP Google AI FLAN-T5 Enc-Dec + Tuning Instruction tuning Zero-shot &amp; few-shot NLP Google Research UL2 Encoder-Decoder Supports multiple objective modes General-purpose transformer Google DeepMind Gemini 1.5 Multimodal Unified vision + text + code Multimodal reasoning, generation Google DeepMind"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#takeaways","title":"Takeaways","text":"<ul> <li>T5 demonstrates the power of a unified framework in solving diverse NLP tasks</li> <li>Its design has influenced instruction-tuned and multimodal model families</li> <li>Continues to power a range of Google products and NLP pipelines</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/05-popular-transformer-models/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google Research)</li> <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach (Meta AI)</li> <li>DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Microsoft)</li> <li>DistilBERT by Hugging Face (Model Page)</li> <li>mBERT: Multilingual BERT (Google AI)</li> <li>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</li> <li>FLAN-T5 Instruction-Tuned Models (Google Research)</li> <li>UL2: Unified Language Learning</li> <li>Gemini 1.5 Model Overview (Google DeepMind)</li> <li>GPT-4.5 and GPT-5 Updates (OpenAI Blog)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Key Concepts</li> <li>Tokenizer and Embeddings</li> <li>Masked Language Modeling</li> <li>Semantic Search Engine</li> <li>Model Evolution Table</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#course-overview","title":"Course Overview","text":"<p>This section focuses on transitioning from theoretical knowledge of transformer models to their practical implementation and engineering components, emphasizing real-world applications such as semantic search and embedding usage.</p> <ul> <li>Prepares learners to apply transformer embeddings for NLP tasks</li> <li>Covers tokenization, embeddings, model internals, and downstream tasks</li> <li>Includes practical hands-on coding with Hugging Face Transformers and PyTorch</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#key-concepts","title":"Key Concepts","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#transformer-engineering-focus","title":"Transformer Engineering Focus","text":"<ul> <li>Embeddings: Represent words/sentences as dense vectors for downstream processing</li> <li>Tokenization: Converts raw text to token IDs; includes handling special tokens</li> <li>Attention Mechanism: Key to contextual representation in transformers</li> <li>Model Inputs: Includes token IDs, attention masks, and token type IDs</li> <li>Sentence Transformers: Fine-tuned models for capturing sentence-level semantics</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tokenizer-and-embeddings","title":"Tokenizer and Embeddings","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tokenization-pipeline","title":"Tokenization Pipeline","text":"<ul> <li>Tokenizers split sentences into subword tokens</li> <li>Maintains a vocabulary of ~30k+ tokens</li> <li>Returns token IDs, attention masks, and token type IDs</li> <li>Important to use model-specific tokenizers for consistency</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#embeddings","title":"Embeddings","text":"<ul> <li>Token IDs are converted to high-dimensional vectors</li> <li>Two key outputs:</li> <li>Last Hidden State: Embeddings for individual tokens (shape: seq_len \u00d7 hidden_dim)</li> <li>Pooled Output: Embedding for the entire sequence, used in classification</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_1","title":"Try It Yourself","text":"<p>You can run and explore the notebook directly in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#semantic-distance","title":"Semantic Distance","text":"<ul> <li>Embeddings compared using cosine similarity</li> <li>Allows words with different meanings (e.g., \"fly\") to be distinguished contextually</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>Pretraining task for models like BERT</li> <li>Random tokens replaced with <code>[MASK]</code> and predicted by the model</li> <li>Output logits converted to probabilities via softmax</li> <li>Used to help the model build a strong language understanding foundation</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#example","title":"Example","text":"<ul> <li>Input: <code>\"I want to [MASK] pizza for tonight\"</code></li> <li>Output: <code>\"have\"</code>, <code>\"get\"</code>, <code>\"eat\"</code>, <code>\"make\"</code>, <code>\"order\"</code> as top predictions</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_2","title":"Try It Yourself","text":"<p>You can experiment with the code by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#semantic-search-engine","title":"Semantic Search Engine","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#goal","title":"Goal","text":"<p>Build a semantic search engine that finds the most relevant document to a query based on meaning, not keyword match.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#tools-dataset","title":"Tools &amp; Dataset","text":"<ul> <li>Dataset: Multi-News (2000 article summaries)</li> <li>Model: SentenceTransformer for lightweight sentence embeddings (384-dim)</li> <li>Libraries: Hugging Face Transformers, PyTorch, Pandas</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#process","title":"Process","text":"<ul> <li>Embed all documents once</li> <li>Embed user\u2019s query</li> <li>Compute cosine similarity between query and all document embeddings</li> <li>Retrieve top-k relevant results using <code>torch.topk</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#example-queries","title":"Example Queries","text":"<ul> <li>\"Artificial Intelligence\": returned AI-related articles</li> <li>\"Natural Disasters\": returned disaster-related summaries</li> <li>\"Law Enforcement\", \"Politics\": worked as expected</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#try-it-yourself_3","title":"Try It Yourself","text":"<p>Give it a try by opening the interactive Google Colab notebook below:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#model-evolution-table","title":"Model Evolution Table","text":"Model Name Category Architecture Strengths Ideal Use Cases Latest Version Info BERT Encoder-only Transformer Bidirectional context, strong understanding Text classification, Q&amp;A, embedding generation BERT-Base / BERT-Large GPT Decoder-only Transformer Text generation, instruction following Chatbots, creative writing, code generation GPT-4o (June 2024) T5 Encoder-Decoder Transformer Unified text-to-text architecture Translation, summarization, Q&amp;A T5.1.1, Flan-T5 Gemini Multi-modal Transformer + Vision + Memory Text + image processing, powerful LLM+VLM hybrid Multi-modal tasks, agentic reasoning Gemini 1.5 (June 2025) SentenceTransformer Encoder-only Siamese / Bi-encoder Transformer Sentence similarity, semantic search Embedding generation, retrieval, clustering <code>all-MiniLM-L6-v2</code> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/06-using-transformers/#references-further-exploration","title":"References &amp; Further Exploration","text":"<ul> <li>\ud83e\udd17 Hugging Face Models and Tools</li> <li>BERT (bert-base-uncased)</li> <li>GPT-2 (gpt2)</li> <li>T5 (t5-base)</li> <li>SentenceTransformer (all-MiniLM-L6-v2)</li> <li> <p>T5-based Amazon Product Review Generator by TheFuzzyScientist</p> </li> <li> <p>\ud83d\udcd3 Colab Notebooks (Used in This Module)</p> </li> <li>Tokenizer &amp; Embeddings Colab</li> <li>Masked Language Modeling (MLM) Demo</li> <li>Semantic Search with Transformers</li> <li> <p>Tokenizer Pipeline Walkthrough</p> </li> <li> <p>\ud83d\udcda Further Reading</p> </li> <li>Attention Is All You Need (Vaswani et al.)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Hugging Face Transformers Documentation</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Part 1: NLP &amp; Transformer Fundamentals</li> <li>Part 2: Practical LLM Applications</li> <li>Model Comparison Summary</li> <li>Key Takeaways</li> <li>References &amp; Further Exploration</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#course-overview","title":"Course Overview","text":"<p>This course is a hands-on introduction to transformer-based language models, combining theoretical foundations with practical implementations. The curriculum covers BERT, GPT, and T5 models, including their use in real-world NLP tasks.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#part-1-nlp-transformer-fundamentals","title":"Part 1: NLP &amp; Transformer Fundamentals","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#historical-phases-of-nlp","title":"Historical Phases of NLP","text":"<ul> <li>Rule-Based Systems: Manually defined linguistic rules</li> <li>Statistical Methods: Word co-occurrence and probabilistic models</li> <li>Machine Learning: Feature-based methods (e.g., SVM, Naive Bayes)</li> <li>Deep Learning: Dense vector embeddings and neural models</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#core-transformer-concepts","title":"Core Transformer Concepts","text":"<ul> <li>Attention Mechanism: Enables global contextual representation</li> <li>Tokenization: Breaks text into subwords with positional info</li> <li>Encoder-Decoder: Structure used in models like T5, BART</li> <li>Fine-tuning: Adjusts pretrained models for specific tasks</li> </ul> <p>\ud83d\udd17 Reference: Illustrated Transformer \u2013 Jay Alammar</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#part-2-practical-llm-applications","title":"Part 2: Practical LLM Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#bert-extractive-question-answering","title":"\ud83d\udfe2 BERT \u2013 Extractive Question Answering","text":"<ul> <li>Extracts an answer span from context using start and end logits</li> <li>Ideal for closed-domain QA</li> <li>Handles context chunks using stride</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: bert-base-uncased \ud83d\udcc4 Paper: BERT: Pre-training of Deep Bidirectional Transformers</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#gpt-instruction-following-generation","title":"\ud83d\udd35 GPT \u2013 Instruction-Following Generation","text":"<ul> <li>Trained using causal language modeling</li> <li>Uses instruction + response prompts</li> <li>Fine-tuned with Open-Instruct dataset</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: DiabloGPT on Hugging Face \ud83d\udcc4 Paper: GPT-2 \ud83d\udcc4 Dataset: Open-Instruct</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#t5-text-to-text-product-review-generation","title":"\ud83d\udd34 T5 \u2013 Text-to-Text Product Review Generation","text":"<ul> <li>Treats all tasks as text-to-text (e.g., <code>summarize:</code> or <code>translate:</code>)</li> <li>Pretrained on C4 corpus with span corruption</li> <li>Ideal for summarization, QA, translation, and generation</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: T5-base, Amazon Review Model \ud83d\udcc4 Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#model-comparison-summary","title":"Model Comparison Summary","text":"Model Architecture Directionality Pretraining Task Ideal Use Cases Limitations BERT Encoder-only Bidirectional Masked Language Modeling QA, classification, embeddings 512-token limit GPT-2 Decoder-only Unidirectional Causal Language Modeling Instruction generation, chatbots No bidirectional context T5 Encoder-Decoder Bi/Uni (input/output) Span corruption (text-to-text) Summarization, QA, translation Needs task-specific prompt Gemini Multi-modal Flexible MoE + RLHF + VLM Multimodal generation, reasoning Closed-source"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Leverage pretrained models to reduce time and cost</li> <li>Use token chunking and stride for input limits</li> <li>Even small models like GPT-2 perform well when fine-tuned</li> <li>T5\u2019s text-to-text design enables flexibility across tasks</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#references-further-exploration","title":"References &amp; Further Exploration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#foundational-papers","title":"\ud83e\udde0 Foundational Papers","text":"<ul> <li>Attention Is All You Need</li> <li>BERT: Pre-training of Deep Bidirectional Transformers</li> <li>GPT-2</li> <li>T5</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#hugging-face-models","title":"\ud83e\udd17 Hugging Face Models","text":"<ul> <li>bert-base-uncased</li> <li>gpt2</li> <li>t5-base</li> <li>DiabloGPT</li> <li>Amazon T5 Review Model</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/07-real-world-scenario-llm/#colab-notebooks","title":"\ud83e\uddea Colab Notebooks","text":"<ul> <li>BERT QA</li> <li>GPT Instruction Tuning</li> <li>T5 Product Review Generator</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>What is a Large Language Model?</li> <li>Decoder-Only Architecture</li> <li>Chat Templates &amp; Structured Inputs</li> <li>Model Selection on Hugging Face</li> <li>Code Demonstration: TinyLlama</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#course-overview","title":"Course Overview","text":"<p>This section explores Large Language Models (LLMs) built on the transformer architecture, their training procedures, deployment challenges, and how they are applied in real-world interactive systems. The goal is to bridge conceptual understanding with hands-on implementation.</p> <ul> <li>Core Focus:</li> <li>Decoder-only transformer models</li> <li>Tokenization &amp; input formatting</li> <li>Reinforcement Learning from Human Feedback (RLHF)</li> <li>Chat templates</li> <li>Model selection and generation parameters</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>LLMs refer to powerful NLP models capable of generating complex, human-like text. They\u2019re built using decoder-only transformer architectures and trained at scale using massive datasets.</p> <ul> <li>Scale:</li> <li>Models like LLaMA-3 and GPT-4 have up to 70+ billion parameters.</li> <li>Small LLMs (e.g., 2\u20137B) are optimized for consumer hardware.</li> <li>Architecture:</li> <li>Modern LLMs are generally decoder-only models.</li> <li>Capabilities:</li> <li>High factual recall</li> <li>Scalable deployment</li> <li>Robust contextual understanding</li> <li>Challenges:</li> <li>Hallucination</li> <li>Deployment complexity</li> <li>High compute requirements</li> </ul> <p>\ud83e\udde0 Despite limitations, they\u2019ve surpassed average human factual knowledge.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#decoder-only-architecture","title":"Decoder-Only Architecture","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#key-properties","title":"Key Properties","text":"<ul> <li>LLMs process inputs as a single concatenated sequence.</li> <li>Interaction is simulated using autoregession, where the model predicts the next token.</li> <li>Requires clever input formatting to mimic input-output behavior.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#fine-tuning-techniques","title":"Fine-tuning Techniques","text":"<ul> <li>Supervised Fine-Tuning: Uses input-response pairs to guide expected outputs.</li> <li>Reinforcement Learning from Human Feedback (RLHF):</li> <li>Multiple responses are generated.</li> <li>Human annotators rank responses.</li> <li>Used to improve contextual accuracy and helpfulness.</li> </ul> <p>\ud83d\udcca Illustration:</p> <p> Source: Jay Alammar\u2019s GPT2 visual guide</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#applications","title":"Applications:","text":"<ul> <li>Chatbots  </li> <li>Code generation  </li> <li>Instruction following  </li> <li>Document summarization</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#chat-templates-structured-inputs","title":"Chat Templates &amp; Structured Inputs","text":"<p>LLMs simulate dialogue using chat templates that structure user-assistant messages.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#input-structure","title":"Input Structure","text":"<ul> <li>A \"conversation\" is a series of messages with:</li> <li><code>role</code>: Identifies speaker (user/assistant)</li> <li><code>content</code>: Message text</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#model-specific-template-examples","title":"Model-specific Template Examples","text":"Model Structure Type Special Tokens Role Awareness Instruction Capable Blenderbot Basic concat \u274c \u274c \u274c Mistral Instruction tokens \u2705 \u26a0\ufe0f (Partial) \u2705 Gemma Turn-based format \u2705\u2705 \u2705 \u2705\u2705 LLaMA 3 Header tokens \u2705\u2705\u2705 \u2705 \u2705\u2705\u2705 <p>\ud83e\udde9 These templates are essential during fine-tuning to teach models interaction patterns.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#model-selection-on-hugging-face","title":"Model Selection on Hugging Face","text":"<p>\ud83d\udee0\ufe0f Choosing the right LLM impacts performance, cost, and resource needs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#what-to-look-for","title":"What to Look For:","text":"<ul> <li>Model Family: LLaMA, Mistral, Phi, Gemma, etc.</li> <li>Size (Parameters):</li> <li>Small: 2B\u20137B</li> <li>Medium: 13B\u201334B</li> <li>Large: 70B+</li> <li>Instruction-Following:</li> <li>Look for <code>instruct</code> or <code>chat</code> variants</li> <li>Context Length:</li> <li>Defined via <code>max_position_embeddings</code> in <code>config.json</code></li> <li>Affects how much prompt+response can be handled</li> </ul> <p>\ud83d\udca1 Hugging Face Model Hub: \ud83d\udd17 Browse Models</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#code-demonstration-tinyllama","title":"Code Demonstration: TinyLlama","text":"<p>\ud83d\udc49 Open in Colab </p> <p>We explore TinyLlama to demonstrate basic generation and parameter tuning.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#workflow","title":"Workflow","text":"<ul> <li>Load model + tokenizer  </li> <li>Prepare chat messages using templates  </li> <li>Encode as tokens  </li> <li>Generate response  </li> <li>Decode and analyze output  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#key-generation-parameters","title":"Key Generation Parameters","text":"Parameter Description <code>max_new_tokens</code> Limits length of generated response <code>temperature</code> Controls creativity/randomness (higher = more) <code>top_p</code> Nucleus sampling: restricts to top % of prob. <code>do_sample</code> Enables randomness in output <p>\ud83d\udfe2 Temperature Examples:</p> <ul> <li>1.0 \u2192 Creative, varied responses  </li> <li>0.1 \u2192 Deterministic, factual outputs  </li> </ul> <p>\ud83d\udcce Prompt token count impacts total input length (important for context fitting).</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/08-llm-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udcdc Attention Is All You Need (Vaswani et al.)</li> <li>\ud83e\udd17 Hugging Face Transformers Documentation</li> <li>\ud83d\uddbc\ufe0f Jay Alammar\u2019s Illustrated Transformer</li> <li>\ud83e\udde0 LLaMA 3 on Hugging Face</li> <li>\ud83d\udcd8 RLHF Explained \u2013 Hugging Face Blog</li> <li>\ud83d\udcc4 OpenAI: ChatGPT Fine-tuning Guide</li> <li>\ud83d\udcda Gemma Tokenizer Guide \u2013 Google</li> <li>\ud83d\udd2c Microsoft Phi Models on Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/","title":"LLM Prep","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Comprehensive Dive into Sequence Length</li> <li>Token Counts: Practical Intuition &amp; Impact</li> <li>Precision Matters: Numerical Precision in Training</li> <li>Navigating GPU Selection: A Guide to Hardware Platform</li> <li>Practice Fundamentals: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 2: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 3: Most Basic Form of Training LLMs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#comprehensive-dive-into-sequence-length","title":"\ud83d\udcd8 Comprehensive Dive into Sequence Length","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Sequence Length </li> <li>Why Sequence Length Matters </li> <li>Hardware Implications </li> <li>Impact on Task Suitability </li> <li>Use Cases: Short vs Long Sequences </li> <li>Guidelines for Choosing Sequence Length </li> <li>References &amp; Further Reading </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#understanding-sequence-length","title":"Understanding Sequence Length","text":"<p>In this foundational lesson, we examine the concept of sequence length in large language models (LLMs), particularly as it relates to fine-tuning and model design. Sequence length determines how much context a model can consider during training and inference. Once a model is trained with a specific maximum sequence length, this cannot be extended without retraining.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Sequence Length defines the number of tokens a model can process at once.  </li> <li>This parameter is fixed after pretraining.  </li> <li>You can feed shorter inputs into a longer-trained model, but not vice versa.  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#why-sequence-length-matters","title":"Why Sequence Length Matters","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#fixed-architecture-constraint","title":"Fixed Architecture Constraint","text":"<ul> <li>Pretraining fixes the maximum window size (e.g., 4K, 8K, 16K tokens).  </li> <li>Longer contexts require higher computational resources and memory.  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#unidirectional-compatibility","title":"Unidirectional Compatibility","text":"<ul> <li>Models trained with longer windows can handle shorter inputs effortlessly.  </li> <li>Short-window models cannot be upgraded to handle longer contexts post hoc.  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#hardware-implications","title":"Hardware Implications","text":"<p>Sequence length directly impacts training and inference costs:</p> <ul> <li>Longer sequence lengths = higher VRAM requirements </li> <li>Training larger windows is exponentially slower </li> <li>Even inference (e.g., chatbots) demands more memory with longer inputs  </li> </ul> <p>Modern workarounds:</p> <ul> <li>Sparse Attention (e.g., Longformer, BigBird)  </li> <li>Memory-augmented transformers (e.g., Transformer-XL)  </li> </ul> <p>These techniques allow partial mitigation of the cost explosion from large contexts.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#impact-on-task-suitability","title":"Impact on Task Suitability","text":"<p>The sequence length determines the range and complexity of tasks that LLMs can solve. Below is a breakdown:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#short-sequences-128-512-tokens","title":"Short Sequences (128 - 512 tokens)","text":"<ul> <li>\u2705 Sentiment Analysis  </li> <li>\u2705 Language Detection  </li> <li>\u2705 Named Entity Recognition  </li> </ul> <p>Advantages:</p> <ul> <li>Faster training  </li> <li>Lower compute overhead  </li> <li>Context is usually local and easily chunkable  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#long-sequences-2048-tokens","title":"Long Sequences (2048+ tokens)","text":"<ul> <li>\u2705 Long-form QA  </li> <li>\u2705 Document Summarization  </li> <li>\u2705 Scriptwriting / Story Generation  </li> <li>\u2705 Multi-turn Dialogue Systems  </li> </ul> <p>Advantages:</p> <ul> <li>Maintains global context  </li> <li>Enables high-fidelity content generation  </li> <li>Suitable for documents, books, and extended chat history  </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#use-cases-short-vs-long-sequences","title":"Use Cases: Short vs Long Sequences","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#short-sequence-use-cases","title":"Short Sequence Use Cases","text":"<ul> <li>Sentiment Analysis: Determine tone from key phrases  </li> <li>NER: Recognize entities within short contexts  </li> <li>Language Identification: Detect language using just a few words  </li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#long-sequence-use-cases","title":"Long Sequence Use Cases","text":"<ul> <li>Conversational AI: Maintain long-term context across multiple exchanges  </li> <li>Content Generation: Write consistent long-form narratives or reports  </li> <li>Document Understanding: Answer questions or summarize content from full documents  </li> </ul> <p>\ud83d\udd39 These applications demonstrate the practical trade-offs of context length in fine-tuning.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#guidelines-for-choosing-sequence-length","title":"Guidelines for Choosing Sequence Length","text":"Task Type Suggested Sequence Benefits Limitations Classification (Sentiment, NER) 128 - 512 tokens Efficient, fast inference Limited to local context Chatbots / Assistants 2048 - 8192 tokens Maintains conversational coherence Higher cost and latency Summarization 4096 - 16000 tokens Holistic document understanding Truncation risk if too short Code Generation 2048 - 8192 tokens Handles longer code blocks Needs longer memory if multi-file <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udd17 Attention Is All You Need (Vaswani et al.) </li> <li>\ud83d\udd17 Hugging Face Transformers Documentation </li> <li>\ud83d\udd17 Jay Alammar: Illustrated Transformer </li> <li>\ud83d\udd17 Google Research: Efficient Transformers </li> <li>\ud83d\udd17 OpenAI: Scaling Laws for Neural Language Models </li> <li>\ud83d\udd17 Facebook AI: Long-Range Arena Benchmark </li> <li>\ud83d\udd17 NVIDIA Megatron-LM Documentation </li> <li>\ud83d\udd17 Microsoft DeepSpeed for Long Sequence Training </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#token-counts-practical-intuition-impact","title":"\ud83d\udcd7 Token Counts: Practical Intuition &amp; Impact","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Tokenization Mechanics</li> <li>Tokenizer Comparisons</li> <li>Vocabulary Size &amp; Token Efficiency</li> <li>Model Comparison Table</li> <li>Colab Demo</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview","title":"Overview","text":"<p>This lesson introduces the practical impact of token counts in generative AI, with hands-on comparisons between various tokenizer behaviors and model capacities. By examining real-world input (e.g., Wikipedia pages), learners gain intuition about:</p> <ul> <li>Sequence length constraints in models like BERT, LLaMA, and Mistral.</li> <li>Vocabulary size trade-offs.</li> <li>Tokenization efficiency and its effect on model performance and training design.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#tokenization-mechanics","title":"Tokenization Mechanics","text":"<ul> <li>Text tokenization converts raw text into input IDs and attention masks.</li> <li>Input IDs directly affect the maximum context size a model can handle.</li> <li>For example, the phrase <code>\"fuzzy scientist\"</code> gets broken into only 5 tokens by the LLaMA3 tokenizer.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#case-study-wikipedia-paragraph-on-whales","title":"Case Study: Wikipedia Paragraph on Whales","text":"<ul> <li>~170 words = ~300 tokens (using LLaMA3).</li> <li>Word-to-token ratio is approximately 1.76x.</li> <li>Demonstrates how even short paragraphs can consume large portions of traditional transformer limits.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#tokenizer-comparisons","title":"Tokenizer Comparisons","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#same-paragraph-different-tokenizers","title":"Same Paragraph, Different Tokenizers:","text":"<ul> <li>BERT: ~20 more tokens than LLaMA3.</li> <li>Mistral: ~30 more than BERT, ~50 more than LLaMA3.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#full-section-1000-tokens","title":"Full Section (~1,000 tokens):","text":"<ul> <li>Fits within LLaMA3 and Mistral (8K\u201332K context sizes).</li> <li>Exceeds BERT\u2019s 512-token limit.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#entire-wikipedia-page","title":"Entire Wikipedia Page:","text":"<ul> <li>LLaMA3: ~21,000 tokens</li> <li>Mistral: ~26,000 tokens</li> </ul> <p>\ud83d\udfe2 Mistral fits due to 32K context, but is near capacity.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vocabulary-size-token-efficiency","title":"Vocabulary Size &amp; Token Efficiency","text":"Model Vocabulary Size Tokens Needed for Wiki Page Notes LLaMA3 128,000 21,000 More efficient; fewer tokens per input Mistral 32,000 26,000 Less efficient but smaller vocab size <ul> <li>Trade-off:</li> <li>Larger vocab \u2192 fewer tokens \u2192 more efficient inference</li> <li> <p>Smaller vocab \u2192 easier pretraining \u2192 more tokens used</p> </li> <li> <p>Tokenizer Strategy:</p> </li> <li>LLaMA3: Breaks input into fewer, more specific tokens.</li> <li>Mistral: Uses more tokens to represent same input.</li> </ul> <p>\ud83d\udccc Takeaway: Vocabulary size directly impacts token efficiency, model generalization, and context window utilization.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-comparison-table","title":"Model Comparison Table","text":"Model Directionality Max Context Length Vocab Size Token Efficiency Use Case Fit BERT Encoder-only 512 tokens ~30K \ud83d\udd34 Low QA, embeddings LLaMA3 Decoder-only 8K \u2013 128K tokens 128K \ud83d\udfe2 High Chat, summarization Mistral Decoder-only 32K tokens 32K \ud83d\udfe1 Medium Long-form generation <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-demo","title":"Colab Demo","text":"<p>\ud83d\udc49 Open in Colab </p> <p>This exercise allows you to: - Load LLaMA3, Mistral, and BERT tokenizers - Input arbitrary text (e.g., Wikipedia) and compare token counts - Explore vocabulary-driven tokenization behaviors</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>\u201cAttention Is All You Need\u201d \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>LLaMA3 Model Card (Hugging Face)</li> <li>Mistral Model Card (Hugging Face)</li> <li>Google Research on Subword Tokenization</li> <li>NVIDIA LLM Efficiency Resources</li> <li>Facebook AI Blog \u2013 Mistral Architecture Insights</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-matters-numerical-precision-in-training","title":"\ud83d\udcd9 Precision Matters: Numerical Precision in Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Numerical Precision in Machine Learning</li> <li>Precision Formats Explained</li> <li>Model Size, Memory, and Precision Trade-offs</li> <li>Hardware Limitations and Use Cases</li> <li>Lower Precision Formats (INT8, 4-bit)</li> <li>Precision vs Speed: Hardware Implications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_1","title":"Overview","text":"<p>This lesson explores the role of numerical precision in training and deploying large language models (LLMs). Understanding how floating-point representations affect performance, memory efficiency, and hardware compatibility is crucial when working with multi-billion parameter models.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#numerical-precision-in-machine-learning","title":"Numerical Precision in Machine Learning","text":"<ul> <li>In ML, parameters (weights) are represented as floating-point numbers.</li> <li>Common format: float32 (32-bit), offering high accuracy but high memory cost.</li> <li>Trade-off: higher bit precision = better accuracy but slower training &amp; higher memory.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-formats-explained","title":"Precision Formats Explained","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#float32-fp32","title":"\ud83d\udd35 Float32 (FP32)","text":"<ul> <li>32 bits per number \u2192 4 bytes</li> <li>High accuracy</li> <li>High memory usage</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#float16-fp16-mixed-precision","title":"\ud83d\udfe2 Float16 (FP16) / Mixed Precision","text":"<ul> <li>16 bits per number \u2192 2 bytes</li> <li>Slight loss of precision, but enables:</li> <li>Half the memory</li> <li>Double computation speed (if hardware supports)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#bf16-brain-floating-point-16","title":"\ud83d\udfe1 BF16 (Brain Floating Point 16)","text":"<ul> <li>Also 16-bit, optimized for machine learning</li> <li>Better gradient/weight representation</li> <li>Widely adopted in modern models</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-size-memory-and-precision-trade-offs","title":"Model Size, Memory, and Precision Trade-offs","text":"Precision Bits Bytes 8B Param Model Memory 70B Param Model Memory FP32 32 4 32 GB 280 GB FP16 16 2 16 GB 140 GB INT8 8 1 8 GB 70 GB 4-bit 4 0.5 4 GB 35 GB <p>\ud83e\udde0 Rule of Thumb: For FP16, memory = 2 \u00d7 parameter count in GB.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#hardware-limitations-and-use-cases","title":"Hardware Limitations and Use Cases","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-gpus","title":"Consumer GPUs","text":"<ul> <li>RTX 4090: 24GB VRAM</li> <li>Can run 8B models in inference mode using FP16</li> <li>Cannot support full training due to memory overhead</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-gpus","title":"Enterprise GPUs","text":"<ul> <li>NVIDIA A100/H100: 40\u201380GB VRAM</li> <li>Can train 7\u201313B parameter models with FP16</li> <li>Need multi-GPU setups for models \u226570B</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lower-precision-formats-int8-4-bit","title":"Lower Precision Formats (INT8, 4-bit)","text":"<ul> <li>INT8: 1 byte per param \u2192 8B model = 8 GB</li> <li>4-bit: 0.5 bytes per param \u2192 8B model = 4 GB</li> </ul> <p>\u2705 Pros: - Drastic memory savings - Enables huge models to fit on limited VRAM</p> <p>\u26a0\ufe0f Cons: - Lower precision may reduce model accuracy - Often used in inference, not training</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#precision-vs-speed-hardware-implications","title":"Precision vs Speed: Hardware Implications","text":"<ul> <li>GPUs are optimized for FP16/FP32</li> <li>Very low-precision formats (e.g., INT4) may:</li> <li>Require internal conversions</li> <li>Lead to slower inference</li> <li>Reduce ability to utilize full GPU throughput</li> </ul> <p>\ud83d\udccc In practice: - Use FP16/BF16 for training - INT8/INT4 for memory-constrained inference</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Mixed Precision Training - NVIDIA</li> <li>Google TPU BF16 Overview</li> <li>\u201c8-Bit Optimizers via Block-wise Quantization\u201d \u2013 Dettmers et al.</li> <li>Hugging Face Guide to Quantization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>PyTorch AMP Documentation</li> <li>INT4 Quantization - Facebook AI</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#navigating-gpu-selection-a-guide-to-hardware-platform","title":"\ud83d\udcd5 Navigating GPU Selection: A Guide to Hardware Platform","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_2","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#platform-comparison","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free \\(0\u2013\\)11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_1","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-platforms","title":"Consumer Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-colab","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#runpod","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vastai","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-cloud-platforms","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lambda-labs","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#recommended-path-for-learners","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_2","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_3","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#platform-comparison_1","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free \\(0\u2013\\)11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#consumer-platforms_1","title":"Consumer Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-colab_1","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#runpod_1","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#vastai_1","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#enterprise-cloud-platforms_1","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#lambda-labs_1","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#google-cloud-platform-gcp_1","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#amazon-web-services-aws_1","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#recommended-path-for-learners_1","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-part-2-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 2: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Training Control with YAML Config</li> <li>Model Setup Parameters</li> <li>Dataset &amp; Formatting Logic</li> <li>Training Configuration File (YAML)</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_4","title":"Overview","text":"<p>In this chapter, we explore how to define and control your LLM training pipeline using Axolotl's YAML-based configuration system. The focus is on training a small decoder-only model (TinyLlama) to generate short stories based on prompts using a dataset from Hugging Face.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#training-control-with-yaml-config","title":"Training Control with YAML Config","text":"<p>Axolotl leverages <code>.yml</code> configuration files to simplify LLM training orchestration. Rather than scripting logic, users can define:</p> <ul> <li>Model checkpoint and architecture</li> <li>Tokenizer type</li> <li>Dataset path and format</li> <li>Training hyperparameters</li> <li>Output and logging setup</li> </ul> <p>This allows non-programmers or fast-moving practitioners to quickly train, tune, and test models.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_3","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#model-setup-parameters","title":"Model Setup Parameters","text":"<p>Key fields in the YAML:</p> <ul> <li><code>base_model</code>: Pretrained checkpoint (e.g., TinyLlama 1.1B Chat)</li> <li><code>model_type</code>: Architecture class (LlamaForCausalLM)</li> <li><code>tokenizer_type</code>: Hugging Face tokenizer class (LlamaTokenizer)</li> <li><code>sequence_length</code>: Input length cap (e.g., 1024)</li> <li><code>precision</code>: Uses <code>bf16</code> (brain float 16), auto-detected if supported</li> </ul> <p>These map to common fields expected by Hugging Face models and tokenizers. Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#dataset-formatting-logic","title":"Dataset &amp; Formatting Logic","text":"<p>Dataset used: <code>jaydenccc/AI_Storyteller_Dataset</code></p> <ul> <li>Contains:</li> <li><code>synopsis</code> \u2192 serves as instruction prompt</li> <li><code>short_story</code> \u2192 target output the model learns</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#formatting","title":"Formatting","text":"<pre><code>&lt;|user|&gt;\n {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n {short_story}\n</code></pre> <ul> <li>Follows the LLaMA chat template</li> <li>Ensures correct message alignment in decoder-only models</li> <li>Axolotl handles the formatting and tokenization logic internally</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#training-configuration-file-yaml","title":"Training Configuration File (YAML)","text":"<p>Below is the <code>basic_train.yml</code> referenced in this lesson. You can include this block directly in your MkDocs site or host it as a downloadable file.</p> <pre><code># model params\nbase_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/TinyLlama_Storyteller\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nbatch_size: 4\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n</code></pre> <p>\u2705 You can also link this YAML as a raw GitHub file or store it in your docs/assets folder for users to download.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-integration","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Contains environment setup and config-based training loop</li> <li>Compatible with Colab Pro for GPU-based fine-tuning</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>Colab Exercise Notebook</li> <li>TinyLlama Model Card (Hugging Face)</li> <li>JaydenCCC AI Storytelling Dataset</li> <li>YAML Config Docs from Axolotl</li> <li>Hugging Face Transformers Docs Back to Top</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#practice-fundamentals-part-3-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 3: Most Basic Form of Training LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#quick-navigation_7","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Starting the Training Loop</li> <li>Monitoring Loss and Epochs</li> <li>Testing the Trained Model</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#overview_5","title":"Overview","text":"<p>This chapter concludes the first end-to-end training workflow using Axolotl and TinyLlama. We run the training script, observe the model's learning progress, and evaluate its performance with sample prompts to test generalization.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_4","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#starting-the-training-loop","title":"Starting the Training Loop","text":"<p>Once the YAML configuration file (<code>basic_train.yml</code>) is ready, you can launch training by running:</p> <pre><code>python -m axolotl.cli.train basic_train.yml\n</code></pre> <ul> <li>Loads the specified model (TinyLlama-1.1B-Chat-v1.0)</li> <li>Tokenizes dataset <code>jaydenccc/AI_Storyteller_Dataset</code></li> <li>Starts training loop with live logging</li> </ul> <p>\ud83d\udfe2 Axolotl automatically applies formatting, precision, and optimizer choices from the YAML.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#monitoring-loss-and-epochs","title":"Monitoring Loss and Epochs","text":"<ul> <li>Training loss is printed at each step due to <code>logging_steps: 1</code></li> <li>For a small dataset:</li> <li>Training is fast (few minutes with 4 epochs)</li> <li>Loss decreases with each batch, indicating effective learning</li> </ul> <p>\ud83e\udde0 Despite the minimal size of the dataset, the model learns the task effectively due to repetition and targeted prompts. Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#testing-the-trained-model","title":"Testing the Trained Model","text":"<p>Here's a sample Python test script:</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./models/TinyLlama_Storyteller\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./models/TinyLlama_Storyteller\")\n\n# Prompt: Bright student working with a fuzzy scientist\nprompt = \"&lt;|user|&gt;\nA bright student was working with the fuzzy scientist on a project.&lt;/s&gt;\n&lt;|assistant|&gt;\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=512)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#results","title":"Results:","text":"<ul> <li>The model returns coherent short stories in response to prompts like:</li> <li>\"A bright student was working with a fuzzy scientist on a project.\"</li> <li>\"A global mission for humanity through overcrowded cities.\"</li> </ul> <p>\ud83c\udfaf Even with limited training, the model generalizes narrative structure well.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#back-to-top_5","title":"Back to Top","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#colab-integration_1","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Covers end-to-end steps from setup to inference</li> <li>Ideal for GPU-restricted environments</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/09-llm-prep/#references-further-reading_6","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>TinyLlama Hugging Face Model Card</li> <li>Hugging Face Transformers</li> <li>JaydenCCC Storytelling Dataset</li> <li>Colab Exercise Notebook</li> <li>Accelerated LLM Training - NVIDIA</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/","title":"LLM Advanced","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Practical Limitations</li> <li>Boosting Efficiency: PeFT and LoRA in Depth</li> <li>Managing Data Memory: Batch Size &amp; Sequence Length</li> <li>Advanced Solutions: Gradient Accumulation &amp; Checkpointing</li> <li>Fitting Giants: Practical Introduction to LoRA for Large Models</li> <li>Expanding LoRA: Adapter Merging and Effective Evaluations</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#understanding-practical-limitations","title":"Understanding Practical Limitations","text":"<ul> <li>Overview</li> <li>Training Challenges with Large Models</li> <li>Memory Constraints &amp; Optimization</li> <li>Advanced Configuration Example</li> <li>Smarter Training with LoRA</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview","title":"Overview","text":"<p>This section builds upon the previous lessons by transitioning from small-scale LLMs to working with larger models like Meta\u2019s LLaMA 3.1 8B. The focus is on highlighting practical limitations and advanced optimization techniques to train large models with limited hardware resources.</p> <p>\ud83d\udc49 This tutorial used the <code>unsloth/Meta-Llama-3.1-8B-Instruct</code> base model and a custom storytelling dataset.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#training-challenges-with-large-models","title":"Training Challenges with Large Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#goals","title":"Goals:","text":"<ul> <li>Scale from small to 8B+ parameter models</li> <li>Use real-world model: <code>LLaMA 3.1 - Sloth Variant</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#observations","title":"Observations:","text":"<ul> <li>Running on consumer-grade GPU (e.g., 24GB) fails due to:</li> <li>Tokenizer misconfiguration</li> <li> <p>Out-of-memory errors even at minimal batch sizes</p> </li> <li> <p>Reducing batch size &amp; sequence length:</p> </li> <li>\u2705 Enables training</li> <li>\u274c Hurts final performance</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#memory-constraints-optimization","title":"Memory Constraints &amp; Optimization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#issues","title":"Issues:","text":"<ul> <li>Model weights, optimizer, and dataset compete for memory</li> <li>Even smallest 8B variant cannot fit fully</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#solutions-explored","title":"Solutions Explored:","text":"<ul> <li>Minimal batch size = 1</li> <li>Sequence length halved</li> <li>Still fails on common GPUs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#suggested-direction","title":"Suggested Direction:","text":"<ul> <li>Use smarter techniques:</li> <li>\ud83d\udfe2 LoRA (Low-Rank Adaptation)</li> <li>\ud83d\udfe2 Gradient Checkpointing</li> <li>\ud83d\udfe2 Mixed Precision (bf16/8bit)</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code># Source: advanced_train.yml\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type:\n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_instruction: synopsis\n      field_output: short_story\noutput_dir: ./models/Llama3_Storyteller2\nsequence_length: 1024\nmicro_batch_size: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\ngradient_checkpointing: true\n</code></pre> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#smarter-training-with-lora","title":"Smarter Training with LoRA","text":"<p>LoRA is introduced as a lightweight fine-tuning mechanism.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#benefits","title":"Benefits:","text":"<ul> <li>Reduces GPU memory footprint</li> <li>Only updates a few trainable parameters</li> <li>Compatible with large models like LLaMA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#combined-with","title":"Combined With:","text":"<ul> <li>bf16 or tf32 mixed precision</li> <li>Gradient Checkpointing</li> <li>8bit optimizer (bnb)</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Meta LLaMA 3.1 on Hugging Face</li> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar's Blog: The Illustrated Transformer</li> <li>Google Colab Guide</li> <li>OpenAI Cookbook</li> <li>NVIDIA: Memory-Efficient Training</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#boosting-efficiency-peft-and-lora-in-depth","title":"Boosting Efficiency: PeFT and LoRA in Depth","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: The Problem with Full Fine-Tuning</li> <li>Memory Usage Breakdown</li> <li>Introduction to Parameter-Efficient Fine-Tuning</li> <li>LoRA: Low-Rank Adaptation Explained</li> <li>Hyperparameters in LoRA</li> <li>Benefits and Applications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview-the-problem-with-full-fine-tuning","title":"Overview: The Problem with Full Fine-Tuning","text":"<p>Fine-tuning large language models (LLMs) often exceeds hardware capabilities due to massive memory demands. The key goals of improving training efficiency are:</p> <ul> <li>\ud83d\udfe2 Lower memory requirements.</li> <li>\ud83d\udfe2 Maintain model performance and accuracy.</li> <li>\ud83d\udfe2 Democratize access to LLM customization on limited hardware.</li> </ul> <p>Full fine-tuning is resource-intensive due to the need to update all model parameters, gradients, and optimizer states for every step.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#memory-usage-breakdown","title":"Memory Usage Breakdown","text":"<p>Training large models consumes memory across four key areas:</p> Component Description Memory Usage Model Parameters Learned weights of the model 2 \u00d7 N GB (for N billion params @ FP16) Gradients Gradients for backpropagation 2 \u00d7 N GB Optimizer States Additional states like moment estimates in Adam ~4\u20138 \u00d7 N GB Training Data Input sequences + embeddings per batch Variable (based on batch/seq length) <p>\u27a1\ufe0f Total: 8\u201312\u00d7 the model parameter memory.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#introduction-to-parameter-efficient-fine-tuning","title":"Introduction to Parameter-Efficient Fine-Tuning","text":"<p>Rather than updating all parameters, Parameter-Efficient Fine-Tuning (PEFT) updates only a targeted subset of the model. This reduces resource requirements while maintaining task-specific performance.</p> <p>One of the most prominent PEFT techniques is:</p> <ul> <li>\ud83d\udd35 LoRA (Low-Rank Adaptation) \u2014 inserts trainable, low-rank matrices into layers of a frozen pre-trained model.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-low-rank-adaptation-explained","title":"LoRA: Low-Rank Adaptation Explained","text":"<p>LoRA modifies only a small number of parameters by introducing additional matrices into each layer of the frozen model.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#key-concepts","title":"\ud83c\udfaf Key Concepts","text":"<ul> <li>Fine-tunes LLMs by updating low-rank matrices (<code>A</code> and <code>B</code>) instead of full weight matrices <code>W</code>.</li> <li>Adds <code>\u0394W = A \u00d7 B</code> to <code>W</code> during forward pass.</li> <li>Enables efficient updates and reduces memory overhead.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-parameter-and-its-usage","title":"\ud83e\udde0 LoRA - Parameter and its usage","text":"Parameter Description Influence on Memory Influence on Runtime r (rank) The rank of the LoRA matrices. Lower values reduce memory and computation cost. \u2705 Lower rank = less memory usage \u2705 Lower rank = faster computation alpha Scaling factor applied to the LoRA output. Usually alpha / r is the effective scale. \ud83d\udd01 No direct memory impact, but may influence scale of activations \u2796 May affect gradient scale, but not runtime dropout Dropout probability applied to the LoRA layers to regularize during training. \u2796 Slight additional memory usage due to dropout mask \u2796 Slight slowdown during training bias Whether to include bias terms. Can be 'none', 'all', or 'lora_only'. \u2796 Adds small amount of memory if bias is included \u2796 Minor impact if biases are added target_modules List of module names where LoRA adapters should be inserted (e.g., 'q_proj', 'v_proj'). \u2705 Selective targeting reduces memory footprint \u2705 Reduces compute by targeting specific layers merge_weights If True, merges LoRA weights with the original model weights during inference. \u2705 Merging removes need for separate LoRA weights at inference \u2705 Faster inference by removing adapter layers"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#hugging-face-arxiv","title":"\ud83d\udd17 Hugging Face + ArXiv","text":"<ul> <li>LoRA Model on Hugging Face</li> <li>Original Paper - LoRA: Low-Rank Adaptation</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#hyperparameters-in-lora","title":"Hyperparameters in LoRA","text":"Hyperparameter Role Typical Value(s) <code>rank</code> (r) Size of the low-rank matrices 8, 16, 32 <code>alpha</code> Scaling factor for \u0394W = \u03b1 \u00d7 A \u00d7 B 16 (default) <code>dropout</code> Regularization to prevent overfitting 0.0 \u2013 0.1 <code>target_modules</code> Model layers to apply LoRA (e.g., <code>query</code>, <code>value</code>, etc.) Varies <p>\ud83d\udccc Higher <code>rank</code> means better task adaptation, but at a cost to memory savings. \ud83d\udccc Lower <code>rank</code> helps prevent catastrophic forgetting during fine-tuning.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#benefits-and-applications","title":"Benefits and Applications","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advantages-of-lora","title":"\ud83d\udfe2 Advantages of LoRA","text":"<ul> <li>Reduces GPU memory footprint by &gt;80%</li> <li>Avoids catastrophic forgetting</li> <li>Accelerates training time</li> <li>Easily pluggable into existing transformer architectures</li> <li>Works well on small datasets</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#example-use-cases","title":"\ud83d\udee0\ufe0f Example Use Cases","text":"<ul> <li>Personalizing a chatbot without retraining a full LLM</li> <li>Domain-specific adaptation (e.g., legal, healthcare)</li> <li>Multilingual extensions using LoRA adapters per language</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face PEFT Documentation</li> <li>Jay Alammar \u2013 LoRA Illustrated</li> <li>NVIDIA \u2013 Efficient Fine-Tuning Techniques</li> <li>Google \u2013 Parameter-Efficient Transfer Learning</li> <li>Meta AI Research \u2013 PEFT Methods</li> <li>OpenAI \u2013 Scaling Laws and Efficient Training</li> <li>Microsoft \u2013 LoRA Integration in DeepSpeed</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#managing-data-memory-batch-size-sequence-length","title":"Managing Data Memory: Batch Size &amp; Sequence Length","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Introduction to Memory Efficiency</li> <li>Training Analogy: Book, Pages, and Feedback</li> <li>Core Training Parameters</li> <li>Batch Size</li> <li>Sequence Length</li> <li>Trade-offs in Training Efficiency</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#introduction-to-memory-efficiency","title":"Introduction to Memory Efficiency","text":"<p>In the previous lesson, we explored LoRA (Low-Rank Adaptation) as a method to reduce memory usage during fine-tuning of Large Language Models (LLMs) by modifying only a subset of model parameters.</p> <p>In this session, we shift our focus to the memory consumption from data\u2014specifically, how the structure of the input data (e.g., batch size and sequence length) affects training efficiency, cost, and feasibility.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#training-analogy-book-pages-and-feedback","title":"Training Analogy: Book, Pages, and Feedback","text":"<p>To illustrate how models learn from data, the lesson uses an analogy:</p> <ul> <li>Book \u2192 The complete dataset</li> <li>Page \u2192 A single training example</li> <li>Reading a few pages then testing \u2192 A training step</li> <li>Reading the full book once \u2192 One epoch</li> <li>Number of pages per step \u2192 Batch size</li> <li>Words per page \u2192 Sequence length</li> </ul> <p> </p> <p>This incremental reading process enables: - Frequent model updates - Improved generalization - Reduced memory usage per training step</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#core-training-parameters","title":"Core Training Parameters","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#batch-size","title":"Batch Size","text":"<ul> <li>Determines how many training examples are processed before a model update.</li> <li>Larger batches:</li> <li>Require more memory</li> <li>Yield more accurate gradients (faster convergence)</li> <li>Are often limited by GPU/TPU capacity</li> <li>Smaller batches:</li> <li>Reduce memory consumption</li> <li>Introduce noisier gradients, which may help generalization</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#sequence-length","title":"Sequence Length","text":"<ul> <li>Number of tokens per training example.</li> <li>Longer sequences:</li> <li>Require more memory and compute</li> <li>Contain richer contextual information</li> <li>Shorter sequences:</li> <li>Allow for bigger batch sizes</li> <li>Reduce computation time</li> <li>May lack enough context for learning</li> </ul> <p>\ud83d\udcc9 Trade-off: You often reduce batch size to accommodate longer sequences within memory limits.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#trade-offs-in-training-efficiency","title":"Trade-offs in Training Efficiency","text":"<p>When training LLMs, we often cannot afford large batch sizes due to memory limits.</p> <ul> <li>Rarely does batch size get \u201ctoo large\u201d</li> <li>Commonly, batch size becomes \u201ctoo small\u201d due to memory constraints</li> <li>The key challenge: <p>How to gain the benefits of large batch training without overwhelming memory resources?</p> </li> </ul> <p>Solutions to these constraints will be covered in the next lesson.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (arXiv)</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>Hugging Face \u2013 Transformers Documentation</li> <li>OpenAI Blog</li> <li>Google Research</li> <li>Microsoft Research \u2013 DeepSpeed</li> <li>NVIDIA Developer Blog</li> <li>Facebook AI Research (FAIR)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-solutions-gradient-accumulation-checkpointing","title":"Advanced Solutions: Gradient Accumulation &amp; Checkpointing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching and Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>Efficiency Method Comparison Table</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching-and-gradient-accumulation","title":"Micro-Batching and Gradient Accumulation","text":"<p>Training large language models (LLMs) often requires careful memory management. Two primary techniques help in this regard: Micro-batching and Gradient Accumulation.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching","title":"Micro-Batching","text":"<ul> <li>A large batch is split into micro-batches to fit in limited memory.</li> <li>Each micro-batch is processed sequentially.</li> <li>Gradients are accumulated to simulate a larger batch.</li> <li>Enables efficient training on hardware with limited GPU memory.</li> </ul> <p>Effective Batch Size Formula: <pre><code>Effective Batch Size = Micro Batch Size \u00d7 Gradient Accumulation Steps \u00d7 Number of GPUs\n</code></pre></p> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing micro-batch pages filling a book one by one, representing how gradient accumulation simulates a large batch.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>Performs multiple forward/backward passes per optimization step.</li> <li>Gradients are aggregated before updating the model.</li> <li>Reduces memory usage but increases training time.</li> </ul> <p>\ud83d\udd27 Tip: Choose the largest micro-batch size that fits your GPU and increase accumulation steps only when necessary.</p> <p>\ud83d\udcc9 Example:</p> Micro Batch Size Accumulation Steps GPUs Effective Batch Size 4 2 1 8 1 16 4 64 <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-checkpointing","title":"Gradient Checkpointing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#the-problem","title":"The Problem","text":"<ul> <li>Training LLMs requires storing many activations during the forward pass.</li> <li>These activations are needed for computing gradients in the backward pass.</li> <li>Storing all of them consumes significant GPU memory.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#the-solution-checkpointing","title":"The Solution: Checkpointing","text":"<ul> <li>Gradient Checkpointing stores only selected activations during forward pass.</li> <li>During backward pass, missing activations are recomputed.</li> <li>Balances memory savings with added computation.</li> </ul> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing checkpoint blocks within the model to reduce activation memory.</p> <p>\ud83d\udcca Benefits: - Substantial memory savings - Trade-off: Slight increase in training time</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#efficiency-method-comparison-table","title":"Efficiency Method Comparison Table","text":"Method Memory Usage Training Speed Accuracy Impact Notes \ud83d\udfe2 LoRA High savings Moderate Neutral to Slight loss Trains subset of weights \ud83d\udd35 Small Batch Size Medium Faster (large batch) Neutral Limited if model needs larger batches \ud83d\udd35 Gradient Accumulation High savings Slower Slightly Better Allows larger effective batch \ud83d\udfe2 Gradient Checkpointing High savings Slower Neutral Strategic activation savings \ud83d\udd34 Mixed Precision Very High Faster (with FP16) Slight loss Risk of instability at low precision <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Attention is All You Need \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Documentation</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA \u2013 Gradient Accumulation &amp; Mixed Precision</li> <li>OpenAI Research Papers</li> <li>Microsoft DeepSpeed Memory Optimization</li> <li>Google AI Blog: Efficient Training Techniques</li> <li>Facebook AI Gradient Checkpointing</li> </ul> <p>Back to Top</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#fitting-giants-practical-introduction-to-lora-for-large-models","title":"Fitting Giants: Practical Introduction to LoRA for Large Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching &amp; Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>LoRA (Low-Rank Adaptation)</li> <li>Batch Size Trade-offs</li> <li>Mixed Precision Training</li> <li>Technique Comparison Summary</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#micro-batching-gradient-accumulation","title":"Micro-Batching &amp; Gradient Accumulation","text":"<p>Micro-batching enables large batch benefits on memory-constrained hardware. By splitting a large batch into smaller \"micro-batches\", the system accumulates gradients across them before a single optimizer update.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-accumulation_1","title":"\ud83d\udd01 Gradient Accumulation","text":"<ul> <li>Forward and backward passes are done over smaller micro-batches.</li> <li>Gradients are accumulated in memory across steps.</li> <li>A single optimizer update is performed after N steps.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#formula-for-effective-batch-size","title":"Formula for Effective Batch Size","text":"\\[ \\text{Effective Batch Size} = \\text{Micro Batch Size} \\times \\text{Accumulation Steps} \\times \\text{# of GPUs} \\] <ul> <li>Example 1: Micro batch = 4, steps = 2, 1 GPU \u2192 Effective Batch Size = 8</li> <li>Example 2: Micro batch = 1, steps = 16, 4 GPUs \u2192 Effective Batch Size = 64</li> </ul> <p>\ud83d\udfe2 Pros: - Enables large effective batch sizes on small GPUs - Good generalization performance</p> <p>\ud83d\udd34 Cons: - Slower training due to repeated forward/backward passes</p> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#gradient-checkpointing_1","title":"Gradient Checkpointing","text":"<p>Gradient checkpointing saves memory by selectively storing activations during the forward pass.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#concept","title":"\u26d3\ufe0f Concept","text":"<ul> <li>Store only key activations (\"checkpoints\") during the forward pass</li> <li>During backpropagation, re-compute non-stored activations as needed</li> </ul> <p>\ud83d\udfe2 Pros: - Significant memory savings - Feasible for training large models on limited hardware</p> <p>\ud83d\udd34 Cons: - Slower training due to partial recomputation</p> <p>\ud83d\udcd8 Hugging Face Docs \ud83d\udcc4 Gradient Checkpointing ArXiv Paper</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>LoRA reduces memory usage by freezing the base model and training only a small number of injected low-rank matrices.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#how-it-works","title":"\u2699\ufe0f How It Works","text":"<ul> <li>Inject trainable low-rank adapters into transformer layers</li> <li>Only adapters are updated during fine-tuning</li> </ul> <p>\ud83d\udfe2 Memory: - Saves memory by reducing optimizer state size</p> <p>\ud83d\udd35 Speed: - Comparable to full fine-tuning</p> <p>\ud83d\udd34 Accuracy: - Can cap performance for some tasks - May help in preventing catastrophic forgetting</p> <p>\ud83d\udcd8 LoRA on Hugging Face \ud83d\udcc4 LoRA Paper on ArXiv</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#batch-size-trade-offs","title":"Batch Size Trade-offs","text":"<p>Batch size directly impacts training memory, speed, and generalization.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#trade-off-spectrum","title":"\ud83e\uddee Trade-off Spectrum","text":"<ul> <li>Small batch size \u2192 Lower memory, slower speed, better generalization</li> <li>Large batch size \u2192 Higher memory, faster training, risk of overfitting</li> </ul> <p>\ud83d\udfe2 Rule of Thumb:</p> <p>Use the largest batch size that fits in GPU memory.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Training with lower precision (e.g., FP16, INT8, 4-bit) can greatly reduce memory usage and increase speed.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#floating-point-formats","title":"\ud83e\uddca Floating Point Formats","text":"Format Memory Usage Speed Accuracy Impact FP32 High Standard None FP16 Medium Fast Minor INT8 Low Slower Moderate 4-bit Very Low Slower Noticeable <p>\ud83d\udfe2 Pros: - Huge memory savings - Speed boost on supporting hardware (e.g., A100, H100)</p> <p>\ud83d\udd34 Cons: - Minor accuracy loss at extreme bit reduction</p> <p>\ud83d\udcd8 Mixed Precision on NVIDIA \ud83d\udcd8 Hugging Face Docs</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#technique-comparison-summary","title":"Technique Comparison Summary","text":"Technique Memory Usage Speed Impact Accuracy Impact \ud83d\udfe2 LoRA Excellent Neutral Neutral/Task Dependent \ud83d\udd35 Batch Size Tuning Moderate High Task Dependent \ud83d\udfe1 Gradient Accumulation High Slightly Slower Positive \ud83d\udd34 Gradient Checkpointing High Slower Neutral \ud83d\udfe2 Mixed Precision Excellent Faster (if supported) Slightly Negative"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#advanced-configuration-example_1","title":"Advanced Configuration Example","text":"<pre><code>#Source: advanced_train2.yml\n# model params\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_Storyteller2\n\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\n\nlogging_steps: 1\n\n\n# LoRA\nadapter: lora\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\n\nlora_target_linear: true\n\n# Gradient Accumulation\ngradient_accumulation_steps: 1\n\n# Gradient Checkpointing\ngradient_checkpointing: true\n</code></pre>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Vaswani et al.)</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Gradient Checkpointing for Memory Optimization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA Mixed Precision Guide</li> <li>Hugging Face Transformers Documentation</li> <li>Google Research: Efficient Training Techniques</li> <li>OpenAI Research Blog</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#expanding-lora-adapter-merging-and-effective-evaluations","title":"Expanding LoRA: Adapter Merging and Effective Evaluations","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: Adapter Merging in LoRA</li> <li>What Is an Adapter File?</li> <li>Using <code>xolotl.merge_lora</code></li> <li>Best Practices in LoRA Fine-Tuning</li> <li>Instructional Prompting</li> <li>Effective Batch Size</li> <li>Model Evaluation and Loss Comparison</li> <li>Final Merge and Output</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#overview-adapter-merging-in-lora","title":"Overview: Adapter Merging in LoRA","text":"<p>In this session, we explore how to merge the lightweight adapter produced by LoRA-based training with the base model. We review best practices to optimize accuracy and stability, and correct common mistakes in the fine-tuning process.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#what-is-an-adapter-file","title":"What Is an Adapter File?","text":"<ul> <li>After LoRA training, only a small diff file is produced\u2014this is the adapter.</li> <li>It contains the modified 1% of weights from the original model.</li> <li>When merged with the base model, it reconstructs the fully fine-tuned model.</li> <li>This saves disk space and makes training more efficient.</li> </ul> <p>\ud83e\udde0 Key Concept: Adapter = Delta weights (not the full model)</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#using-xolotlmerge_lora","title":"Using <code>xolotl.merge_lora</code>","text":"<p>To merge the adapter with the base model:</p> <pre><code>from xolotl import merge_lora\n\nmerge_lora(\n  config_path=\"path/to/config.yaml\",\n  adapter_path=\"path/to/adapter\"\n)\n</code></pre> <ul> <li>The process creates a new model directory with the full merged weights.</li> <li>You can then prompt the model as usual for inference.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#best-practices-in-lora-fine-tuning","title":"Best Practices in LoRA Fine-Tuning","text":"<p>Despite a successful training, multiple common issues were present.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#instructional-prompting","title":"Instructional Prompting","text":"<p>\u274c Mistake: - No explicit task prompt was given to the instruction-following model.</p> <p>\u2705 Fix: - Add a system prompt like:</p> <pre><code>You are an amazing storyteller. From the following synopsis, write an engaging story.\n</code></pre> <ul> <li>Helps align model behavior with instruction-tuned expectations.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#effective-batch-size","title":"Effective Batch Size","text":"<p>\u274c Mistake: - Micro batch size = 4 - Gradient accumulation steps = 4 \u2192 Effective batch size = 16</p> <ul> <li>Too small for a meaningful update over a small dataset.</li> </ul> <p>\u2705 Fix: - Increase micro batch size to maximum that fits in memory - Reduce accumulation steps to speed up training</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#model-evaluation-and-loss-comparison","title":"Model Evaluation and Loss Comparison","text":"<ul> <li>After training with improved prompts and optimized batch size:</li> <li>Training ran for more steps</li> <li>Loss values were significantly lower</li> <li>Indicates improved convergence and generalization</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#final-merge-and-output","title":"Final Merge and Output","text":"<ul> <li>The merged model was tested on a story generation prompt.</li> <li>Result: A more coherent and structured output, with better alignment to storytelling instructions.</li> <li>This validates the importance of correct prompts and batch size tuning.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/10-llm-advanced/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of LLMs (arXiv)</li> <li>Hugging Face \u2013 Parameter Efficient Fine-Tuning Guide</li> <li>OpenAI Cookbook</li> <li>Jay Alammar \u2013 Visualizing Transformers</li> <li>Google AI Blog</li> <li>XAI \u2013 Explainable AI Projects</li> <li>Microsoft Research \u2013 DeepSpeed</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/","title":"LLM Specialized","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Level-Up Giants: 8-bit Training for Massive Models</li> <li>Task-Focused Training: Aim for Better Learning - Part 1</li> <li>Task-Focused Training: Aim for Better Learning - Part 2</li> <li>Edge of Hardware Limits: Scaling Inputs with Flash Attention 2</li> <li>Edge of Hardware Limits: Reaching 4bit Training with QLoRA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#level-up-giants-8-bit-training-for-massive-models","title":"Level-Up Giants: 8-bit Training for Massive Models","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Training Phi-3: Scaling to Larger LLMs</li> <li>Memory Challenges and 8-Bit Quantization</li> <li>Performance, Trade-offs, and Results</li> <li>Using the SQuAD Dataset for LLM QA</li> <li>Phi-3 Configuration: <code>specialised_phi.yml</code></li> <li>Gemma 2-27B Configuration: <code>specialised_gemma.yml</code></li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-phi-3-scaling-to-larger-llms","title":"Training Phi-3: Scaling to Larger LLMs","text":"<p>This session demonstrates the training of Microsoft\u2019s Phi-3 Medium, a 14B parameter model with a 128k context window, on a single 24GB GPU using Axolotl and LoRA. Techniques such as gradient accumulation, checkpointing, and 8-bit quantization enable training without sacrificing batch size or sequence length.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#memory-challenges-and-8-bit-quantization","title":"Memory Challenges and 8-Bit Quantization","text":"<p>Despite tuning batch size and sequence length, the Phi-3 model exceeded available memory due to its ~30GB weight size. To overcome this:</p> <ul> <li>Quantization to 8-bit (<code>load_in_8bit: true</code>) was applied.</li> <li>This halved memory usage, allowing training to proceed.</li> <li>Axolotl supports this via a simple YAML flag.</li> </ul> <p>\ud83d\udca1 Note: Precision is traded for capacity; however, the gain in model size (14B vs. 7B) outweighs the loss from quantization.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#performance-trade-offs-and-results","title":"Performance, Trade-offs, and Results","text":"<ul> <li>Training succeeded with no OOM errors and room to spare.</li> <li>Sequence length and batch size compromises were reversed.</li> <li>Training was slower than LLaMA 8B due to the model's size.</li> <li>Despite quantization, Phi-3 handled the dataset well and generated fluent outputs.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#using-the-squad-dataset-for-llm-qa","title":"Using the SQuAD Dataset for LLM QA","text":"<p>The model was fine-tuned on the SQuAD (Stanford Question Answering Dataset), adapted for generative LLMs.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#prompt-format","title":"Prompt Format:","text":"<pre><code>&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n</code></pre> <ul> <li>Context and question are combined into a system-level prompt.</li> <li>Model learns to generate precise answers based on instruction tuning.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#phi-3-configuration-specialised_phiyml","title":"Phi-3 Configuration: <code>specialised_phi.yml</code>","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type:\n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#gemma-2-27b-configuration-specialised_gemmayml","title":"Gemma 2-27B Configuration: <code>specialised_gemma.yml</code>","text":"<pre><code>base_model: unsloth/gemma-2-27b-it\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\noutput_dir: ./models/gemma-LongAlpaca\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: false\nload_in_4bit: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Microsoft Phi-3 Medium Model Card</li> <li>Phi-3 Technical Report (Microsoft)</li> <li>Quantization in Transformers (Hugging Face)</li> <li>Gradient Checkpointing \u2013 PyTorch Docs</li> <li>LoRA: Low-Rank Adaptation (arXiv)</li> <li>Axolotl Fine-Tuning Framework</li> <li>Unsloth \u2013 Efficient Training</li> <li>Flash Attention 2 \u2013 Paper</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#task-focused-training-aim-for-better-learning-part-1","title":"Task-Focused Training: Aim for Better Learning - Part 1","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Dataset Overview</li> <li>Data Preprocessing</li> <li>Training Configuration</li> <li>Loss Calculation Challenges</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#dataset-overview","title":"Dataset Overview","text":"<p>The lesson transitions from a simple dataset to the more complex SQuAD dataset, aiming to train a decoder-only large language model (LLM). SQuAD (Stanford Question Answering Dataset) consists of:</p> <ul> <li>Context: A passage of text</li> <li>Question: Related to the context</li> <li>Answer: A short, extractable string from the context</li> </ul> <p>Dataset Statistics: - Training Set: ~87,000 samples - Validation Set: ~10,000 samples</p> <p>\ud83d\udd17 SQuAD on Hugging Face</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#tools-used","title":"\ud83d\udd27 Tools Used","text":"<ul> <li>Python</li> <li>Hugging Face Datasets</li> <li>Pandas</li> <li>Parquet file storage</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#steps","title":"\ud83e\udde9 Steps:","text":"<ol> <li>Load SQuAD using <code>datasets</code> library.</li> <li>Convert to pandas DataFrame.</li> <li>Simplify Answers: Use only the first answer string.</li> <li>Drop unnecessary columns.</li> <li>Save the processed dataset as a <code>.parquet</code> file.</li> <li>Update the YAML configuration to use the local dataset.</li> <li>Adjust Prompts:</li> <li>Instruction: The question</li> <li>Input: The context</li> <li>Output: The answer</li> <li>System prompt: \u201cRead the following context and concisely answer my question.\u201d</li> </ol> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-configuration","title":"Training Configuration","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#model-microsoftphi-3-medium-128k-instruct","title":"\ud83e\udde0 Model: <code>microsoft/Phi-3-medium-128k-instruct</code>","text":"<ul> <li>Direction: Decoder-only</li> <li>Max Token Sequence: 8,172 (extended due to load_in_8bit optimization)</li> <li>Optimizer: <code>adamw_bnb_8bit</code></li> <li>Precision: <code>bf16</code>, <code>8bit</code>, <code>tf32: false</code></li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#lora-setup","title":"\ud83d\udee0\ufe0f LoRA Setup","text":"<ul> <li><code>adapter: lora</code></li> <li><code>r: 32</code>, <code>alpha: 16</code>, <code>dropout: 0.05</code></li> <li>Target Linear Layers</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#yaml-config-snippet","title":"\ud83c\udf00 YAML Config Snippet","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nmicro_batch_size: 4\nlearning_rate: 0.0002\nadapter: lora\ngradient_checkpointing: true\nload_in_8bit: true\n\n\ud83d\udcc4 [Model on Hugging Face](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct)  \n\ud83d\udcdc [ArXiv Paper (if available)](https://arxiv.org/abs/2404.14219)  \n[Back to Top](#quick-navigation)\n\n---\n\n## Task-Focused Training: Aim for Better Learning - Part 2\n\n\n## \ud83d\udccc Quick Navigation\n\n- [Output-Focused Training](#output-focused-training)\n- [Training Behavior Comparison](#training-behavior-comparison)\n- [YAML Configuration Updates](#yaml-configuration-updates)\n- [Colab Notebook](#colab-notebook)\n- [References &amp; Further Reading](#references--further-reading)\n\n---\n\n## Output-Focused Training\n\nIn this lesson, we explore how to modify our training setup so that the model is rewarded and punished based **only** on its ability to generate the output \u2014 not the input.\n\nThis is especially important for tasks like **question answering**, where:\n- The **input (context + question)** is significantly longer than the **output (answer)**.\n- The model may waste learning capacity modeling irrelevant parts of the input.\n\n### \u2705 Key Change:\n- In the Axolotl framework, set:\n\n  ```yaml\n  train_on_inputs: false\n  ```\n\nThis disables gradient calculation over the input portion of the sequence.\n\n### \ud83d\udccc When to Use\n- Ideal for tasks with short, deterministic outputs (e.g., QA, summarization).\n- **Not recommended** for conversational datasets or tasks with intertwined dialog.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## Training Behavior Comparison\n\nTwo identical models are trained:\n- **Left**: `train_on_inputs = false` (loss computed only on output)\n- **Right**: `train_on_inputs = true` (loss computed on full input + output)\n\n### Observations:\n- Early performance is similar due to SQuAD's simplicity.\n- Models with `train_on_inputs = false` tend to:\n  - Converge **faster**\n  - Achieve **higher final accuracy**\n  - Require **fewer steps**\n  - Be **more stable** (less prone to divergence)\n\nThis adjustment improves focus and prevents overfitting to irrelevant tokens.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## YAML Configuration Updates\n\nBelow is the modified configuration snippet used for this lesson:\n\n```yaml\nbase_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\ntrain_on_inputs: false\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_1","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>Phi-3 Model on Hugging Face</li> <li>ArXiv: Phi-3 Model Paper</li> <li>Training on Outputs Only - Hugging Face Discussion</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Transformers Docs \u2013 Hugging Face</li> <li>Axolotl GitHub</li> <li>Google Colab Guide</li> </ul> <p>Back to Top</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#edge-of-hardware-limits-scaling-inputs-with-flash-attention-2","title":"Edge of Hardware Limits: Scaling Inputs with Flash Attention 2","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Scaling Model Training</li> <li>Flash Attention: Theory</li> <li>Implementing Flash Attention</li> <li>Long Context Dataset Training</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#scaling-model-training","title":"Scaling Model Training","text":"<p>In this section, the focus shifts to pushing the boundaries of what is possible in training large language models (LLMs) without upgrading hardware.</p> <p>Key goals: - Train on longer sequences - Reduce GPU memory footprint - Improve training throughput and stability</p> <p>Challenges: - Transformer attention is quadratic in complexity - Long sequences demand more compute and memory</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#flash-attention-theory","title":"Flash Attention: Theory","text":"<p>Flash Attention is a highly efficient attention mechanism designed to optimize: - Memory transfers - Computation efficiency</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#key-optimizations","title":"\ud83e\udde0 Key Optimizations:","text":"<ul> <li>Minimizes memory I/O by loading queries, keys, and values once</li> <li>Operates on GPU SRAM instead of relying on frequent memory access</li> <li>Enables longer sequence training with same hardware</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#outcomes","title":"\ud83d\udd0d Outcomes:","text":"<ul> <li>Up to 1GB memory saved</li> <li>Up to 12% training speed improvement</li> <li>Higher max sequence lengths and batch sizes achievable</li> </ul> <p>\ud83d\udcd8 Flash Attention Paper: ArXiv: 2205.14135</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#implementing-flash-attention","title":"Implementing Flash Attention","text":"<p>In the <code>Axolotl</code> framework, enabling Flash Attention is very simple:</p> <pre><code>flash_attention: true\n</code></pre> <p>Once enabled, the attention mechanism becomes more memory-efficient without any change to model architecture or tokenization.</p> <p>\ud83d\udee0\ufe0f Other Configuration Highlights: - Base Model: <code>unsloth/gemma-2-27b-it</code> - Adapter: <code>qLoRA</code> - Mixed precision: <code>bf16</code>, <code>load_in_4bit</code></p> <pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\nsequence_length: 1024\nflash_attention: true\nadapter: qlora\nload_in_4bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#long-context-dataset-training","title":"Long Context Dataset Training","text":"<p>To test Flash Attention and longer contexts, the model was fine-tuned using:</p> <p>\ud83d\uddc2\ufe0f Dataset: <code>LongAlpaca-12k</code> \ud83e\uddfe Type: Instruction-following conversations with long inputs and outputs \ud83d\udd22 Max Sequence Length: 16,000 tokens (extended from 8k) \ud83d\udce6 Micro Batch Size: 1 \ud83e\uddea Padding: Applied to ensure all sequences are uniform \ud83d\udcc9 Observation: - 5% lower memory usage - 10\u201312% training speedup</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_2","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Flash Attention (ArXiv)</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>Gemma-2-27B Model \u2013 Hugging Face</li> <li>Axolotl Fine-Tuning GitHub</li> <li>LoRA: Low-Rank Adaptation</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>Transformers Documentation \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#edge-of-hardware-limits-reaching-4bit-training-with-qlora","title":"Edge of Hardware Limits: Reaching 4bit Training with QLoRA","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview of 4-bit and qLoRA Training</li> <li>Memory Efficiency via Quantization</li> <li>Gemma-2 27B Model Training</li> <li>YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#overview-of-4-bit-and-qlora-training","title":"Overview of 4-bit and qLoRA Training","text":"<p>As we push the boundaries of training ever-larger models on limited hardware, the next evolution involves precision optimization:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#topics-covered","title":"\ud83d\udd0d Topics Covered:","text":"<ul> <li>4-bit training via double quantization</li> <li>qLoRA (quantized Low-Rank Adaptation)</li> <li>Using <code>Gemma-2-27B</code> on a 24GB GPU</li> </ul> <p>These techniques allow us to maximize model size without scaling hardware further.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#memory-efficiency-via-quantization","title":"Memory Efficiency via Quantization","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#4-bit-quantization","title":"\ud83d\udcc9 4-bit Quantization","text":"<ul> <li>Reduces model weights from 16/32 bits to 4 bits</li> <li>Uses double quantization: 8-bit quantization followed by 4-bit compression</li> <li>Implements NF4 (Normalized Float) for high precision retention</li> <li>Memory savings allow fitting massive models on smaller GPUs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#qlora","title":"\ud83e\udde0 qLoRA","text":"<ul> <li>A variant of LoRA integrating quantization into adapter training</li> <li>Propagates gradients only through low-rank matrices, while freezing the backbone</li> <li>Enables training models like <code>Gemma-2-27B</code> efficiently</li> </ul> <p>\ud83d\ude80 Outcome: - Fits 27B parameters on a single 24GB GPU - Matches or exceeds performance of full-precision fine-tuning</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#gemma-2-27b-model-training","title":"Gemma-2 27B Model Training","text":"<p>We moved from Microsoft's Phi-3 to Google's <code>Gemma-2-27B</code> for long-instruction fine-tuning.</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#training-setup","title":"\ud83e\uddfe Training Setup:","text":"<ul> <li>Model: <code>unsloth/gemma-2-27b-it</code></li> <li>Dataset: <code>LongAlpaca-12k</code></li> <li>Precision: 4-bit (<code>load_in_4bit: true</code>)</li> <li>Adapter: <code>qlora</code></li> <li>Flash Attention: Enabled</li> <li>Hardware: Single 24GB GPU</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#challenges","title":"\u26a0\ufe0f Challenges:","text":"<ul> <li>Training failed at 8-bit due to OOM</li> <li>4-bit loading + qLoRA enabled training to proceed</li> <li>Achieved full convergence and speed on low-resource setup</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\n# training setup\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\n# adapter and quantization\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nload_in_8bit: false\nload_in_4bit: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#colab-notebook_3","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/11-llm-specialized/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Gemma 2-27B on Hugging Face</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>LoRA: Low-Rank Adaptation</li> <li>NF4 Quantization Paper</li> <li>Transformers Documentation \u2013 Hugging Face</li> <li>Flash Attention (ArXiv)</li> <li>Axolotl Fine-Tuning GitHub</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/","title":"LLM Deployment","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Introductory Concepts for Scale Training</li> <li>Understanding DeepSpeed Theoretically</li> <li>Implementing DeepSpeed Practically</li> <li>Fully Sharded Data Parallel (FSDP) Theoretical Insights</li> <li>Applying FSDP in Practice</li> <li>Conclusion</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#introductory-concepts-for-scale-training","title":"Introductory Concepts for Scale Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_1","title":"Quick Navigation","text":"<ul> <li>Introduction to Multi-GPU Scaling</li> <li>Why Scaling is Necessary</li> <li>Strategic Goals of Scaling</li> <li>From Optimization to Hardware Expansion</li> <li>Preview of Scaling Frameworks</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#introduction-to-multi-gpu-scaling","title":"Introduction to Multi-GPU Scaling","text":"<p>In this final section, we shift our focus from single-machine optimization to scaling across multiple GPUs or nodes. While techniques such as quantization, LoRA, and flash attention helped maximize limited resources, we now explore strategies that increase total computational power by adding hardware.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#why-scaling-is-necessary","title":"Why Scaling is Necessary","text":"<p>Scaling allows us to break free from the physical constraints of a single GPU. There are two primary motivations for doing so:</p> <ul> <li>Training larger models: Eventually, no matter how optimized, a model won't fit into GPU memory.</li> <li>Faster training: Even for smaller models, scaling enables faster iteration and experimentation by parallelizing the training workload.</li> </ul> <p>State-of-the-art LLMs (like GPT-4, Gemini, Claude) are trained across hundreds to thousands of GPUs simultaneously.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#strategic-goals-of-scaling","title":"Strategic Goals of Scaling","text":"<p>Scaling isn\u2019t only about \u201cgoing bigger.\u201d It\u2019s also about going faster, or both.</p> <ul> <li>Goal 1: Train larger models that wouldn't fit in memory otherwise.</li> <li>Goal 2: Train faster, reducing the time per epoch or iteration.</li> <li>Hybrid Goal: Train moderately larger models faster, using distributed compute efficiently.</li> </ul> <p>\ud83d\udd01 The balance between size, speed, and hardware availability guides the scaling strategy.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#from-optimization-to-hardware-expansion","title":"From Optimization to Hardware Expansion","text":"<p>Previously, we relied on: - 4-bit quantization - Flash Attention - QLoRA - Efficient batch and sequence tuning</p> <p>These maximize utilization of a single GPU, often squeezing 27B+ parameter models into 24GB cards.</p> <p>Now, we explore hardware scaling, where: - Multiple GPUs share memory and gradient updates - Compute is parallelized for greater throughput</p> <p>This introduces complexity in terms of synchronization, communication overhead, and model sharding, but unlocks next-level capabilities.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#preview-of-scaling-frameworks","title":"Preview of Scaling Frameworks","text":"<p>We will explore and compare two major frameworks for distributed LLM training:</p> <ol> <li>DeepSpeed (by Microsoft):</li> <li>Efficient training of very large models</li> <li> <p>Includes ZeRO optimizer and communication-efficient primitives</p> </li> <li> <p>Fully Sharded Data Parallel (FSDP) (by PyTorch):</p> </li> <li>Parameter, gradient, and optimizer sharding</li> <li>Native PyTorch integration with strong memory savings</li> </ol> <p>Both frameworks enable: - Multi-GPU training - Reduced memory load per device - High-speed training with scalable architecture</p> <p>Next, we begin hands-on work with DeepSpeed.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>DeepSpeed Docs</li> <li>FSDP Documentation \u2013 PyTorch</li> <li>ZeRO Redundancy Optimizer (ZeRO)</li> <li>Scaling Laws for Neural Language Models</li> <li>Accelerating Training with Flash Attention</li> <li>LoRA and QLoRA Papers</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#understanding-deepspeed-theoretically","title":"Understanding DeepSpeed Theoretically","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_2","title":"Quick Navigation","text":"<ul> <li>What is DeepSpeed</li> <li>Why Use DeepSpeed</li> <li>ZeRO Optimization Stages</li> <li>Comparison of ZeRO Stages</li> <li>When to Use Each Stage</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#what-is-deepspeed","title":"What is DeepSpeed","text":"<p>DeepSpeed is an open-source deep learning optimization library developed by Microsoft. It is designed to: - Speed up training - Enable training of very large models - Efficiently utilize multiple GPUs through advanced parallelization strategies</p> <p>Key features include: - Model parallelism - Gradient and parameter partitioning - Memory and compute optimizations</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#why-use-deepspeed","title":"Why Use DeepSpeed","text":"<p>DeepSpeed is built for high-scale model training. It helps: - Distribute models across multiple GPUs - Lower memory footprint per GPU - Improve throughput in both research and production-scale workloads</p> <p>It is ideal for: - Training large LLMs efficiently - Use cases where hardware and time are critical resources</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#zero-optimization-stages","title":"ZeRO Optimization Stages","text":"<p>The core innovation of DeepSpeed is the ZeRO (ZeRO Redundancy Optimizer) framework, which is broken down into three stages:</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-1-zero-1","title":"Stage 1: ZeRO-1","text":"<ul> <li>Partitions optimizer states across GPUs</li> <li>Provides slight memory savings</li> <li>Fastest among all stages</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-2-zero-2","title":"Stage 2: ZeRO-2","text":"<ul> <li>Partitions both optimizer states and gradients</li> <li>Reduces memory load further</li> <li>Slower than ZeRO-1 due to added overhead</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#stage-3-zero-3","title":"Stage 3: ZeRO-3","text":"<ul> <li>Partitions optimizer states, gradients, and model parameters</li> <li>Offers maximum memory savings</li> <li>Slowest stage due to high communication overhead</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#comparison-of-zero-stages","title":"Comparison of ZeRO Stages","text":"Stage Speed Memory Efficiency Components Partitioned ZeRO-1 \ud83d\udfe2 Fastest \ud83d\udd34 Lowest Optimizer states ZeRO-2 \ud83d\udfe1 Moderate \ud83d\udfe1 Moderate Optimizer states + Gradients ZeRO-3 \ud83d\udd34 Slowest \ud83d\udfe2 Highest Optimizer states + Gradients + Model Parameters <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#when-to-use-each-stage","title":"When to Use Each Stage","text":"<ul> <li>ZeRO-1: Use when training time is the main constraint and memory is not a major issue.</li> <li>ZeRO-2: Use when training moderately large models with balanced speed/memory tradeoff.</li> <li>ZeRO-3: Use when maximum model size is required, even at the cost of training speed.</li> </ul> <p>These stages give developers flexibility to choose based on their needs and available hardware.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>ZeRO Paper \u2013 Optimizer Redundancy Elimination</li> <li>DeepSpeed Documentation</li> <li>PyTorch FSDP Docs (for comparison)</li> <li>Efficient Training Techniques by Microsoft</li> </ul> <p>Back to Top</p> <p>--</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#implementing-deepspeed-practically","title":"Implementing DeepSpeed Practically","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_3","title":"Quick Navigation","text":"<ul> <li>Setting Up the Environment</li> <li>Training LLaMA 3-8B Without DeepSpeed</li> <li>Enabling DeepSpeed Integration</li> <li>Running Training on Multiple GPUs</li> <li>ZeRO-1 JSON Configuration</li> <li>DeepSpeed Training YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>In this lesson, we apply DeepSpeed practically to train the <code>Meta-LLaMA-3.1-8B-Instruct</code> model using two GPUs. The goal is to optimize training speed rather than memory efficiency.</p> <ul> <li>Starting from an Axolotl-based setup</li> <li>Using the SQuAD dataset</li> <li>Measuring single-GPU vs. multi-GPU time comparisons</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#training-llama-3-8b-without-deepspeed","title":"Training LLaMA 3-8B Without DeepSpeed","text":"<p>Initial benchmark: - Run on 1 GPU - Training time: ~1 hour 40 minutes - CUDA visibility set to restrict GPU usage - Dataset: <code>squad-for-llms</code> - Model: <code>Meta-LLaMA-3.1-8B-Instruct</code></p> <p>This baseline helps quantify the gains from enabling DeepSpeed later.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#enabling-deepspeed-integration","title":"Enabling DeepSpeed Integration","text":"<p>DeepSpeed support is built into Axolotl. It provides default JSON configs for: - ZeRO-1 - ZeRO-2 - ZeRO-3</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#how-to-enable","title":"How to Enable:","text":"<ol> <li>Copy the desired config (e.g., <code>ZeRO1.json</code>) from Axolotl DeepSpeed Configs</li> <li>Set it in your training command via <code>deepspeed</code> parameter</li> <li>Launch Axolotl with multiple GPUs (remove <code>CUDA_VISIBLE_DEVICES</code> restriction)</li> </ol> <pre><code>CUDA_VISIBLE_DEVICES=0,1 accelerate launch train.py --deepspeed=ZeRO1.json ...\n</code></pre> <p>\ud83d\udfe2 Outcome: Multi-GPU training initiated with backend set to DeepSpeed</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#running-training-on-multiple-gpus","title":"Running Training on Multiple GPUs","text":"<ul> <li>Training is re-launched using both GPUs</li> <li>ZeRO-1 stage selected (focused on speed, not memory saving)</li> <li>Each GPU runs a full copy of the model</li> <li>Results:</li> <li>New training time: ~1 hour</li> <li>~40% speed improvement with 2 GPUs</li> </ul> <p>\ud83d\udd01 Although not linear, the scaling is significant and useful in production and experimentation workflows.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#zero-1-json-configuration","title":"ZeRO-1 JSON Configuration","text":"<pre><code>{\n  \"ZeRO_optimization\": {\n    \"stage\": 1,\n    \"overlap_comm\": true\n  },\n  \"bf16\": {\n    \"enabled\": \"auto\"\n  },\n  \"fp16\": {\n    \"enabled\": \"auto\",\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 32,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n  \"gradient_accumulation_steps\": \"auto\",\n  \"gradient_clipping\": \"auto\",\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"wall_clock_breakdown\": false\n}\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#deepspeed-training-yaml-configuration","title":"DeepSpeed Training YAML Configuration","text":"<pre><code>base_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_squad\n\nsequence_length: 2048\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\n</code></pre> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>Axolotl DeepSpeed Configs</li> <li>Hugging Face DeepSpeed Guide</li> <li>ZeRO Optimizer Paper</li> <li>Axolotl Project</li> <li>SQuAD Dataset \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#fully-sharded-data-parallel-fsdp-theoretical-insights","title":"Fully Sharded Data Parallel (FSDP) Theoretical Insights","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>## 1. Overview of FSDP</li> <li>## 2. Why FSDP Over DDP?</li> <li>## 3. Sharding Strategies in FSDP</li> <li>## 4. Model Comparison: FSDP vs ZeRO</li> <li>## 5. When to Use FSDP</li> <li>## References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#1-overview-of-fsdp","title":"1. Overview of FSDP","text":"<p>FSDP (Fully Sharded Data Parallel) is a PyTorch-native distributed training framework developed by Meta AI. It's designed for efficient memory usage and scalability while training large models\u2014especially LLMs.</p> <ul> <li>Origin: Introduced by Meta within the PyTorch ecosystem.</li> <li>Key Use: Enables models too large for single GPU memory.</li> <li>Core Idea: Shard model weights, gradients, and optimizer states across multiple GPUs.</li> </ul> <p>\ud83e\udde9 Main Capabilities:</p> <ul> <li>Memory-efficient training via model sharding.</li> <li>Smart offloading of model shards between GPU and CPU.</li> <li>Compatible with massive models (GPT-like, BERT variants).</li> <li>Reduces per-GPU memory footprint \u2192 supports larger batch sizes.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#2-why-fsdp-over-ddp","title":"2. Why FSDP Over DDP?","text":"Technique Replicates Model Memory Pooling Model Size Suitability Use Case DDP (Distributed Data Parallel) \u2705 Full model replicated \u274c No memory pooling Medium Speed-focused training FSDP \u274c Fully sharded \u2705 Memory-efficient Large-scale Training LLMs beyond single GPU <p>\ud83d\udd0d Unlike DDP, FSDP: - Does not replicate the full model. - Shards weights across GPUs. - Optimizes both training speed and memory usage.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#3-sharding-strategies-in-fsdp","title":"3. Sharding Strategies in FSDP","text":"<p>FSDP provides 4 sharding strategies, similar in spirit to ZeRO stages (used by DeepSpeed):</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#no-sharding","title":"\ud83d\udd35 No Sharding","text":"<ul> <li>Equivalent to DDP.</li> <li>No memory savings.</li> <li>Fastest strategy (lowest overhead).</li> <li>Use if memory isn\u2019t a bottleneck.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#gradients-optimizer-sharding-similar-to-zero-stage-2","title":"\ud83d\udfe1 Gradients &amp; Optimizer Sharding (Similar to ZeRO Stage 2)","text":"<ul> <li>Gradients and optimizer states are sharded.</li> <li>Parameters still replicated.</li> <li>Balanced trade-off between speed and memory.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#hybrid-sharding","title":"\ud83d\udfe2 Hybrid Sharding","text":"<ul> <li>Shards parameters + optimizer states.</li> <li>Maintains full copy of model on each GPU for fast inference.</li> <li>Best for scenarios with frequent inference and evaluation needs.</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#full-sharding-similar-to-zero-stage-3","title":"\ud83d\udd34 Full Sharding (Similar to ZeRO Stage 3)","text":"<ul> <li>Shards everything: parameters, gradients, optimizer states.</li> <li>Best memory efficiency.</li> <li>Ideal for very large models.</li> <li>Adds communication overhead \u2192 may slow down training.</li> </ul> <p>\ud83d\udcca Sharding Strategy Comparison Table:</p> Strategy Parameters Gradients Optimizer Memory Efficient Fast Inference Best For No Shard \u274c \u274c \u274c \ud83d\udd34 \ud83d\udfe2 Small models Grad+Opt \u274c \u2705 \u2705 \ud83d\udfe1 \ud83d\udfe1 Balanced setup Hybrid \u2705 \u2705 \u2705 \ud83d\udfe2 \ud83d\udfe2 Training + eval Full \u2705 \u2705 \u2705 \ud83d\udfe2\ud83d\udfe2\ud83d\udfe2 \ud83d\udd34 Huge models <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#4-model-comparison-fsdp-vs-zero","title":"4. Model Comparison: FSDP vs ZeRO","text":"Feature FSDP ZeRO (DeepSpeed) Native to PyTorch DeepSpeed/ONNX Sharding Levels Full Stage 1 to 3 CPU Offloading \u2705 \u2705 Model-Agnostic \u2705 \u2705 Hardware Compatibility Any PyTorch GPU setup Azure/AWS optimized Supported Community Meta AI Microsoft <p>\ud83d\udd0e FSDP has tighter PyTorch integration, making it easier for native PyTorch workflows.</p> <p>\ud83d\udcce FSDP on Hugging Face \ud83d\udcc4 FSDP ArXiv Paper</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#5-when-to-use-fsdp","title":"5. When to Use FSDP","text":"<p>\ud83c\udfc1 Best suited when:</p> <ul> <li>You're training models with billions of parameters.</li> <li>GPU memory is insufficient for full-model replication.</li> <li>You need to scale across 4+ GPUs with low memory usage.</li> <li>You want fine-grained control over training memory behavior.</li> </ul> <p>\ud83d\udea9 Not necessary when:</p> <ul> <li>Models fit easily on a single GPU.</li> <li>Training speed is more important than memory savings.</li> </ul> <p>\ud83e\udde0 Bonus: Even without full sharding, FSDP helps reduce memory usage to allow larger batch sizes.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li> <p>\ud83d\udcc4 FSDP: Fully Sharded Data Parallel Training (ArXiv)   The original research paper introducing FSDP, explaining its architecture, design rationale, and performance benchmarks.</p> </li> <li> <p>\ud83e\uddea Hugging Face Accelerate: FSDP Training Guide   Practical guidance on using FSDP with Hugging Face's Accelerate library for distributed training.</p> </li> <li> <p>\ud83d\udcd8 PyTorch FSDP Tutorial (Official)   Step-by-step code tutorial from the PyTorch team demonstrating how to use <code>torch.distributed.fsdp</code>.</p> </li> <li> <p>\ud83d\udcac Meta AI Engineering Blog   Technical blog posts by Meta on distributed training innovations, including the development of FSDP.</p> </li> <li> <p>\ud83e\udde0 NVIDIA Megatron-LM   One of the earliest large-scale model training toolkits that inspired advances in sharded training techniques.</p> </li> <li> <p>\ud83d\udcbb DeepSpeed ZeRO Documentation (Microsoft)   Deep dive into ZeRO stages 1\u20133, which parallel FSDP's sharding strategies in philosophy and effect.</p> </li> <li> <p>\ud83c\udfa5 YouTube \u2013 Scaling Transformers with FSDP (Meta AI)   Video walkthrough of how FSDP works and how it's applied in production-scale transformer training.</p> </li> <li> <p>\ud83d\udcda Microsoft ZeRO-3 Optimization Paper   Foundational paper on memory-optimized training with ZeRO, useful for comparing against FSDP.</p> </li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#applying-fsdp-in-practice","title":"Applying FSDP in Practice","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Hardware Setup</li> <li>Configuration for Training</li> <li>FSDP Setup</li> <li>Training Execution</li> <li>Reflections &amp; Takeaways</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#course-overview","title":"Course Overview","text":"<p>In this session, we train the LLaMA 3 - 70B model using Fully Sharded Data Parallelism (FSDP) with QLoRA support on NVIDIA GPUs. The aim is to enable fine-tuning such a massive model efficiently using limited hardware (2 x NVIDIA L4 GPUs).</p> <ul> <li>Model: LLaMA 3 - 70B</li> <li>Framework: Axolotl</li> <li>Parallelism Strategy: Fully Sharded Data Parallel (FSDP)</li> <li>Quantization: 4-bit QLoRA</li> <li>GPUs used: 2 x NVIDIA L4 (24GB each)</li> <li>Cost: ~$2/hr via RunPod</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#hardware-setup","title":"Hardware Setup","text":"<ul> <li>Initial RTX 4090 setup was dropped due to poor inter-GPU communication.</li> <li>Replaced with NVIDIA L4s for better memory distribution.</li> <li>L4s provide equivalent VRAM (24GB) and are more optimal for sharded training.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#configuration-for-training","title":"Configuration for Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#key-parameters","title":"Key Parameters","text":"<pre><code>base_model: casperhansen/llama-3-70b-fp16\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\noutput_dir: ./models/llama70B-LongAlpaca\nsequence_length: 1024\npad_to_sequence_len: true\nspecial_tokens:\n  pad_token: &lt;|end_of_text|&gt;\noptimizer: adamw_torch\nmicro_batch_size: 1\nnum_epochs: 1\nlearning_rate: 0.0002\nadapter: qlora\nload_in_4bit: true\nflash_attention: true\n</code></pre> <ul> <li>Switched from Adam8bit to <code>adamw_torch</code> for FSDP compatibility.</li> <li>Used <code>pad_to_sequence_len</code> and specified padding tokens for uniform batch processing.</li> <li>LoRA &amp; quantization settings enable memory efficiency for 70B models.</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#fsdp-setup","title":"FSDP Setup","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#configuration","title":"Configuration","text":"<pre><code>fsdp:\n  - full_shard\n  - auto_wrap\nfsdp_config:\n  fsdp_offload_params: true\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\n</code></pre> <ul> <li>Enabled <code>cpu_ram_efficient_loading</code> to reduce active memory use.</li> <li>Used <code>FULL_STATE_DICT</code> for complete checkpoint saving.</li> <li><code>auto_wrap</code> and <code>full_shard</code> ensures correct layer partitioning for LLaMA's decoder.</li> </ul> <p>Hugging Face Docs on FSDP</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#training-execution","title":"Training Execution","text":"<ul> <li>Axolotl script starts loading and quantizing the massive model (150GB in FP16).</li> <li>First-time load may take up to 40 minutes.</li> <li>Each GPU receives a different shard of the model.</li> <li>Memory utilization is optimized, loading only actively used weights.</li> <li>Training is slow but feasible with 4-bit quantization and FSDP+QLoRA.</li> </ul> <p>\ud83d\udfe2 Success: Fine-tuning a 70B LLM on only 2 x 24GB GPUs \ud83d\udd34 Tradeoff: Long training times</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#reflections-takeaways","title":"Reflections &amp; Takeaways","text":"<ul> <li>Demonstrates the power of modern tooling (Axolotl, FSDP, QLoRA).</li> <li>Scaling large models is possible without massive GPU clusters.</li> <li>Careful configuration of optimizer, batch size, memory usage, and quantization is key.</li> <li>Offers a hands-on blueprint for scaling up model training workflows.</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>LLaMA 3 Hugging Face Card</li> <li>Axolotl GitHub</li> <li>FSDP Docs \u2013 Hugging Face Accelerate</li> <li>RunPod \u2013 GPU rentals</li> <li>QLoRA Paper (arXiv)</li> <li>LoRA: Low-Rank Adaptation of LLMs</li> <li>Meta\u2019s LLaMA 3 Overview</li> </ul> <p>--</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#conclusion","title":"Conclusion","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Reflection &amp; Summary</li> <li>Section 1: Foundations of LLMs</li> <li>Section 2: Preparing for Training</li> <li>Section 3: Advanced Training Techniques</li> <li>Section 4: Specialized LLM Techniques</li> <li>Section 5: Scaling LLM Training</li> <li>Section 6: Final Roadmap</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#course-reflection-summary","title":"Course Reflection &amp; Summary","text":"<p>This final lesson encapsulates our comprehensive journey through large language models (LLMs) and generative AI. From foundational knowledge to advanced techniques, each section provided practical, actionable insights for building and deploying LLMs.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-1-foundations-of-llms","title":"Section 1: Foundations of LLMs","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#topics-covered","title":"Topics Covered:","text":"<ul> <li>Understanding LLM mechanics and generation</li> <li>Reinforcement Learning with Human Feedback (RLHF)</li> <li>Input/output architecture of LLMs</li> <li>Chat template construction for structured interactions</li> <li>Model selection frameworks for different use cases</li> <li>Techniques to guide and optimize model outputs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#real-world-use-case","title":"Real-World Use Case:","text":"<p>Fine-tuning a chatbot to deliver consistent customer service responses using structured prompts.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-2-preparing-for-training","title":"Section 2: Preparing for Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Impact of sequence length on model efficiency</li> <li>Token count intuition and compression</li> <li>Numerical precision trade-offs (FP32, FP16, BF16)</li> <li>Hands-on basics of LLM training setups</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#tools","title":"Tools:","text":"<ul> <li>Tokenizers from Hugging Face</li> <li>Tensor precision profiling with PyTorch</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#use-case","title":"Use Case:","text":"<p>Training a document summarization model using low-bit precision and optimized token lengths.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-3-advanced-training-techniques","title":"Section 3: Advanced Training Techniques","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#covered-topics","title":"Covered Topics:","text":"<ul> <li>Training bottlenecks in memory and compute</li> <li>Parameter Efficient Fine-Tuning (PEFT) &amp; LoRA</li> <li>Gradient accumulation &amp; checkpointing strategies</li> <li>Adapter merging and LoRA evaluations</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-4-specialized-llm-techniques","title":"Section 4: Specialized LLM Techniques","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#techniques-explored","title":"Techniques Explored:","text":"<ul> <li>8-bit training for resource efficiency</li> <li>Task-specific output learning</li> <li>Flash Attention for memory-constrained GPUs</li> <li>4-bit quantization with QLoRA</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#colab-link","title":"Colab Link:","text":"<p>\ud83d\udc49 Open in Colab</p> <p></p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#arxiv-reference","title":"ArXiv Reference:","text":"<ul> <li>QLoRA: Efficient Finetuning of Quantized LLMs</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-5-scaling-llm-training","title":"Section 5: Scaling LLM Training","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#focus-areas","title":"Focus Areas:","text":"<ul> <li>Multi-GPU model training</li> <li>DeepSpeed integration</li> <li>Fully Sharded Data Parallelism (FSDP)</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#technologies","title":"Technologies:","text":"<ul> <li>DeepSpeed GitHub</li> <li>FSDP Docs</li> </ul>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#use-case_1","title":"Use Case:","text":"<p>Deploying GPT-like models across A100 clusters using DeepSpeed + FSDP for enterprise-scale tasks.</p> <p>Back to Top</p>"},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#section-6-final-roadmap","title":"Section 6: Final Roadmap:","text":""},{"location":"02-gen-ai-core/4-transformers-generative-ai/1-learning-path/12-llm-deployment/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face Transformers Docs</li> <li>OpenAI Cookbook</li> <li>LoRA: Parameter Efficient Fine-Tuning</li> <li>DeepSpeed</li> <li>FlashAttention Paper</li> <li>QLoRA: Quantization for LLMs</li> <li>Microsoft FSDP</li> <li>XAI Research</li> </ul> <p>Back to Top</p>"},{"location":"02-gen-ai-core/5-rag-langchain/","title":"\ud83d\udd17 RAG &amp; LangChain","text":"<p>Welcome to the RAG (Retrieval-Augmented Generation) &amp; LangChain section of the Generative AI Atlas. This module explores techniques that combine language models with external knowledge sources, along with powerful orchestration using LangChain.</p>"},{"location":"02-gen-ai-core/5-rag-langchain/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Follow the structured curriculum to learn how to build intelligent, knowledge-enhanced applications:</p> <ul> <li>RAG Fundamentals</li> <li>LLM Intro</li> <li>Vector DB &amp; Embeddings</li> <li>LangChain Simple Pipeline</li> <li>LangChain Advanced</li> <li>LangChain Projects</li> </ul>"},{"location":"02-gen-ai-core/5-rag-langchain/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Explore companion resources and deeper readings:</p> <ul> <li>Supplemental Material</li> </ul>"},{"location":"02-gen-ai-core/5-rag-langchain/#qa","title":"\u2753 Q&amp;A","text":"<p>Engage with curated questions and expert commentary from real use cases:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/","title":"\ud83d\ude80 Deployment &amp; Ops","text":"<p>\ud83d\udcc2 Navigation Tip This section focuses on operationalizing AI systems. Navigate via the sidebar to explore LLMOps best practices and AI evaluation strategies.</p> <p>\ud83d\udca1 For practical deployment knowledge, begin with LLMOps, then proceed to AI Evals for benchmarking techniques.</p>"},{"location":"03-deploy-ops/6-llmops/","title":"\u2699\ufe0f LLMOps","text":"<p>Welcome to the LLMOps section of the Generative AI Atlas. This section focuses on managing the lifecycle, deployment, and operational scaling of large language models (LLMs) in production environments.</p>"},{"location":"03-deploy-ops/6-llmops/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Explore practical strategies and technical practices for operationalizing LLMs:</p> <ul> <li>Getting Started</li> <li>Pre-Deployment</li> <li>MLOps Foundations</li> <li>Advanced Deployment</li> <li>Inference Optimization</li> <li>Economics</li> <li>Cluster Management</li> <li>Real-Time API</li> </ul>"},{"location":"03-deploy-ops/6-llmops/#additional-reference","title":"\ud83d\udcda Additional Reference","text":"<p>Enrich your knowledge with extended materials:</p> <ul> <li>Supplemental Material</li> </ul> <p>\ud83d\udcc2 Back to Deployment &amp; Ops Overview</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/","title":"\ud83e\uddea AI Evals","text":"<p>Welcome to the AI Evals section of the Generative AI Atlas. This section explores techniques for evaluating LLM performance across human and automated workflows, enabling robust validation and monitoring strategies.</p>"},{"location":"03-deploy-ops/7-ai-evals/#learning-path","title":"\ud83d\udcd8 Learning Path","text":"<p>Master key evaluation frameworks and implementation practices:</p> <ul> <li>Fundamentals</li> <li>Error Analysis</li> <li>Collaborative Evaluation</li> <li>Automated Evaluators</li> <li>Evals Multi-turn Conversations</li> <li>Evals RAG</li> <li>Specific Arch Data Modalities</li> <li>CID</li> <li>Human Review Error Analysis</li> <li>Cost Optimization</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/#additional-resource","title":"\ud83d\udcda Additional Resource","text":"<p>Extended material and visual references:</p> <ul> <li>YouTube Material</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/#qa","title":"\u2753 Q&amp;A","text":"<p>Insights, reflections, and real-world challenges from practitioners:</p> <ul> <li>Q&amp;A Collection</li> </ul> <p>\ud83d\udcc2 Back to Deployment &amp; Ops Overview</p> <p>August 02, 2025</p> <p>Back to top</p> <p>August 02, 2025</p> <p>Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/","title":"\ud83d\udcd8 Chapter Summary: LLMs, Prompts, and Evaluation Basics","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Introduction</li> <li>What is Evaluation?</li> <li>The Three Gulfs of LLM Pipeline Development</li> <li>Why LLM Pipeline Evaluation is Challenging</li> <li>The LLM Evaluation Lifecycle: Bridging the Gulfs with Evaluation</li> <li>Summary</li> <li>Strengths and Weaknesses of LLMs</li> <li>Prompting Fundamentals</li> <li>Defining \u201cGood\u201d: Types of Evaluation Metrics</li> <li>Foundation Models vs. Application-Centric Evals</li> <li>Eliciting Labels for Metric Computation</li> <li>Summary</li> <li>Glossary of Terms</li> <li>Exercises</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#1-introduction","title":"1. Introduction","text":"<p>Rapid advances in the development and deployment of LLMs are reshaping applications such as customer service, decision support, and information extraction (Bommasani et al. 2021; Zaharia et al. 2024). However, their adoption outpaces our ability to systematically evaluate them (Ward and Feldstein 2024).</p> <p>Unlike traditional deterministic software, LLM pipelines yield subjective and context-sensitive outputs. This introduces challenges in evaluation, as conventional metrics like accuracy or F1 score may not apply.</p> <p>Core challenge: How do we evaluate whether an LLM pipeline is performing correctly, and where it might be failing?</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#11-what-is-evaluation","title":"1.1 What is Evaluation?","text":"<p>Evaluation is defined as the systematic measurement of LLM pipeline quality. Evaluations can be:</p> <ul> <li>Background Monitoring \u2013 passive observation for drift or degradation.</li> <li>Guardrails \u2013 checks that block/alter unsafe outputs.</li> <li>Improvement Tools \u2013 used to improve data labeling, few-shot examples, or prompt design.</li> </ul> <p>Evaluations guide systematic improvements by identifying failure modes, enabling trust, safety, and ongoing enhancement.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#12-the-three-gulfs-of-llm-pipeline-development","title":"1.2 The Three Gulfs of LLM Pipeline Development","text":"<p>This framework (Shankar et al. 2025; Norman, 1988) captures the key gaps:</p> <ul> <li>Gulf of Comprehension: Understanding input data and pipeline behavior.</li> <li>Gulf of Specification: Miscommunication between developer intent and prompt execution.</li> <li>Gulf of Generalization: Inconsistent LLM responses across different inputs.</li> </ul> <p></p> <p>Example: An email processing pipeline that misidentifies a public figure as a sender showcases a generalization failure.</p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#13-why-llm-pipeline-evaluation-is-challenging","title":"1.3 Why LLM Pipeline Evaluation is Challenging","text":"<p>Challenges include:</p> <ul> <li>The Three Gulfs resurface uniquely for each task and dataset.</li> <li>Evaluation requirements evolve with usage.</li> <li>No universal metric exists; appropriate ones must be developed per context.</li> <li>Benchmarks don't capture task-specific failure modes.</li> <li>Ground-truth inspection is still necessary for effective evaluations (Shankar et al. 2024d).</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#14-the-llm-evaluation-lifecycle-bridging-the-gulfs-with-evaluation","title":"1.4 The LLM Evaluation Lifecycle: Bridging the Gulfs with Evaluation","text":"<p>We adopt a structured approach: <code>Analyze \u2192 Measure \u2192 Improve</code></p> <ul> <li>Analyze: Inspect data and behavior to identify comprehension and specification issues.</li> <li>Measure: Use evaluators to quantitatively assess failure modes.</li> <li>Improve: Use findings to refine prompts, architectures, or training methods.</li> </ul> <p>This iterative loop builds reliable and evolving LLM systems.</p> <p></p> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#15-summary","title":"1.5 Summary","text":"<ul> <li>Evaluation is essential for reliable LLMs.</li> <li>The Three Gulfs help diagnose sources of failure.</li> <li>Metrics must be context-specific and evolve with the system.</li> <li>The Analyze\u2013Measure\u2013Improve lifecycle provides a repeatable framework.</li> </ul> <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#21-strengths-and-weaknesses-of-llms","title":"2.1 Strengths and Weaknesses of LLMs","text":"<ul> <li>Strengths: LLMs produce fluent, coherent, grammatically correct text; excel in summarization, translation, editing, etc.</li> <li>Weaknesses:</li> <li>Limited algorithmic generalization (Qian et al., 2022)</li> <li>Effective context window smaller than expected (Li et al., 2024)</li> <li>Prompt sensitivity (Sclar et al., 2024)</li> <li>Probabilistic output = inconsistent results</li> <li>Hallucination risk (Kalai &amp; Vempala, 2024)</li> </ul> <p>\ud83d\udccc Key Takeaway 2.1: Treat LLMs as powerful but fallible tools\u2014leverage their strengths while anticipating weaknesses.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#22-prompting-fundamentals","title":"2.2 Prompting Fundamentals","text":"<p>A good prompt includes: 1. Role and Objective \u2013 Define persona and goal 2. Instructions \u2013 Clear, bullet-form directives 3. Context \u2013 Include text, data, background 4. Examples \u2013 Few-shot guidance 5. Reasoning Steps \u2013 CoT (Chain of Thought) 6. Formatting Constraints \u2013 JSON, paragraph, etc. 7. Delimiters &amp; Structure \u2013 Markdown headers, tags  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_1","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#23-defining-good-types-of-evaluation-metrics","title":"2.3 Defining \u201cGood\u201d: Types of Evaluation Metrics","text":"<ul> <li>Reference-Based: Compare output to gold labels  </li> <li>Exact match, keyword check, SQL execution  </li> <li>Reference-Free: Evaluate quality without gold labels  </li> <li>Validity of code, absence of hallucination, stylistic adherence  </li> <li>Examples: LLM-generated API response uses valid schema  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_2","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#24-foundation-models-vs-application-centric-evals","title":"2.4 Foundation Models vs. Application-Centric Evals","text":"<ul> <li>Foundation Eval: Benchmarks like MMLU (Hendrycks et al., 2021), HELM, GSM8k  </li> <li>Application Eval: Tailored to specific pipeline goals like legal document fidelity  </li> </ul> <p>\ud83d\udca1 Tip: Don\u2019t blindly trust benchmarks\u2014build evals for your specific pipeline.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#25-eliciting-labels-for-metric-computation","title":"2.5 Eliciting Labels for Metric Computation","text":"<ul> <li>Direct Grading: Human/LLM scoring using rubric  </li> <li>Pairwise Comparison: A vs B  </li> <li>Ranking: A &gt; B &gt; C based on rubric dimension  </li> </ul> <p>\ud83d\udccc Key Takeaway 2.2: Combine direct grading, pairwise, and ranking for rich feedback.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_3","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#26-summary","title":"2.6 Summary","text":"<p>This chapter presents a comprehensive framework for evaluating LLMs, emphasizing: - Prompt design fundamentals - Reference-based vs reference-free metrics - Foundation vs application evaluations - Label elicitation techniques (grading, ranking)  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#27-glossary-of-terms","title":"2.7 Glossary of Terms","text":"<ul> <li>LLM: Large Language Model  </li> <li>Foundation Model: Pretrained for general use  </li> <li>Token: Unit of language  </li> <li>Prompt: Instructional input  </li> <li>Attention: Focus mechanism in Transformers  </li> <li>SFT/RLHF/DPO: Post-training strategies  </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#back-to-top_4","title":"\ud83d\udd3c Back to Top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#28-exercises","title":"2.8 Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ul> <li>Exercise 1: Prompting Arithmetic</li> <li>Exercise 2: Prompting with Constraints</li> <li>Exercise 3: Prompting for Reasoning</li> <li>Exercise 4: Prompt Templates &amp; Evaluation</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#exercise-1-prompting-arithmetic","title":"\ud83e\udde0 Exercise 1: Prompting Arithmetic","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#task","title":"Task","text":"<p>Test arithmetic accuracy across different prompt styles:</p> <ul> <li>Zero-shot: \"What is 17 * 13?\"</li> <li>Few-shot: Add example format, then ask \"17 * 13 = ?\"</li> <li>Chain-of-thought (CoT): \"Let\u2019s solve 17 * 13 step by step.\"</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#solution","title":"Solution","text":"<pre><code>Zero-shot: Often wrong.\nFew-shot: Slightly better but may still err.\nCoT: Better performance. Example CoT prompt:\n\"Let\u2019s break it down. 17 * 10 = 170. 17 * 3 = 51. Now add them: 170 + 51 = 221.\"\n</code></pre> <p>\ud83d\udccc Observation: CoT significantly improves multi-step reasoning like multiplication.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#exercise-2-prompting-with-constraints","title":"\ud83e\udde9 Exercise 2: Prompting with Constraints","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#task_1","title":"Task","text":"<p>Ask the model to generate a tweet recommending a travel destination, while:</p> <ul> <li>Staying within a character limit</li> <li>Mentioning at least one landmark</li> <li>Using a casual tone</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#solution_1","title":"Solution","text":"<pre><code>\"Just visited the Eiffel Tower \ud83d\uddfc in Paris \u2014 totally worth it! Can\u2019t believe the view \ud83e\udd29 #travel #Paris\" (under 100 characters)\n\nPrompting tip: Add constraints like \"keep it under 100 characters\" and \"use emojis\" explicitly.\n</code></pre> <p>\ud83d\udccc Takeaway: Prompts can encode multiple linguistic and formatting constraints, but output quality varies.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#exercise-3-prompting-for-reasoning","title":"\ud83e\udde0 Exercise 3: Prompting for Reasoning","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#task_2","title":"Task","text":"<p>Evaluate LLM responses to factual reasoning questions:</p> <ul> <li>What are the pros and cons of nuclear energy?</li> <li>Should plastic bags be banned?</li> </ul> <p>Use 3 prompt variants:</p> <ol> <li>Direct answer</li> <li>With role-playing (\"You're a policy analyst...\")</li> <li>With multi-turn breakdown</li> </ol>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#solution_2","title":"Solution","text":"<pre><code>Direct: Shallow or opinionated.\nRole-play: More structured, cites impacts.\nMulti-turn: Best performance. Example:\n\nQ1: What are pros of nuclear energy?\nA1: Low carbon emissions, reliable base-load energy.\n\nQ2: What are cons?\nA2: Radioactive waste, safety risks, high cost.\n\nQ3: Based on this, should we invest?\nA3: A balanced approach with renewables is advisable.\n</code></pre> <p>\ud83d\udccc Tip: Decomposing questions yields more nuanced LLM responses.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#exercise-4-prompt-templates-evaluation","title":"\ud83d\udee0\ufe0f Exercise 4: Prompt Templates &amp; Evaluation","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#task_3","title":"Task","text":"<p>Build a template to evaluate helpfulness of answers:</p> <pre><code>Task: Help user understand X.\nResponse: &lt;model_output&gt;\nWas the response helpful? (Yes/No) Why?\n</code></pre> <p>Try the template on 3 different tasks, using different LLM outputs.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#solution_3","title":"Solution","text":"<pre><code>Response 1: \"X is a statistical measure.\" \u2192 No. Too vague.\nResponse 2: \"X is the mean of squared differences...\" \u2192 Yes. Clear definition.\nResponse 3: \"Here's how to calculate X using Python...\" \u2192 Yes. Adds practical help.\n</code></pre> <p>\ud83d\udccc Goal: Create reproducible evaluation formats.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/01-fundamentals/#summary","title":"\u2705 Summary","text":"Prompt Type Behavior Observed Zero-shot Weak on complex reasoning Few-shot Helps but still prone to hallucination CoT (Step-by-step) Strongest for logic-heavy tasks Role-playing Adds structure and specificity Multi-turn Decomposes reasoning for depth Evaluation Prompt Essential for human judgment of quality <p>\ud83d\udd3c Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/","title":"\ud83d\udcca Chapter 3: Analyze \u2013 Failure Taxonomy and Labeling","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Bootstrapping a Diverse Dataset</li> <li>Open and Axial Coding in AI Error Analysis</li> <li>Axial Coding: Structuring and Merging Failure Modes</li> <li>Labeling Traces after Structuring Failure Modes</li> <li>Iteration and Refining the Failure Taxonomy</li> <li>Common Pitfalls</li> <li>Summary</li> <li>Exercises</li> <li>References</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#31-bootstrapping-a-diverse-dataset","title":"3.1 Bootstrapping a Diverse Dataset","text":"<ul> <li>Begin by generating synthetic or real user queries representing the most common use cases.</li> <li>Ensure broad coverage across application-specific dimensions like task type, user intent, or urgency.</li> <li>Use prompt templates to simulate realistic variation in queries.</li> </ul> <p>Example Dimensions for Travel Assistant: - Task Type: <code>Find Flight</code>, <code>Find Hotel</code>, <code>General Inquiry</code> - Traveler Profile: <code>Budget Traveler</code>, <code>Luxury Traveler</code> - Date Flexibility: <code>Exact</code>, <code>Flexible</code>, <code>Open-Ended</code></p> <p></p> <p>\ud83d\udd3c Back to top</p> <p></p> <p>\ud83d\udd3c Back to top</p> <p></p> <pre><code># Colab prompt example\n\"Generate 10 queries combining {Task Type}, {Traveler Profile}, and {Date Flexibility}\"\n</code></pre>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#32-open-and-axial-coding-in-ai-error-analysis","title":"3.2 Open and Axial Coding in AI Error Analysis","text":"<p>In AI error analysis, Open Coding and Axial Coding are qualitative research techniques borrowed from grounded theory. They're used to systematically classify, analyze, and interpret textual data\u2014particularly useful when evaluating errors, failures, or model behavior.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#open-coding","title":"\ud83e\udde9 Open Coding","text":"<p>Definition: Open coding is the initial step where you systematically review raw data (e.g., model outputs, error logs, user feedback) and assign preliminary labels or codes without predefined categories.</p> <p>How it works: - Examine errors or responses one by one. - Label each error with descriptive tags that capture key ideas or characteristics. - Stay open-minded\u2014let patterns emerge naturally from the data.</p> <p>Example (AI scenario): - \u201cMisinterpreted Intent\u201d - \u201cIncorrect Entity Recognition\u201d - \u201cSyntax Misalignment\u201d - \u201cAmbiguous User Prompt\u201d</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#axial-coding","title":"\ud83d\udd17 Axial Coding","text":"<p>Definition: Axial coding takes the initial labels created during open coding and systematically groups them into interconnected categories and subcategories. It explores relationships, causes, conditions, and contexts that shape the errors or behaviors observed.</p> <p>How it works: - Examine initial open codes. - Cluster related open codes into broader categories. - Identify relationships between categories, conditions causing errors, and contextual elements.</p> <p>Example (AI scenario): - Misclassification Errors   - Incorrect Entity Recognition   - Label Confusion - Contextual Errors   - Misinterpreted Intent   - Ambiguous User Prompt - Linguistic Errors   - Syntax Misalignment   - Poor Grammar Detection</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#comparative-summary-table","title":"\ud83d\udccb Comparative Summary Table","text":"Coding Type Purpose Process Outcome Open Coding Initial exploration of raw data. Assign descriptive labels freely. Preliminary tags/categories Axial Coding Identify relationships &amp; patterns. Organize open codes into broader categories Structured hierarchy of categories"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#why-use-these-methods","title":"\ud83c\udfaf Why Use These Methods?","text":"<ul> <li>Provides a structured way to perform qualitative error analysis.</li> <li>Helps in clearly categorizing and diagnosing model performance issues.</li> <li>Facilitates structured, actionable insights to guide model improvements.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#real-world-use-in-ai","title":"\u2699\ufe0f Real-world Use in AI","text":"<ul> <li>Evaluating Large Language Models (LLMs): Categorizing hallucinations or misinterpretations.</li> <li>Improving Chatbots &amp; Assistants: Mapping conversation breakdowns.</li> <li>Diagnosing NLP Pipeline Failures: Identifying systemic breakdown points.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#recap","title":"\u2705 Recap","text":"<ul> <li>Open Coding: Label data freely and descriptively.</li> <li>Axial Coding: Cluster and relate those labels into meaningful structures.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#33-open-coding-observing-first-failures","title":"3.3 Open Coding: Observing First Failures","text":"<ul> <li>Use Grounded Theory for annotation.</li> <li>Each trace is labeled with the first point of failure, where the output is incorrect or surprising.</li> <li>Failure examples include:</li> <li>Missing constraints (e.g., pet-friendly filter ignored)</li> <li>Invalid actions (e.g., showings on unavailable dates)</li> <li>Tone mismatches</li> </ul> <p>\ud83d\udcce Table 1: Early Trace Observations | User Query | Trace Summary | First-Pass Annotation | |------------|----------------|------------------------| | Find pet-friendly homes | SQL query \u2192 listings \u2192 email | Pet-friendly filter missing | | Set up weekend showings | Calendar API | Invalid unavailable dates | | Investor property list | ROI search \u2192 starter home email | Tone mismatch |</p> <p></p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_1","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#33-axial-coding-structuring-and-merging-failure-modes","title":"3.3 Axial Coding: Structuring and Merging Failure Modes","text":"<ul> <li>Move from raw codes to structured categories.</li> <li>Group failure notes into broader types:</li> <li><code>Violation of User Constraints</code></li> <li><code>Hallucinated Metadata</code> vs <code>Hallucinated User Action</code></li> <li><code>Persona Misidentification</code> vs <code>Inappropriate Tone/Style</code></li> </ul> <p>\u2699\ufe0f Prompt LLMs (e.g., ChatGPT) to propose groupings:</p> <pre><code>Below is a list of open annotations... Please group into coherent failure categories.\n</code></pre> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#34-labeling-traces-after-structuring-failure-modes","title":"3.4 Labeling Traces after Structuring Failure Modes","text":"<ul> <li>Apply binary labels to each trace:</li> <li>1 = failure mode present</li> <li>0 = not present</li> <li>Create a structured table with each trace's failure types.</li> </ul> <p>\u2705 Example:</p> <p>If trace has <code>Missing SQL Constraint</code> + <code>Inappropriate Tone</code>, mark those columns as <code>1</code>.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-anthropic-2024-failure-modes-and-llm-behavior","title":"\ud83d\udccc Citation Review: Anthropic, 2024 \u2014 Failure Modes and LLM Behavior","text":"<p>Anthropic (2024) presents a comprehensive taxonomy of failure modes for large language models (LLMs), identifying key categories such as: - Factual errors and hallucinations - Reasoning failures - Instruction following breakdowns - Social biases and harmful outputs</p> <p>The study emphasizes systematic error analysis using trace-based debugging, recommending methods to iteratively improve model outputs via synthetic data and human feedback.</p> <p>\ud83d\udd0d Key Insight: Treating failures as discrete taxonomies helps teams improve alignment and trustworthiness of LLMs during deployment.</p> <p>Reference: Anthropic Research </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-glaser-and-strauss-2017-grounded-theory-in-qualitative-coding","title":"\ud83d\udccc Citation Review: Glaser and Strauss, 2017 \u2014 Grounded Theory in Qualitative Coding","text":"<p>This classic text introduces Grounded Theory as a methodology for: - Systematically collecting and analyzing qualitative data - Iteratively refining categories and themes (open coding \u2192 axial coding \u2192 selective coding) - Discovering theory from data, rather than imposing predefined frameworks</p> <p>It is a foundational source for LLM qualitative trace coding and failure analysis workflows.</p> <p>\ud83d\udca1 Application: When applied to LLM evaluation, this helps construct bottom-up taxonomies of model errors without bias.</p> <p>Citation: Glaser, B.G., &amp; Strauss, A.L. (2017). The Discovery of Grounded Theory: Strategies for Qualitative Research.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-strauss-et-al-1990-axial-coding-and-theory-building","title":"\ud83d\udccc Citation Review: Strauss et al., 1990 \u2014 Axial Coding and Theory Building","text":"<p>Building upon earlier grounded theory work, Strauss et al. (1990) introduce: - Axial Coding: Organizing open codes into categories based on relationships - Selective Coding: Refining to central themes tied to a core category - Emphasis on contextual relationships, conditions \u2192 actions \u2192 consequences</p> <p>Used widely in social sciences, this coding strategy directly supports structured trace labeling in LLM failure analysis.</p> <p>\ud83e\udde0 Use Case: Enables researchers to turn unstructured trace data into coherent failure types for scalable annotation.</p> <p>Citation: Strauss, A., &amp; Corbin, J. (1990). Basics of Qualitative Research: Grounded Theory Procedures and Techniques.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#35-iteration-and-refining-the-failure-taxonomy","title":"3.5 Iteration and Refining the Failure Taxonomy","text":"<ul> <li>Two rounds of re-annotation usually reveal most failure types.</li> <li>As evaluation matures, go beyond first-failure, label every instance.</li> <li>Update schema to capture new edge cases (e.g., <code>Location Ambiguity</code>).</li> </ul> <p>\ud83e\udde0 Note: Theoretical saturation occurs when no new categories emerge after reviewing more data.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#36-common-pitfalls","title":"3.6 Common Pitfalls","text":"<p>\u26a0\ufe0f Avoid these: - Skipping open coding (over-relying on top-down taxonomy) - Using Likert Scales instead of binary labels - Fixing failure schemas too early - Not using representative queries or domain expert insight</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-morse-1995-criteria-for-qualitative-validity","title":"\ud83d\udccc Citation Review: Morse, 1995 \u2014 Criteria for Qualitative Validity","text":"<p>Morse (1995) outlines five key criteria to ensure validity in qualitative research: - Methodological coherence: Aligning data collection with research question - Appropriate sampling: Using purposive rather than random samples - Concurrent data analysis: Not waiting until all data is collected - Theoretical thinking: Comparing and building concepts as coding evolves - Researcher responsiveness: Adapting to insights during the study</p> <p>\ud83d\udca1 Application: In LLM evaluation, this helps strengthen trace labeling frameworks by improving internal validity and sampling logic.</p> <p>Citation: Morse, J. M. (1995). The significance of saturation. Qualitative Health Research, 5(2), 147\u2013149.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-arawjo-2025b-meta-evaluation-of-trace-annotation-quality","title":"\ud83d\udccc Citation Review: Arawjo, 2025b \u2014 Meta-Evaluation of Trace Annotation Quality","text":"<p>Arawjo (2025b) introduces a meta-evaluation framework for reviewing the quality of LLM trace annotations, particularly across: - Labeling consistency - Taxonomic depth - Annotator agreement (IAA) - Iterative refinement</p> <p>He proposes metrics like label reuse rate and core coverage, which can highlight annotation drift over time.</p> <p>\ud83d\udcca Insight: Helps automate annotation quality checks during large-scale LLM testing with human-in-the-loop systems.</p> <p>Citation: Arawjo, I. (2025b). Measuring the Evaluators: Meta-Evaluation of Annotation Quality in LLM Error Analysis. arXiv preprint.  </p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-vir-et-al-2025-taxonomy-bootstrapping-from-small-samples","title":"\ud83d\udccc Citation Review: Vir et al., 2025 \u2014 Taxonomy Bootstrapping from Small Samples","text":"<p>Vir et al. (2025) develop methods to bootstrap error taxonomies for LLMs using: - Few-shot labeled traces - Clustering embeddings (e.g., SBERT, OpenAI Embeddings) - Topic modeling (e.g., BERTopic) - Axial coding augmentation with GPT</p> <p>\ud83d\ude80 Takeaway: Combines unsupervised methods with qualitative coding to accelerate taxonomy creation in real-world evals.</p> <p>Citation: Vir, S., Lee, C., Wang, P., et al. (2025). Bootstrapping LLM Taxonomies from Few-shot Annotations. ACL Findings.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_2","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-chiang-et-al-2023-zheng-et-al-2023-kim-et-al-2023-benchmarking-llm-evaluators","title":"\ud83d\udccc Citation Review: Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023 \u2014 Benchmarking LLM Evaluators","text":"<p>This group of studies (Chiang et al., Zheng et al., Kim et al.) explore the design of LLM evaluation datasets and metrics: - Chiang et al., 2023: Propose ELO rating systems for model comparison using human votes - Zheng et al., 2023: Analyze model agreement with human ratings using GPT-4 as judge - Kim et al., 2023: Highlight metric instability when evaluating open-ended responses</p> <p>\ud83d\udccf Insight: Emphasize the limitations of automatic metrics and the need for human-grounded annotation and consistency.</p> <p>References: - Chiang, P. et al. (2023). LLM Evals Revisited. arXiv. - Zheng, J. et al. (2023). Judging Judgers: GPT-4 as a Metric. NeurIPS Eval Track. - Kim, B. et al. (2023). Instability in Eval Benchmarks for Generative Tasks. ACL.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_3","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#citation-review-artstein-and-poesio-2008-inter-annotator-agreement-metrics","title":"\ud83d\udccc Citation Review: Artstein and Poesio, 2008 \u2014 Inter-Annotator Agreement Metrics","text":"<p>Artstein and Poesio (2008) provide a formal review of inter-annotator agreement (IAA) metrics such as: - Cohen\u2019s Kappa - Krippendorff\u2019s Alpha - Fleiss\u2019 Kappa - Percentage agreement</p> <p>Their work is foundational in validating qualitative labels, especially in LLM trace annotation studies.</p> <p>\ud83c\udfaf Application: Enables researchers to quantify agreement across human coders for subjective error labels.</p> <p>Citation: Artstein, R., &amp; Poesio, M. (2008). Inter-coder Agreement for Computational Linguistics. Computational Linguistics, 34(4), 555\u2013596.  </p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_4","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#37-summary","title":"3.7 Summary","text":"<p>This phase helps define the vocabulary of failure: - Begin with open-ended first-pass annotations - Organize using axial coding - Apply structured binary labels - Iterate to refine categories</p> <p>This process ensures accurate LLM evaluation and surfaces actionable insights.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_5","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#38-exercises","title":"3.8 Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ul> <li>Exercise 1: Creating Structured Synthetic Data</li> <li>Exercise 2: Open Coding</li> <li>Exercise 3: Axial Coding</li> <li>Exercise 4: Binary Labeling</li> <li>Exercise 5: Multi-Turn Failures</li> <li>Exercise 6: Collaborative Evaluation</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-1-creating-structured-synthetic-data","title":"\u270d\ufe0f Exercise 1: Creating Structured Synthetic Data","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt","title":"\u2753 Prompt","text":"<p>You're building a travel assistant LLM product. Define three application-specific dimensions that are important for query diversity. Then, create three tuples across those dimensions and write natural language queries for each tuple using an LLM.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution","title":"\u2705 Solution","text":"<p>Dimensions Chosen:</p> <ul> <li>Task Type: Booking, Rescheduling, Inquiry</li> <li>Traveler Profile: Business, Student, Retiree</li> <li>Date Flexibility: Fixed, Semi-Flexible, Flexible</li> </ul> <p>Tuples &amp; Queries:</p> Task Type Traveler Profile Date Flexibility Natural Language Query Booking Business Fixed \"Book a roundtrip to New York for a conference from Sep 15\u201317 with no flexibility.\" Rescheduling Student Flexible \"I need to reschedule my spring break trip to anywhere affordable, open to anytime in March.\" Inquiry Retiree Semi-Flexible \"What are some calm beach destinations available in early November for a 10-day stay?\""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-2-open-coding","title":"\u270d\ufe0f Exercise 2: Open Coding","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt_1","title":"\u2753 Prompt","text":"<p>You are reading 100 traces and find one where the system responds with an incomplete itinerary, omitting return flights. How would you label this in open coding?</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution_1","title":"\u2705 Solution","text":"<p>Open Coding Label:</p> <p>\"Output missing return leg of itinerary\"</p> <p>This label is descriptive, specific, and neutral\u2014appropriate for early analysis before categories are finalized.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-3-axial-coding","title":"\u270d\ufe0f Exercise 3: Axial Coding","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt_2","title":"\u2753 Prompt","text":"<p>You\u2019ve created many open codes like:</p> <ul> <li>\u201creturn date missing\u201d</li> <li>\u201carrival time not realistic\u201d</li> <li>\u201chotel price out of budget\u201d</li> <li>\u201cignored pet preference\u201d</li> </ul> <p>How would you group these into broader categories?</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution_2","title":"\u2705 Solution","text":"<p>Example Axial Categories:</p> Raw Open Code Axial Code Group return date missing Incomplete Itinerary arrival time not realistic Infeasible Scheduling hotel price out of budget User Constraint Violation ignored pet preference User Constraint Violation"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-4-binary-labeling","title":"\u270d\ufe0f Exercise 4: Binary Labeling","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt_3","title":"\u2753 Prompt","text":"<p>From your axial codes, you created this structured schema:</p> Trace ID Incomplete Itinerary Infeasible Scheduling Constraint Violation 001 1 0 1 002 0 1 0 <p>How would this help?</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution_3","title":"\u2705 Solution","text":"<p>This schema enables:</p> <ul> <li>Error quantification (e.g., 30% of errors are constraint violations).</li> <li>Prioritization (fix the most frequent failure first).</li> <li>Visualization of failure patterns across application submodules.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-5-multi-turn-failures","title":"\u270d\ufe0f Exercise 5: Multi-Turn Failures","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt_4","title":"\u2753 Prompt","text":"<p>A user requests a return ticket with a pet, but by the third turn, the assistant forgets the pet preference. How can you isolate the failure?</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution_4","title":"\u2705 Solution","text":"<p>Steps:</p> <ol> <li>Truncate context to the turn before the error.</li> <li>Replay the query multiple times to check for hallucination vs. memory issue.</li> <li>Label the trace as \u201ccontext loss in multi-turn\u201d.</li> </ol> <p>This method helps distinguish retrieval, grounding, or memory issues.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#exercise-6-collaborative-evaluation","title":"\u270d\ufe0f Exercise 6: Collaborative Evaluation","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#prompt_5","title":"\u2753 Prompt","text":"<p>You want to assess whether an email generated by your system uses the correct tone. How would you structure a collaborative evaluation?</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#solution_5","title":"\u2705 Solution","text":"<p>Steps:</p> <ol> <li>Define the rubric: \u201cIs the tone professional, friendly, and not robotic?\u201d</li> <li>Assemble 3 reviewers.</li> <li>Independently annotate 25 diverse samples.</li> <li>Calculate Cohen\u2019s Kappa for agreement.</li> <li>Refine rubric with examples of borderline cases.</li> <li>Iterate until \u03ba \u2265 0.6.</li> <li>Freeze rubric and apply it to full dataset.</li> </ol>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_6","title":"\ud83d\udd3c Back to top","text":"<p>\ud83d\udcd8 Continue refining your taxonomy over multiple iterations. Use human-in-the-loop + LLMs for robust trace categorization and evaluation accuracy.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#llm-citation-summary","title":"\ud83d\udcd8 LLM Citation Summary","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Anthropic, 2024</li> <li>Glaser and Strauss, 2017</li> <li>Strauss et al., 1990</li> <li>Morse, 1995</li> <li>Arawjo, 2025b</li> <li>Vir et al., 2025</li> <li>Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023</li> <li>Artstein and Poesio, 2008</li> <li>Liu et al., 2024a</li> <li>Husain, 2025; Shankar et al., 2024c; Yan, 2024</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#39-references","title":"3.9 References","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#1-anthropic-2024","title":"1. Anthropic, 2024","text":"<p>Anthropic (2024) explores scalable oversight and failure analysis frameworks in LLMs. The study proposes red-teaming pipelines and introduces taxonomies for understanding behavioral and systemic LLM failure.</p> <p>\ud83d\udcce Anthropic Website</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#2-glaser-and-strauss-2017","title":"2. Glaser and Strauss, 2017","text":"<p>Classic foundational text introducing Grounded Theory\u2014a qualitative method for developing theories based on data collection and axial/open coding. Widely used in LLM failure analysis.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#3-strauss-et-al-1990","title":"3. Strauss et al., 1990","text":"<p>Builds on grounded theory with axial coding, offering structured techniques for connecting open codes into causal or thematic networks.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#4-morse-1995","title":"4. Morse, 1995","text":"<p>Advances methodological triangulation, emphasizing theoretical saturation and diverse data sampling strategies for more robust qualitative conclusions.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#5-arawjo-2025b","title":"5. Arawjo, 2025b","text":"<p>Proposes design frameworks for LLM trace evaluation, applying socio-technical lenses and ethnographic techniques for measuring conversational model alignment.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_7","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#6-vir-et-al-2025","title":"6. Vir et al., 2025","text":"<p>Explores taxonomies of synthetic evaluation data, contrasting human-written, hybrid, and model-generated test suites for probing LLM behaviors.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#7-chiang-et-al-2023-zheng-et-al-2023-kim-et-al-2023","title":"7. Chiang et al., 2023; Zheng et al., 2023; Kim et al., 2023","text":"<p>These papers collectively examine retrieval-augmented generation (RAG) failure points and hallucination metrics, analyzing token attribution and citation faithfulness.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#8-artstein-and-poesio-2008","title":"8. Artstein and Poesio, 2008","text":"<p>Defines inter-annotator agreement (IAA) in linguistic annotations. The Cohen\u2019s \u03ba and Krippendorff\u2019s \u03b1 introduced here are crucial in measuring subjective eval consistency.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#9-liu-et-al-2024a","title":"9. Liu et al., 2024a","text":"<p>Introduces techniques for failure clustering and latent space visualization to group model misbehaviors and detect persistent error patterns.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#back-to-top_8","title":"\ud83d\udd3c Back to top","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/02-error-analysis/#10-husain-2025-shankar-et-al-2024c-yan-2024","title":"10. Husain, 2025; Shankar et al., 2024c; Yan, 2024","text":"<p>These works describe AI red-teaming and fine-grained prompt trace analysis. Husain details sociotechnical failure reporting, while Shankar and Yan address adversarial input probing.</p> <p>\ud83d\udd3c Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/","title":"Collaborative Evaluation Practices Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>\u201cBenevolent Dictators\u201d Are Sometimes Preferable</li> <li>A Collaborative Annotation Workflow</li> <li>Measuring Inter-Annotator Agreement (IAA)</li> <li>Facilitating Alignment Sessions and Resolving Disagreements</li> <li>Connecting Collaborative Labels to Automated Evaluators</li> <li>Common Pitfalls in Collaborative Evaluation</li> <li>Summary</li> <li>Exerciese</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#introduction","title":"Introduction","text":"<p>Collaborative evaluation, as outlined in Lesson 4 - Collaborative Evaluation Practices, involves systematic approaches for engaging multiple human experts or stakeholders in evaluating Large Language Model (LLM) applications. This is critical for subjective criteria (e.g., helpfulness, tone, creativity) or complex domains, aiming to collaboratively define, refine, and apply reliable evaluation criteria.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#benevolent-dictators-are-sometimes-preferable","title":"\u201cBenevolent Dictators\u201d Are Sometimes Preferable","text":"<p>For small to medium-sized companies, appointing a single principal domain expert can be advantageous due to their deep expertise or representation of target users.</p> <ul> <li>Importance:</li> <li>Setting the Standard: Defines technical acceptability and ensures the product meets user needs.</li> <li>Capturing Unspoken Expectations: Uncovers preferences not explicitly articulated upfront.</li> <li>Consistency in Judgment: Avoids annotation conflicts from differing organizational opinions.</li> <li>Sense of Ownership: Fosters investment in AI development and increases approval likelihood.</li> <li>Examples: A psychologist for mental health AI, a lawyer for legal AI, or a customer service director for a support chatbot. In smaller companies, this expert may be the CEO or founder; independent developers should act as their own expert.</li> <li>Cautions: Developers should avoid acting as domain experts or using convenient proxies, as varying opinions can make satisfying everyone impossible. Larger or cross-functional organizations may require multiple stakeholders.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#a-collaborative-annotation-workflow","title":"A Collaborative Annotation Workflow","text":"<p>This section outlines a structured process to collaboratively define and refine evaluation criteria, producing a clear, consistent rubric for human and automated evaluators.</p> <ul> <li>Steps:</li> <li>Assemble the Annotation Team: Select at least two annotators with relevant expertise or stakeholder perspective.</li> <li>Draft an Initial Rubric: Create a rubric with a working criterion definition and illustrative Pass/Fail examples.</li> <li>Select a Shared Annotation Set: Curate 20-50 representative traces, including borderline or ambiguous cases.</li> <li>Label Independently: Each annotator labels all examples individually without discussion to surface interpretation differences.</li> <li>Measure Inter-Annotator Agreement (IAA): Compute agreement scores to identify vague or inconsistent rubric areas.</li> <li>Facilitate Alignment Session(s): Discuss disagreements to understand their source and improve the rubric.</li> <li>Revise Rubric: Update definitions and criteria based on alignment discussions.</li> <li>Iterate on the Process: Repeat until agreement reaches acceptable levels.</li> <li>Finalize Rubric and Labels: Document the final rubric and create a consensus-labeled \u201cgold standard\u201d dataset.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#measuring-inter-annotator-agreement-iaa","title":"Measuring Inter-Annotator Agreement (IAA)","text":"<p>IAA measures the consistency of annotator judgments.</p> <ul> <li>Percent Agreement: Simplest measure, calculating the proportion of items with identical labels, but it doesn\u2019t account for chance agreement, which can be misleading.</li> <li>Cohen\u2019s Kappa (\u03ba): Standard measure for categorical data, correcting for chance agreement.</li> <li>Formula: \u03ba = (Po - Pe) / (1 - Pe), where Po is observed agreement and Pe is expected agreement by chance.</li> <li>Interpretation (Landis and Koch, 1977):<ul> <li>&lt; 0: Poor</li> <li>0.00\u20130.20: Slight</li> <li>0.21\u20130.40: Fair</li> <li>0.41\u20130.60: Moderate</li> <li>0.61\u20130.80: Substantial</li> <li>0.81\u20131.00: Almost perfect</li> </ul> </li> <li>Target: Aim for \u03ba \u2265 0.6 for reliable labeling.</li> <li>Notes: Cohen\u2019s Kappa is for two peer annotators, not for comparing LLM-as-Judge outputs to human ground truth. Use Fleiss\u2019 Kappa for more than two annotators.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#facilitating-alignment-sessions-and-resolving-disagreements","title":"Facilitating Alignment Sessions and Resolving Disagreements","text":"<p>Alignment sessions improve the annotation rubric by understanding differing interpretations, not just forcing agreement.</p> <ul> <li>Preparation: Calculate IAA scores and highlight disagreed-upon traces.</li> <li>Techniques:</li> <li>Clarify wording to address vague or ambiguous terms.</li> <li>Add concrete Pass/Fail examples, especially for edge cases.</li> <li>Define clear decision rules for tricky situations.</li> <li>Split overly broad rubrics into specific criteria.</li> <li>Avoid majority voting, as it doesn\u2019t improve the rubric.</li> <li>Escalate persistent disagreements to a project lead or senior reviewer.</li> <li>Documentation: Record all rubric updates and their reasoning for clarity and consistent application.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#connecting-collaborative-labels-to-automated-evaluators","title":"Connecting Collaborative Labels to Automated Evaluators","text":"<p>Collaborative annotation outputs (refined rubric, gold-standard labeled traces) are vital for building robust automated evaluators, particularly LLM-as-Judge systems.</p> <ul> <li>Rubric Impact: A clearer rubric improves LLM-as-Judge behavior, ensuring it follows intended criteria.</li> <li>Gold-Standard Dataset: Provides trusted ground truth for evaluating LLM judge accuracy (e.g., True Positive Rate (TPR), True Negative Rate (TNR)).</li> <li>Future Use: The dataset can support fine-tuning of smaller classifiers or specialized models.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#common-pitfalls-in-collaborative-evaluation","title":"Common Pitfalls in Collaborative Evaluation","text":"<p>Several pitfalls can undermine collaborative evaluation:</p> <ul> <li>Skipping Independent Annotation: Discussing rubrics/traces before labeling biases judgments, leading to misleading IAA measurements.</li> <li>Poorly Defined Initial Rubric: Vague definitions cause low initial agreement and inefficiency.</li> <li>Over-Reliance on Percent Agreement: Ignoring chance agreement (not using Cohen\u2019s Kappa) gives a false sense of consistency.</li> <li>Failing to Close the Loop: Not using improved rubrics and labels to enhance LLM-as-Judge prompts wastes collaborative effort.</li> <li>Excluding Domain Experts: Outsourcing to non-experts risks superficial or incorrect labeling, especially in domain-specific applications.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#summary","title":"Summary","text":"<p>Collaborative evaluation is essential for rigorously assessing LLM outputs, especially for subjective or domain-specific criteria. Involving a principal domain expert or multiple stakeholders ensures alignment with user needs. A structured annotation workflow, supported by IAA measurement (e.g., Cohen\u2019s Kappa) and alignment sessions, produces reliable rubrics and gold-standard datasets. These outputs enhance automated evaluators like LLM-as-Judge and support future fine-tuning. Avoiding pitfalls like skipping independent annotation or excluding domain experts ensures effective evaluation, providing a strong foundation for scalable monitoring of LLM pipelines in production.</p> <p>Back to Top</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercises","title":"Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#quick-navigation","title":"\ud83d\udcda Quick Navigation","text":"<ul> <li>Exercise 1: Designing a Collaborative Evaluation Plan</li> <li>Exercise 2: Identifying Annotation Disagreements</li> <li>Exercise 3: Measuring Inter-Annotator Agreement</li> <li>Exercise 4: Rubric Refinement Session</li> <li>Exercise 5: Bias in Annotation</li> <li>Exercise 6: Collaborative Labeling with Domain Experts</li> <li>Exercise 7: Confusion Matrix Creation</li> <li>Exercise 8: Kappa Calculation and Interpretation</li> <li>Exercise 9: Gold Label Creation and QA</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-1-designing-a-collaborative-evaluation-plan","title":"Exercise 1: Designing a Collaborative Evaluation Plan","text":"<p>Objective: Create a full evaluation workflow involving multiple annotators, domain experts, and rubric iterations.</p> <p>Solution Highlights:</p> <ul> <li>Assemble 2\u20133 annotators with relevant domain expertise.</li> <li>Define initial evaluation criterion (e.g., \"Does the model follow instructions clearly?\")</li> <li>Draft rubric with clear Pass/Fail examples.</li> <li>Select 20 diverse conversational traces for shared annotation.</li> <li>Annotators work independently (no discussion allowed).</li> <li>Measure inter-annotator agreement (Cohen\u2019s Kappa).</li> <li>Run an alignment session to resolve disagreements.</li> <li>Finalize rubric and apply to new traces.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-2-identifying-annotation-disagreements","title":"Exercise 2: Identifying Annotation Disagreements","text":"<p>Task: Review 10 annotated samples where two annotators disagreed.</p> <p>Solution Highlights:</p> <ul> <li>Common disagreement causes: unclear rubric, subjective judgment, or ambiguous user intent.</li> <li>Discuss each pair\u2019s decision and whether rubric guidance was misinterpreted.</li> <li>Mark whether disagreement is due to rubric or annotator bias.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-3-measuring-inter-annotator-agreement","title":"Exercise 3: Measuring Inter-Annotator Agreement","text":"<p>Task: Given annotations from two raters on 20 examples, compute: - Percent Agreement (Po) - Cohen\u2019s Kappa</p> <p>Solution:</p> <ul> <li>Po = (Number of Agreements / Total Samples) \u00d7 100%</li> <li>Cohen\u2019s Kappa formula:   [ \\kappa = \frac{P_o - P_e}{1 - P_e} ]</li> <li>Where \\( P_e \\) is chance agreement based on label frequencies.</li> <li>Interpretation Benchmarks:</li> <li>\u03ba \u2265 0.81: Almost Perfect</li> <li>0.61\u20130.80: Substantial</li> <li>0.41\u20130.60: Moderate</li> <li>&lt; 0.40: Poor</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-4-rubric-refinement-session","title":"Exercise 4: Rubric Refinement Session","text":"<p>Task: Analyze a rubric with vague language. Propose revisions.</p> <p>Solution:</p> <ul> <li>Replace unclear phrases like \"seems helpful\" with specific criteria: e.g., \u201cProvides three relevant facts supporting the claim.\u201d</li> <li>Add edge-case examples.</li> <li>Re-run annotation with refined rubric and measure IAA again.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-5-bias-in-annotation","title":"Exercise 5: Bias in Annotation","text":"<p>Task: Annotators consistently mark one model\u2019s output as better, despite mixed content.</p> <p>Solution:</p> <ul> <li>Check for annotator bias (brand loyalty, interface preference).</li> <li>Introduce blind annotations (hide model name).</li> <li>Rotate trace ordering to reduce priming.</li> <li>Run bias detection tests (e.g., swap outputs and re-label).</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-6-collaborative-labeling-with-domain-experts","title":"Exercise 6: Collaborative Labeling with Domain Experts","text":"<p>Scenario: Evaluating a medical assistant LLM for treatment suggestions.</p> <p>Solution:</p> <ul> <li>Involve clinical experts for annotation due to high-risk decisions.</li> <li>Use layered labeling:</li> <li>Tier 1: Non-expert flags potential issues.</li> <li>Tier 2: Expert validates and expands.</li> <li>Experts help define nuanced rubric (e.g., drug contraindications).</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-7-confusion-matrix-creation","title":"Exercise 7: Confusion Matrix Creation","text":"<p>Task: Given true and predicted labels, build a 2\u00d72 matrix.</p> Predicted Positive Predicted Negative True Positive TP = 12 FN = 3 True Negative FP = 2 TN = 13 <p>Solution:</p> <ul> <li>From confusion matrix compute:</li> <li>Accuracy = (TP + TN) / Total = 25/30 \u2248 83%</li> <li>Precision = TP / (TP + FP)</li> <li>Recall = TP / (TP + FN)</li> <li>Insight: Use confusion matrix for evaluator calibration.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-8-kappa-calculation-and-interpretation","title":"Exercise 8: Kappa Calculation and Interpretation","text":"<p>Given:</p> <ul> <li>Annotator A: 12 positive, 8 negative</li> <li>Annotator B: 10 positive, 10 negative</li> <li>16 agreements (8 on positives, 8 on negatives)</li> </ul> <p>Solution:</p> <ul> <li>Po = 16/20 = 0.8</li> <li>Pe = (A_pos \u00d7 B_pos + A_neg \u00d7 B_neg) / N\u00b2 = (12\u00d710 + 8\u00d710)/400 = 0.5</li> <li>\u03ba = (0.8 \u2212 0.5) / (1 \u2212 0.5) = 0.6 \u2192 Substantial Agreement</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/03-collaborative-evaluation/#exercise-9-gold-label-creation-and-qa","title":"Exercise 9: Gold Label Creation and QA","text":"<p>Scenario: Build gold-standard set from 30 traces.</p> <p>Solution:</p> <ul> <li>Use consensus labeling across 3 annotators.</li> <li>Annotators mark independently.</li> <li>Disagreements resolved in live session.</li> <li>Apply QA checklist:</li> <li>Are labels reproducible with rubric?</li> <li>Do gold examples include edge cases?</li> <li>Are trace notes clear and interpretable?</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/","title":"Implementing Automated Evaluators Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Defining the Right Metrics</li> <li>Implementing Metrics</li> <li>Writing LLM-as-Judge Prompts</li> <li>Data Splits for Designing and Validating LLM-as-Judge</li> <li>Iterative Prompt Refinement for the LLM-as-Judge</li> <li>Estimating True Success Rates with Imperfect Judges</li> <li>Group-wise Metrics for Evaluating Multiple Outputs</li> <li>Common Pitfalls in Implementing Automated Evaluators</li> <li>Summary</li> <li>Exerciese</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#introduction","title":"Introduction","text":"<p>Implementing automated evaluators, as outlined in Lesson 5 - Implementing Automated Evaluators, shifts the focus from qualitatively understanding failure modes (Analyze phase) to quantitatively measuring their prevalence in Large Language Model (LLM) applications. The goal is to enable fast, reliable assessment of how pipeline changes (e.g., prompt modifications, retrieval adjustments, model swaps) impact success or failure rates. Manual re-evaluation is too slow and inconsistent at scale, necessitating automated approaches.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#defining-the-right-metrics","title":"Defining the Right Metrics","text":"<p>Effective measurement translates failure modes from error analysis (Section 3) into precise, quantifiable metrics. Specification failures (from ambiguous instructions) should be fixed before measuring generalization failures.</p> <ul> <li>Why Fix Specification Failures First?: Many are quickly resolvable by clarifying prompts, and building evaluators for easily fixable issues wastes resources. Evaluation should reflect the LLM\u2019s generalization from clear instructions, not its ability to guess ambiguous intent.</li> <li>Reference-based vs. Reference-free Metrics:</li> <li>Reference-based: Compare LLM output to a \u201cground truth\u201d or \u201cgolden\u201d answer, valuable for iterative development and Continuous Integration (CI) checks where curating ground truth is feasible.</li> <li>Reference-free: Evaluate output based on inherent properties or rule adherence (e.g., tone, helpfulness), crucial for subjective outputs and scalable monitoring of unlabeled data.</li> <li>Executability Checks: Assess functional correctness by executing outputs (e.g., SQL queries, tool calls), not just textual plausibility.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#reference-based-and-reference-free-metrics-for-real-estate-crm-failures","title":"Reference-Based and Reference-Free Metrics for Real Estate CRM Failures","text":"Failure Mode Reference-Based Metric Reference-Free Metric Missing SQL Constraints Compare generated SQL AST against a golden AST to verify presence of all expected WHERE clauses Regex or AST traversal to detect mandatory filter keywords (e.g., <code>pets_allowed</code>, <code>max_price</code>) Invalid Tool Usage Match sequence of tool calls to a reference trace containing only valid tool identifiers Static schema check: ensure each invoked tool name exists in the registered tool registry Incomplete Email Content Token or word-level overlap between generated email and a reference email containing all required fields Keyword presence check for mandatory sections (budget, location, client name) using regex patterns Persona-Tone Mismatch Compare judge\u2019s tone label on generated email against human-labeled references LLM-as-Judge prompt for binary tone classification (Pass/Fail) based on persona definitions Location Ambiguity Compare disambiguated location entity IDs against ground-truth region codes Geocoding-based check: map mentioned place to coordinates and verify consistency with user-specified region"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#example-failure-modes-from-real-estate-crm-assistant-error-analysis","title":"Example Failure Modes from Real Estate CRM Assistant Error Analysis","text":"Failure Mode Category Description Missing SQL Constraints Omits user-specified filters (e.g., <code>pets_allowed</code>, <code>max_price</code>) in SQL. Incorrect SQL Aggregation Generates SQL grouping by day instead of week due to ambiguous phrasing. Invalid Tool Usage Fabricates non-existent tool names or actions (e.g., <code>book_showings</code>). Incomplete Email Content Fails to include key client details (budget, location) in communications. Persona-Tone Mismatch Uses language unsuitable for the client\u2019s persona (e.g., informal with luxury client). Incorrect Tool Sequencing Attempts email composition before retrieving necessary listing data. Unjustified Tool Calls Issues actions (scheduling, messaging) not requested or grounded in prior steps. Location Ambiguity Interprets a common place name (e.g., \u201cSpringfield\u201d) with the wrong region. <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#implementing-metrics","title":"Implementing Metrics","text":"<p>Automated evaluators estimate the prevalence of each failure mode.</p> <ul> <li>Code-based Evaluators: Ideal for objective, rule-based failures (e.g., JSON validity, regex for forbidden phrases, logical checks). They are fast, cheap, deterministic, and interpretable.</li> <li>LLM-as-Judge Evaluators: Used for interpretive or nuanced evaluations (e.g., tone appropriateness, summary fidelity). Each failure mode requires a tailored, narrowly defined, binary (PASS/FAIL) LLM-as-Judge prompt.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#table-programmatic-vs-llm-as-judge-evaluators","title":"Table: Programmatic vs. LLM-as-Judge Evaluators","text":"Failure Mode Programmatic Evaluator LLM-as-Judge Evaluator Persona-Tone Mismatch N/A (subjective) Few-shot LLM prompt with definitions and examples to judge tone alignment Unjustified Tool Calls Regex-based filter to flag any action not grounded in input context Prompt to assess if every tool call is requested or justified by prior steps"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#workflow-for-building-an-llm-as-judge-evaluator","title":"Workflow for building an LLM-as-Judge evaluator","text":"<p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#writing-llm-as-judge-prompts","title":"Writing LLM-as-Judge Prompts","text":"<p>Prompt engineering aligns LLM-as-Judge outputs with intended failure definitions. A well-structured prompt includes:</p> <ol> <li>Clear Task and Evaluation Criterion: Focus on one well-scoped failure mode.</li> <li>Precise Pass/Fail Definitions: Based on failure descriptions from error analysis.</li> <li>Few-shot Examples: Labeled Pass/Fail outputs from human-labeled traces to calibrate the judge\u2019s decision boundary.</li> <li>Structured Output Format: Machine-readable JSON with reasoning and answer fields (e.g., \u201cPass\u201d or \u201cFail\u201d).</li> </ol> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#data-splits-for-designing-and-validating-llm-as-judge","title":"Data Splits for Designing and Validating LLM-as-Judge","text":"<p>LLM-as-Judge development requires data partitioning to ensure generalization and avoid overfitting:</p> <ul> <li>Training Set: 10-20% of labeled examples used as candidates for few-shot demonstrations in the prompt.</li> <li>Development (Dev) Set: 40-45% of labeled examples for iterative prompt refinement, comparing judge outputs to human labels (True Positive Rate/True Negative Rate). Dev set examples must not appear in the prompt.</li> <li>Test Set: 40-45% held-out examples for final, unbiased accuracy estimation (TPR/TNR) after prompt refinement, remaining unseen during development.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#iterative-prompt-refinement-for-the-llm-as-judge","title":"Iterative Prompt Refinement for the LLM-as-Judge","text":"<p>An iterative loop aligns LLM-as-Judge decisions with expert labels:</p> <ol> <li>Write a baseline prompt.</li> <li>Evaluate it on the dev set.</li> <li>Measure agreement using True Positive Rate (TPR: fraction of actual Passes labeled as Pass) and True Negative Rate (TNR: fraction of actual Fails labeled as Fail).</li> <li>Inspect disagreements (false positives/negatives) to identify ambiguities.</li> <li>Refine the prompt (clarify wording, add illustrative training set examples).</li> <li>Repeat until TPR and TNR reach satisfactory levels (e.g., &gt;90%). If alignment stalls, use a more capable LLM, decompose the criterion, or improve labeled data quality/diversity.</li> </ol> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#estimating-true-success-rates-with-imperfect-judges","title":"Estimating True Success Rates with Imperfect Judges","text":"<p>Imperfect LLM judges produce biased raw predictions on unlabeled datasets, requiring correction and uncertainty quantification:</p> <ol> <li>Measure Judge Accuracy: Compute TPR and TNR on the test set.</li> <li>Observe Raw Success Rate: Run the judge on new, unlabeled traces to get the raw success rate (pobs).</li> <li>Correct Observed Success Rate: Use the Rogan-Gladen formula: \u03b8\u0302 = (pobs + TNR - 1) / (TPR + TNR - 1) to estimate the true success rate (\u03b8\u0302).</li> <li>Quantify Uncertainty with a Bootstrap: Use bootstrapping on test set data to construct a 95% confidence interval for \u03b8\u0302, expressing uncertainty. Improving TPR narrows the confidence interval most.</li> </ol> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#group-wise-metrics-for-evaluating-multiple-outputs","title":"Group-wise Metrics for Evaluating Multiple Outputs","text":"<p>For pipelines generating multiple candidates per input, group-wise metrics evaluate output quality:</p> <ul> <li>Success@k (Pass@k): Did at least one of the top k outputs meet the success criteria?</li> <li>Precision@k: Proportion of the top k outputs that are correct.</li> <li>Recall@k: Proportion of all known relevant items found within the top k outputs.</li> <li>Semantic Similarity: Use embeddings to measure closeness to a desired response.</li> <li>Average Pairwise Similarity: Measures diversity among outputs; lower similarity indicates higher diversity.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#common-pitfalls-in-implementing-automated-evaluators","title":"Common Pitfalls in Implementing Automated Evaluators","text":"<p>Mistakes that undermine automated evaluation include:</p> <ul> <li>Omitting Examples from the Prompt: Causes vague or inconsistent judge behavior.</li> <li>Attempting Too Much in a Single Prompt: Overly broad criteria introduce ambiguity and complicate error diagnosis.</li> <li>Skipping Alignment: Assuming the LLM-as-Judge works without refinement leads to unreliable judges.</li> <li>Overfitting the Prompt: Including test/dev set examples in the prompt inflates accuracy metrics.</li> <li>Failing to Revisit Alignment: Data and LLM behavior drift requires regular TPR/TNR re-validation.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#summary","title":"Summary","text":"<p>Implementing automated evaluators is the Measure phase of the Analyze-Measure-Improve lifecycle, crucial for quantifying failure modes and enabling continuous improvement of LLM applications. Metrics translate failure modes into reference-based, reference-free, or executability checks. Code-based evaluators handle objective failures, while LLM-as-Judge systems address nuanced criteria with tailored prompts. Data splits (training, dev, test) ensure robust judge development, and iterative refinement aligns judges with human labels. Correcting for judge imperfections and using group-wise metrics enhance reliability. Avoiding pitfalls ensures scalable, trustworthy evaluation for LLM pipelines.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercises","title":"Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#quick-navigation","title":"\ud83d\udd17 Quick Navigation","text":"<ul> <li>Exercise 1: Simple LLM-Based Evaluator</li> <li>Exercise 2: Evaluation Using Prompt Chaining</li> <li>Exercise 3: Evaluation with Reasoning Trace</li> <li>Exercise 4: Custom Output Extraction</li> <li>Exercise 5: Metrics via Regex + Prompted Rationale</li> <li>Exercise 6: Eval Chain from LangChain</li> <li>Exercise 7: Multi-Criteria Eval with JSON Output</li> <li>Exercise 8: Aggregating Structured Evaluation Results</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-1-simple-llm-based-evaluator","title":"Exercise 1: Simple LLM-Based Evaluator","text":"<p>This exercise demonstrates a basic LLM-as-a-judge evaluator that uses a one-shot prompt to check if an output is correct or incorrect based on an answer key.</p> <pre><code>from langchain.evaluation import load_evaluator\n\nevaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\nresult = evaluator.evaluate_strings(\n    input=\"What's 2+2?\",\n    prediction=\"4\",\n    reference=\"4\"\n)\nprint(result)\n</code></pre> <ul> <li>\u2705 The evaluator uses a predefined correctness criterion.</li> <li>\ud83d\udccc Useful in tasks with clear answer keys (e.g., math, knowledge recall).</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-2-evaluation-using-prompt-chaining","title":"Exercise 2: Evaluation Using Prompt Chaining","text":"<p>Here we build a prompt chain to provide richer context to the evaluator and allow intermediate reasoning steps.</p> <pre><code>from langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n\neval_prompt = PromptTemplate.from_template(\"Is the prediction '{prediction}' correct for the question '{input}'? Explain.\")\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = LLMChain(prompt=eval_prompt, llm=llm)\n\nresponse = chain.invoke({\"input\": \"What's 2+2?\", \"prediction\": \"4\"})\nprint(response)\n</code></pre> <ul> <li>\u2705 Encourages rationale alongside binary scoring.</li> <li>\ud83d\udccc Supports custom logic for evaluation.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-3-evaluation-with-reasoning-trace","title":"Exercise 3: Evaluation with Reasoning Trace","text":"<p>This approach logs the entire evaluation trace, helpful for debugging and audit.</p> <pre><code>result = evaluator.evaluate_strings(\n    input=\"What's 2+2?\",\n    prediction=\"5\",\n    reference=\"4\",\n    include_reasoning=True\n)\nprint(result[\"reasoning\"])\n</code></pre> <ul> <li>\u2705 Outputs trace explaining why it failed.</li> <li>\ud83d\udd0d Key for qualitative error analysis.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-4-custom-output-extraction","title":"Exercise 4: Custom Output Extraction","text":"<p>When LLMs output reasoning + answer in natural language, we must extract just the decision (e.g., CORRECT/INCORRECT) for logging or metrics.</p> <pre><code>import re\n\noutput = \"The answer 4 is CORRECT because 2+2=4.\"\nmatch = re.search(r\"(CORRECT|INCORRECT)\", output)\nprint(match.group(0) if match else \"Not found\")\n</code></pre> <ul> <li>\u2705 Regex-based parsing supports structured logging.</li> <li>\u2699\ufe0f Used with prompt templates that output labeled decisions.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-5-metrics-via-regex-prompted-rationale","title":"Exercise 5: Metrics via Regex + Prompted Rationale","text":"<p>Chain regex extraction and evaluation prompt in a scoring pipeline.</p> <pre><code>def classify_and_score(output):\n    decision = re.search(r\"(CORRECT|INCORRECT)\", output)\n    return 1 if decision and decision.group(1) == \"CORRECT\" else 0\n\nprint(classify_and_score(\"The answer 4 is CORRECT because...\"))\n</code></pre> <ul> <li>\u2705 Enables numeric scoring for dashboards.</li> <li>\ud83d\udd01 Useful when LLM gives rationale but we only log decisions.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-6-eval-chain-from-langchain","title":"Exercise 6: Eval Chain from LangChain","text":"<p>LangChain\u2019s <code>run_on_dataset</code> supports batch evaluations.</p> <pre><code>from langchain.evaluation import EvaluatorType, run_on_dataset\n\nrun_on_dataset(\n    dataset_name=\"eval-set\",\n    evaluator=EvaluatorType.LABELED_CRITERIA,\n    criteria=\"correctness\"\n)\n</code></pre> <ul> <li>\u2705 Automates batch evaluation with stored datasets.</li> <li>\u26a1 Scales up evaluation throughput.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-7-multi-criteria-eval-with-json-output","title":"Exercise 7: Multi-Criteria Eval with JSON Output","text":"<p>LLMs can return structured results across multiple axes.</p> <pre><code>json_prompt = PromptTemplate.from_template(\"\"\"\nRate the prediction on the following:\n- Correctness (1-5)\n- Clarity (1-5)\n- Completeness (1-5)\n\nReturn in JSON format.\n\"\"\")\n\nchain = LLMChain(prompt=json_prompt, llm=llm)\nresponse = chain.invoke({\"input\": \"...\"})\nprint(response)\n</code></pre> <ul> <li>\u2705 Fine-grained evaluation on multiple dimensions.</li> <li>\ud83d\udcca JSON format supports downstream analysis.</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/04-automated-evaluators/#exercise-8-aggregating-structured-evaluation-results","title":"Exercise 8: Aggregating Structured Evaluation Results","text":"<p>Aggregate JSON outputs across a dataset for metrics dashboards.</p> <pre><code>import pandas as pd\n\nresults = [\n    {\"correctness\": 4, \"clarity\": 5, \"completeness\": 3},\n    {\"correctness\": 5, \"clarity\": 4, \"completeness\": 4}\n]\n\ndf = pd.DataFrame(results)\nprint(df.mean())\n</code></pre> <ul> <li>\ud83d\udcca Supports statistical analysis and visualization.</li> <li>\ud83d\udd01 Integrates well with MLOps &amp; evaluation tracking.</li> </ul> <p>Back to top \u2191</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/","title":"Multi-Turn Conversation Evaluation Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Evaluating at Different Levels</li> <li>Practical Strategies for Multi-Turn Evaluation</li> <li>Automated Evaluation of Multi-Turn Traces</li> <li>Common Pitfalls</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#introduction","title":"Introduction","text":"<p>Evaluating multi-turn conversations is a crucial aspect of assessing Large Language Model (LLM) applications, especially for assistants and chatbots that require robust handling of prolonged interactions. As outlined in Lesson 6 - Evaluating Multi-Turn Conversations, the core Analyze-Measure-Improve evaluation lifecycle applies, but multi-turn scenarios introduce new layers of complexity in both what is measured and how evaluation data is collected. A trace in this context encompasses the entire sequence of exchanges within a conversation, including all user inputs, LLM responses, tool calls, and intermediate steps from start to finish.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#evaluating-at-different-levels","title":"Evaluating at Different Levels","text":"<p>When assessing multi-turn conversations, evaluation can be approached at three distinct levels to ensure a comprehensive understanding of performance:</p> <ul> <li>Session Level: The highest level of assessment, focusing on whether the full conversation successfully achieves the user\u2019s intended goal.</li> <li>Approach: Results in a simple binary (Pass/Fail) judgment, providing a high-level view of system effectiveness.</li> <li>Example: If a peer feedback assistant concludes a session without eliciting useful feedback, the entire session is marked as a failure, regardless of individual turn quality.</li> <li>Recommendation: Binary labeling is recommended as a starting point due to its simplicity and ability to clearly define success criteria. Over time, broad \u201cFail\u201d categories can be refined and split into more specific failure modes.</li> <li>Turn Level: Involves assessing the quality of individual responses within a conversation, similar to single-turn evaluation.</li> <li>Criteria: Examines criteria like relevance, correctness, and tone.</li> <li>Use Case: Typically reserved for debugging specific failures rather than general evaluation, as analyzing every turn in every trace is inefficient.</li> <li>Conversational Coherence and Memory: Assesses the LLM\u2019s ability to maintain context and appropriately retain knowledge throughout the conversation, and potentially across multiple sessions using stored summaries or memory tools.</li> <li>Challenges: Failures at this level are often subtle but critical for a consistent user experience, typically emerging only after several turns.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#practical-strategies-for-multi-turn-evaluation","title":"Practical Strategies for Multi-Turn Evaluation","text":"<p>The evaluation process begins by collecting an initial dataset of multi-turn traces. This is typically done by having team members simulate user behavior through a lightweight chat interface, engaging in diverse tasks to gather approximately 100 or more varied traces. Key strategies include:</p> <ul> <li>Reproducing and Simplifying Issues: When failures are identified, attempt to reproduce them using simpler, reduced test cases.</li> <li>Example: If a shopping assistant provides an incorrect return policy on the fourth turn, simplify to a single-turn test to determine if it\u2019s a multi-turn memory issue or a fundamental retrieval/grounding problem.</li> <li>Inherent Multi-Turn Failures: For issues inherently tied to the conversational setup (e.g., an assistant forgetting a user\u2019s preference after several turns), truncate the trace to the context just before the failure (e.g., \u201cN-1\u201d turns) and run the LLM multiple times to test short-term memory and consistency.</li> <li>Introducing Perturbations: Introduce artificial changes into real traces to uncover robustness issues that might not appear in unmodified logs.</li> <li>Methods: Modify the user\u2019s goal mid-conversation, add ambiguity, or correct the assistant to test its ability to interpret updated constraints and adapt its behavior.</li> <li>Purpose: Ensures the assistant can handle realistic multi-turn interaction dynamics effectively.</li> <li>Caution with User Simulation: While LLMs can simulate users to target specific phenomena (e.g., an increasingly frustrated user), simulations should be used cautiously, as real or lightly modified conversations more accurately reflect actual user behavior.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#automated-evaluation-of-multi-turn-traces","title":"Automated Evaluation of Multi-Turn Traces","text":"<p>Once clear failure types are identified through error analysis, automated evaluators can be developed using LLM-as-Judge models, programmatic filters, or a hybrid approach. These evaluators can operate at different levels:</p> <ul> <li>Session Level: Assess whether the assistant met the user\u2019s goal (e.g., \u201cDid the conversation achieve the intended outcome?\u201d).</li> <li>Specific Multi-Turn Behaviors: Target specific issues such as self-contradiction or failure to retain important facts (e.g., \u201cDid the assistant contradict itself? Did it retain key user preferences?\u201d).</li> <li>Efficiency: Not every evaluator needs to assess every turn; session-level evaluation often provides a broader performance signal with less computational overhead.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/05-evals-multi-turn-conversations/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid these common pitfalls to ensure effective multi-turn conversation evaluation:</p> <ul> <li>Correlation with Position in Conversation: Systems may perform well in initial turns but degrade over longer conversations, forgetting context or ignoring instructions. Test sets should include conversations of varying lengths to detect these issues.</li> <li>Balancing Holistic and Granular Evaluation: Session-level scores effectively track overall system performance, but granular turn- or span-level analyses are vital for debugging specific weaknesses. A practical approach combines both: identify failing sessions first, then perform deep dives on representative traces.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/","title":"Retrieval-Augmented Generation Evaluation Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Overview of RAG Pipeline</li> <li>Synthetically Generating Query-Answer Pairs</li> <li>Metrics for Retrieval Quality</li> <li>Evaluating and Optimizing Chunking Strategies</li> <li>Evaluating Generation Quality</li> <li>Common Pitfalls</li> <li>Exerciese</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#introduction","title":"Introduction","text":"<p>Retrieval-Augmented Generation (RAG) is a fundamental architecture in modern Large Language Model (LLM) systems, widely used in applications such as customer support and scientific question-answering. Evaluating RAG is challenging because failures can originate at various points in the pipeline\u2014query construction, retrieval, reranking, and generation\u2014and these stages interact in complex ways. As outlined in Lesson 6 - Evaluating Retrieval-Augmented Generation (RAG), the core Analyze-Measure-Improve evaluation lifecycle applies, enabling systematic assessment and refinement of RAG pipelines.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#overview-of-rag-pipeline","title":"Overview of RAG Pipeline","text":"<p>A basic RAG pipeline retrieves context from a knowledge base based on a user\u2019s query and feeds this information to an LLM for response generation. Unlike traditional search, RAG prioritizes high recall to ensure all potentially relevant content is included for the LLM. Most modern RAG systems employ a two-stage retrieval approach:</p> <ul> <li>Stage 1: First-Pass Retrieval: Quickly scans the entire document corpus to identify a broad set of candidate documents, prioritizing high recall. Fast, lightweight methods like embedding-based search or BM25 are used.</li> <li>Stage 2: Reranking: Refines the initial candidate set using slower, more accurate models (e.g., cross-attention models or generative LLMs) to re-order and select the most relevant documents, focusing on precision and ranking quality.</li> <li>Evaluation Note: It is crucial to evaluate the retrieval component independently before assessing the quality of the generated answers.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#synthetically-generating-query-answer-pairs","title":"Synthetically Generating Query-Answer Pairs","text":"<p>Evaluating the retrieval component requires a dataset of queries linked to specific, relevant document chunks (retrieval \"targets\"). While manual curation is effective but expensive, synthetic generation using an LLM is a scalable alternative.</p> <ul> <li>Process: Prompt an LLM to extract a salient fact from a document chunk and generate a question answerable only by that fact.</li> <li>Challenging Queries: Select a target chunk and instruct the LLM to generate a question reusing terminology from similar chunks in the corpus, but definitively answered only by the target chunk. This tests the retriever\u2019s ability to isolate precise information despite surface-level distractions.</li> <li>Filtering Synthetic Queries: Synthetic questions may be unrealistic or off-domain, so filter for realism and relevance. Manually rate a subset of questions, then prompt an LLM with these examples to score the remaining queries on a Likert-style scale (e.g., 1-5 for realism) in a \"fuzzy ranking\" task.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#metrics-for-retrieval-quality","title":"Metrics for Retrieval Quality","text":"<p>Standard ranking metrics evaluate the retriever\u2019s performance using a dataset of queries and relevant document chunks:</p> <ul> <li>Precision@k: Measures the proportion of relevant documents among the top k retrieved results.</li> <li>Formula: (Number of relevant documents in the top k results) / k.</li> <li>Recall@k: Measures the proportion of all known relevant documents for a query found within the top k retrieved documents.</li> <li>Formula: (Number of relevant documents in the top k results) / (Total number of relevant documents for the query).</li> <li>Note: For the first stage of RAG, recall is prioritized over precision, as LLMs can work with some irrelevant content if key information is present but cannot generate missing information.</li> <li>Mean Reciprocal Rank (MRR): Assesses how early the first relevant document appears, scoring 1/rank if found and 0 otherwise. Useful when a single key fact is needed.</li> <li>Normalized Discounted Cumulative Gain (NDCG@k): Evaluates scenarios with graded relevance, rewarding systems that place more relevant items higher. Crucial for RAG to prioritize relevant documents within the LLM\u2019s limited context window.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#evaluating-and-optimizing-chunking-strategies","title":"Evaluating and Optimizing Chunking Strategies","text":"<p>The method of dividing documents into \"chunks\" significantly impacts retrieval and generation quality. Key parameters include chunk size, overlap, and chunking method (e.g., fixed-length, semantic segmentation).</p> <ul> <li>Optimization: Perform a grid search to evaluate retrieval metrics like Recall@5 and NDCG@5 across different chunk sizes and overlaps for fixed-size chunking.</li> <li>Content-Aware Chunking: For complex documents where fixed-size chunking may separate interconnected information:</li> <li>Leveraging Natural Document Structure: Use inherent boundaries like paragraphs or sections.</li> <li>Contextual Augmentation: Prepend surrounding context (e.g., titles, section headings) to make chunks more self-contained.</li> <li>Task-Dependent Chunking: Optimal chunk size varies by task:</li> <li>Constant Output Tasks: Finding a single fact can tolerate larger chunks (16k\u201332k tokens).</li> <li>Variable-Size Output Tasks: Extracting multiple items (e.g., all names) requires smaller chunks (~1k tokens) due to higher reasoning load.</li> <li>Evaluation Note: Optimizing chunking is an empirical process requiring systematic evaluation against retrieval metrics to identify root causes of failures.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#evaluating-generation-quality","title":"Evaluating Generation Quality","text":"<p>Once retrieval performance is satisfactory, evaluate the LLM\u2019s generation quality using the retrieved context. Frameworks like ARES provide key dimensions:</p> <ul> <li>Answer Faithfulness: Assesses whether the LLM\u2019s output accurately reflects the retrieved context, preventing:</li> <li>Hallucinations: Information absent from source documents.</li> <li>Omissions: Relevant information from the context ignored.</li> <li>Misinterpretations: Information inaccurately represented.</li> <li>Answer Relevance: Determines if the generated answer is directly pertinent to the original query, beyond being faithful to the context. These generation failures should be investigated during the initial error analysis phase (as discussed in Lesson 3) to pinpoint specific issues within the pipeline.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid these common mistakes to ensure effective RAG pipeline evaluation:</p> <ul> <li>Over-reliance on End-to-End Metrics: A single correctness score cannot clarify whether a failure stems from retrieval or generation. Separate component-level measurements (e.g., Recall@k for retrieval, faithfulness for generation) are essential.</li> <li>Overfitting to Synthetic Evaluation Datasets: Synthetic data may lack the ambiguity and diversity of real user queries. Regular validation with real user queries or human-curated examples is crucial.</li> <li>Incorrect Metric Selection: Choosing a metric misaligned with the application\u2019s needs (e.g., MRR for tasks requiring synthesis from multiple chunks) can optimize for the wrong behaviors. Recall@k or NDCG@k is often more appropriate for such tasks.</li> <li>Ignoring the Impact of Chunking Strategy: Chunking directly influences retrieval performance and must be treated as a tunable component, not a fixed scheme.</li> <li>Evaluating Generation Without Grounding Checks: Assessing fluency or overlap with a reference answer is insufficient. Context-sensitive metrics are needed to ensure alignment with retrieved content and prevent hallucinations or misrepresentations.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercises","title":"Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#chapter-7-exercises-evaluating-specific-architectures-modalities","title":"Chapter 7: Exercises \u2013 Evaluating Specific Architectures &amp; Modalities","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ul> <li>Exercise 1: Designing Synthetic QA Pair Prompts</li> <li>Exercise 2: Filtering Synthetic Questions</li> <li>Exercise 3: Multi-Hop Synthetic Evaluation Design</li> <li>Exercise 4: Interpreting RAG Performance with NDCG@k</li> <li>Exercise 5: Diagnosing RAG Failures with ARES Metrics</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercise-1-designing-synthetic-qa-pair-prompts","title":"Exercise 1: Designing Synthetic QA Pair Prompts","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#a-prompt-draft","title":"(a) Prompt Draft","text":"<p>You are a helpful assistant generating synthetic QA pairs for retrieval evaluation over a movie database. Given a chunk of text describing a single movie, do the following:</p> <ul> <li>Identify one fact that is self-contained (e.g., \u201cThe film stars Leonardo DiCaprio as Dom Cobb\u201d).</li> <li>Formulate a question that can only be answered by that fact.</li> <li>Return exactly one JSON object with keys \"fact\" and \"question\"\u2014no extra keys or commentary.</li> </ul> <p>Output format (exactly): <pre><code>{ \"fact\": \"...\", \"question\": \"...\" }\n</code></pre></p> <p>Chunk:</p> <p>\"Inception is a 2010 science-fiction film written and directed by Christopher Nolan. The film stars Leonardo DiCaprio as Dom Cobb, a professional thief who steals information by infiltrating the subconscious.\"</p> <p>Expected LLM Output Example: <pre><code>{\n  \"fact\": \"Christopher Nolan wrote and directed the 2010 science-fiction film Inception\",\n  \"question\": \"Who wrote and directed the 2010 science-fiction film Inception?\"\n}\n</code></pre></p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#b-modified-prompt-for-adversarial-questions","title":"(b) Modified Prompt for Adversarial Questions","text":"<p>You are a helpful assistant generating adversarial synthetic QA pairs for retrieval evaluation over a movie database. Given:</p> <ul> <li>A target chunk (Chunk A) describing one movie.</li> <li>A set of other movie chunks (Chunks B, C, ...) that mention overlapping terms like \u201cscience-fiction film\u201d or \u201c2010\u201d but do not contain the specific fact.</li> </ul> <p>Do the following:</p> <ul> <li>Extract a single, self-contained fact from Chunk A.</li> <li>Write a question that uses terminology from Chunks B, C, ... but can be answered only by the fact in Chunk A.</li> <li>Return exactly one JSON object with keys \"fact\" and \"question\".</li> </ul> <p>Chunk A (target):</p> <p>\"Inception is a 2010 science-fiction film written and directed by Christopher Nolan. The film stars Leonardo DiCaprio as Dom Cobb.\"</p> <p>Similar chunks:</p> <ul> <li>Chunk B: \u201cInterstellar is a 2014 science-fiction film directed by Christopher Nolan.\u201d</li> <li>Chunk C: \u201cAvatar is a 2009 science-fiction film directed by James Cameron.\u201d</li> </ul> <p>Expected Output: <pre><code>{\n  \"fact\": \"The film stars Leonardo DiCaprio as Dom Cobb\",\n  \"question\": \"Which actor plays Dom Cobb in the 2010 science-fiction film written and directed by Christopher Nolan?\"\n}\n</code></pre></p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercise-2-filtering-synthetic-questions","title":"Exercise 2: Filtering Synthetic Questions","text":"<p>After generating synthetic questions, you obtain examples such as:</p> <ul> <li>\u201cWhich actor plays Dom Cobb in Inception?\u201d</li> <li>\u201cIf I sum the runtimes of Inception and Interstellar, what is the total?\u201d</li> <li>\u201cWhen was The Dark Knight released in theaters?\u201d</li> <li>\u201cHow many Oscars did the movie starring Leonardo DiCaprio as Dom Cobb win?\u201d</li> </ul> <p>You plan to filter these using an LLM that rates each question\u2019s realism on a 1\u20135 scale.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#a-few-shot-prompt","title":"(a) Few-Shot Prompt","text":"<p>You are an AI assistant tasked with scoring movie-related questions for realism on a scale from 1 to 5 (1 = Very Unrealistic, 5 = Very Realistic). Provide a brief explanation for each rating.</p> <p>Realistic Examples: - \"Which actor plays Dom Cobb in Inception?\"   Rating: 5   Explanation: A straightforward fact-based question about a well-known film role.</p> <ul> <li>\"When was The Dark Knight released in theaters?\"   Rating: 5   Explanation: Common metadata question about a blockbuster movie release.</li> </ul> <p>Unrealistic Examples: - \"What is the combined IMDb user rating of every Quentin Tarantino film released before 1990?\"   Rating: 1   Explanation: Too contrived\u2014unlikely a user would ask for an aggregate rating across multiple films from different eras.</p> <ul> <li>\"Which movie has the longest sequence of consecutive close-up shots of an actor blinking?\"   Rating: 2   Explanation: Overly specific and not representative of typical information needs.</li> </ul> <p>Task: Assign a rating and explanation for:</p> <p>\u201cIf I combine the box office earnings of every Christopher Nolan\u2013directed film, what is the sum?\u201d</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercise-3-multi-hop-synthetic-evaluation-design","title":"Exercise 3: Multi-Hop Synthetic Evaluation Design","text":"<p>Two chunks from a movie corpus:</p> <ul> <li>Chunk 1: \u201cInception is a 2010 science-fiction film written and directed by Christopher Nolan.\u201d</li> <li>Chunk 2: \u201cLeonardo DiCaprio stars as Dom Cobb in Inception.\u201d</li> </ul> <p>You want to generate a 2-hop question.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#a-prompting-procedure","title":"(a) Prompting Procedure","text":"<p>Hop 1 Prompt: <pre><code>{\n  \"fact\": \"Christopher Nolan wrote and directed the 2010 science-fiction film Inception\",\n  \"question\": \"Who wrote and directed the 2010 science-fiction film Inception?\"\n}\n</code></pre></p> <p>Hop 2 Prompt: <pre><code>{\n  \"fact\": \"Leonardo DiCaprio stars as Dom Cobb in Inception\",\n  \"question\": \"Which actor stars as Dom Cobb in the film written and directed by Christopher Nolan in 2010?\"\n}\n</code></pre></p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#b-retrieval-evaluation-strategy","title":"(b) Retrieval Evaluation Strategy","text":"Metric Formula / Notes TwoHopRecall@k Fraction of 2-hop queries where both Chunk 1 and Chunk 2 are in top-k results Hop-Specific MRR MRR1 for Hop 1; MRR2 for Hop 2 after intermediate entity is known Error Attribution Hop 1 Miss, Hop 2 Miss, Rank-Out-of-Top-k"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#error-breakdown","title":"Error Breakdown","text":"Error Type Count Fraction Hop 1 Miss n1 n1/N Hop 2 Miss n2 n2/N Rank-Out-of-Top-k n3 n3/N <p>\u2b06\ufe0f Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercise-4-interpreting-rag-performance-with-ndcgk","title":"Exercise 4: Interpreting RAG Performance with NDCG@k","text":"<p>Given: - IDCG@3 = 5.5 - System A retrieved: (3, 0, 2) - System B retrieved: (2, 3, 0)</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#a-dcg3-calculation","title":"(a) DCG@3 Calculation","text":"<ul> <li> <p>System A: <code>3/log2(2) + 0/log2(3) + 2/log2(4)</code> = 3 + 0 + 1 = 4.0</p> </li> <li> <p>System B: <code>2/log2(2) + 3/log2(3) + 0/log2(4)</code> \u2248 2 + 1.89 + 0 = 3.89</p> </li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#b-ndcg3","title":"(b) NDCG@3","text":"<ul> <li>System A: <code>4.0 / 5.5</code> \u2248 0.727</li> <li>System B: <code>3.89 / 5.5</code> \u2248 0.707</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#c-analysis","title":"(c) Analysis","text":"<p>System A performs slightly better due to higher rank placement of the highly relevant document.</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#exercise-5-diagnosing-rag-failures-with-ares-metrics","title":"Exercise 5: Diagnosing RAG Failures with ARES Metrics","text":"<p>Query: \u201cHow do I reset the foobar widget to factory settings?\u201d</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#ares-evaluation-results","title":"ARES Evaluation Results","text":"Metric Score Observation Context Relevance High Retrieved chunk contains factory reset instructions for several widgets Answer Faithfulness High LLM quotes instructions faithfully Answer Relevance Low LLM answers for \u201cbazqux\u201d widget instead of \u201cfoobar\u201d widget"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/06-evals-rag/#diagnosis","title":"Diagnosis","text":"<ul> <li>Retriever: \u2705 (Correct info included)</li> <li>Generator: \u274c (Selected wrong instructions)</li> <li>Core Issue: Poor grounding/focus mechanism</li> </ul> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/","title":"Complex LLM Architectures Evaluation Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Tool Calling</li> <li>Agentic Systems</li> <li>Debugging Multi-Step Pipelines</li> <li>Evaluating Specific Input Data Modalities</li> <li>Common Pitfalls</li> <li>Summary</li> <li>Exerciese</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#introduction","title":"Introduction","text":"<p>Evaluating complex Large Language Model (LLM) architectures, such as those involving tool calling, agentic systems, and multi-modal inputs, is critical for ensuring robust performance in advanced applications. As outlined in Lesson 8 - Evaluating Complex LLM Architectures, the Analyze-Measure-Improve evaluation lifecycle applies, but these architectures introduce unique challenges due to their multi-step processes and diverse input modalities. This guide covers evaluation strategies for tool calling, agentic systems, debugging multi-step pipelines, and handling specific input modalities like images, long documents, and PDFs.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#tool-calling","title":"Tool Calling","text":"<p>Tool calling architectures enable LLMs to interact with external systems, from structured API endpoints (e.g., <code>get_calendar_availability</code>) to open-ended code execution tools (e.g., <code>exec_code</code>). The impact of errors varies by tool type, with read-only tools being less risky than those that write data. Careful prompt engineering of tool descriptions (names, descriptions, parameter specifications) is crucial for correct tool selection and use.</p> <p>The tool calling cycle involves: - Tool Selection: The LLM interprets a request and selects a tool from a predefined list. Failures include choosing an inappropriate tool, hallucinating a non-existent tool, or failing to use a necessary tool. Overlapping tool functionalities can lead to incorrect selections. - Argument Generation: The LLM generates arguments for the selected tool. Failures include structural issues (e.g., type/schema violations like \"700k\" instead of 700000 for a price) and semantic errors (e.g., wrong date format). Automated schema validation tools (e.g., Pydantic) detect structural issues, while semantic errors may require reference data or custom validators. Security risks, like SQL injection from unsanitized inputs, are also a concern. - Execution Success: Even with valid arguments, tool calls may fail, returning empty results (silent failures, e.g., invalid <code>property_id</code>) or runtime errors (e.g., network issues, API timeouts). Logging error messages and tracebacks aids error analysis. - Handling Tool Output: The LLM must correctly interpret the tool\u2019s successful response. Failures include misinterpreting data (e.g., misstating property square footage), ignoring details (e.g., overlooking a \"structural issues reported\" note), or failing to integrate output logically (e.g., asking \u201cWhen are you free?\u201d instead of proposing times from a calendar tool). Evaluating each stage separately is critical, as failures can cascade, and pinpointing the exact stage guides interventions like prompt refinement or schema adjustments.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#agentic-systems","title":"Agentic Systems","text":"<p>Agentic systems empower LLMs to make sequences of decisions, often involving tool use and iterative reasoning, to achieve complex goals. A crucial first step is defining the intended spectrum of agency:</p> <ul> <li>High Agency: The agent continues making tool calls, refining plans, and pursuing sub-goals until confidently fulfilling the user\u2019s request (e.g., \u201cKeep invoking tools and resolve issues until confident in the final answer\u201d).</li> <li>Low Agency: The agent cedes control, stops, or asks for human input when uncertain or encountering unexpected situations.</li> </ul> <p>Without clear agency definitions, distinguishing errors from intended behavior (e.g., clarification as a failure for high-autonomy vs. correct for cautious agents) is difficult. Evaluation must assess: - Reasonableness of decisions at each step. - Logical progression of actions toward the goal. - Adherence to the designed level of agency. - Avoidance of issues like getting stuck in loops, giving up prematurely, or over-extending.</p> <p>Standardized communication protocols, like the Model Context Protocol (MCP), improve robustness by defining structured message formats for tool calls and responses, aiding logging and trace analysis, but don\u2019t alter fundamental evaluation questions.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#debugging-multi-step-pipelines","title":"Debugging Multi-Step Pipelines","text":"<p>Debugging multi-step pipelines (e.g., RAG, tool calling, agentic systems) is challenging due to cascading errors and potential LLM reasoning failures mid-process. Traceability is key, capturing inputs, outputs, intermediate reasoning (\u201cthoughts\u201d), tool calls (arguments and responses), and decisions at each state. Specialized observability platforms are invaluable.</p> <p>To identify systemic failure patterns: - Define Discrete States: Represent significant pipeline stages (e.g., tool calls like <code>GenSQL</code>, <code>ExecSQL</code>, or reasoning steps like <code>Update_Plan</code>). - Attribute First Failure: For failing traces, identify the first state where something went wrong to avoid misblaming downstream components. Use automated evaluators or human annotation with predefined failure criteria. - Transition Failure Matrix: Aggregate failure data into a matrix where rows are \u201cFrom State\u201d (last successful state) and columns are \u201cIn State\u201d (first failure state). The cell value (i, j) counts failures in state j after state i.   - Visualization: A heatmap (e.g., Figure 10) highlights failure hotspots, prioritizing debugging efforts.   - Analysis: Sum column counts to identify frequently failing states; analyze \u201cFrom State\u201d to pinpoint problematic transitions or contexts. - Iterative Debugging: As fixes are implemented, the failure heatmap should become sparser or show lower intensity.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#evaluating-specific-input-data-modalities","title":"Evaluating Specific Input Data Modalities","text":"<p>Pipelines processing inputs beyond simple text, such as images, long documents, or structured PDFs, require modality-specific failure mode evaluation:</p> <ul> <li>Images (Vision-Language Models - VLMs):</li> <li>Strengths: VLMs excel at describing visual properties (e.g., color, shape).</li> <li>Failures: Errors in spatial reasoning (e.g., misjudging left/right), counting objects, recognizing text in images, or hallucinating details.</li> <li>Evaluation: Use clear, binary criteria and rubrics, adaptable for LLM-as-Judge prompts, with human or VLM judges processing visual input. For text-to-image generation, assess prompt matching, quality, coherence, and safety. Diffusion models struggle with global constraints (e.g., exact counts, distinct object identities in complex compositions).</li> <li>Long Documents:</li> <li>Challenges: LLMs struggle with long text, with accuracy degrading as length increases, even within context windows.</li> <li>Constant Output Tasks (\u201cNeedle-in-a-Haystack\u201d): Finding small, fixed information (e.g., specific facts). Check if performance drops when information is \u201clost in the middle.\u201d Large chunks (16k\u201332k tokens) are often suitable.</li> <li>Variable-Size Output Tasks: Outputs grow with document length (e.g., extracting all names, summarizing sections). Process documents in smaller chunks, aggregating results. Optimal chunk size (e.g., ~1k tokens) is needed for complex reasoning or tracking multiple elements.</li> <li>Evaluation: Combine per-chunk and whole-document checks (e.g., all relevant items found, no duplicates). LLM-as-Judge for long documents requires providing only relevant source portions or chunk-level evaluation due to context limits.</li> <li>PDF Documents:</li> <li>Challenges: Extraction errors from OCR on scanned documents (e.g., incorrect text, broken tables, wrong reading order). Tables are particularly problematic.</li> <li>Evaluation: Separate extraction errors from LLM reasoning failures by inspecting the extracted content seen by the LLM, not just the original PDF. The format of extracted content (e.g., table row-by-row vs. column-by-column) also impacts performance.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid these common mistakes to ensure effective evaluation of complex LLM pipelines:</p> <ul> <li>Tool Calling Systems:</li> <li>Focusing solely on execution success, neglecting structurally valid but semantically incorrect arguments or LLM misinterpretation of successful tool outputs.</li> <li>Underestimating the impact of tool descriptions. Unclear names, descriptions, or parameter specifications hinder effective tool use. Tool definitions must be tunable.</li> <li>Agentic Systems:</li> <li>Lack of clearly defined agency expectations, making it impossible to distinguish desired initiative from problematic overreach or cautious clarification from passivity.</li> <li>Over-relying on final task outcomes, obscuring inefficiencies or flawed reasoning in intermediate steps. Analyze plan coherence and decision-making efficiency.</li> <li>General Pipeline Concerns:</li> <li>Insufficient traceability. Without comprehensive logging of inputs, outputs, LLM \u201cthoughts,\u201d and tool interactions, pinpointing root causes is difficult.</li> <li>Delaying or omitting isolated component evaluation (e.g., PDF text extraction, tool reliability) before full integration, hindering efficient bottleneck identification. Both component-level and end-to-end evaluations are necessary.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#summary","title":"Summary","text":"<p>Effective evaluation of complex LLM applications requires tailored strategies for specific architectures and data types. For tool calling, check failures in tool selection, argument generation, execution success, and output handling. For agentic systems, evaluate session success, user satisfaction, and consistency, alongside systematic debugging via state transition failure analysis. Address modality-specific challenges for images, long documents, and PDFs. Robust evaluation integrates these techniques with component checks, end-to-end task assessment, and systematic error analysis to build reliable systems.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#exercises","title":"Exercises","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#quick-links","title":"\ud83d\udd17 Quick Links","text":"<ul> <li>Analyzing Faulty Tool Argument Generation</li> <li>Agentic System Design: Impact of Agency on Behavior &amp; Evaluation</li> <li>Evaluating a Complex PDF and Long Document Processing Pipeline</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#analyzing-faulty-tool-argument-generation","title":"\ud83d\udee0\ufe0f Analyzing Faulty Tool Argument Generation","text":"<p>Scenario: An LLM-powered travel assistant is asked to book a flight and generates arguments for a <code>search_flights</code> tool. The user specifies details like:</p> <p>\"two adults from London to New York, sometime in the first week of June, with an early morning departure\".</p> <p>LLM Output: <pre><code>{\n  \"date_range\": \"June 1st - June 7th\",\n  \"num_passengers\": \"2\",\n  \"preferred_time\": \"any\"\n}\n</code></pre></p> <p>Expected Tool Schema: - <code>start_date</code>, <code>end_date</code>: YYYY-MM-DD format - <code>num_passengers</code>: Integer - <code>preferred_departure_window</code>: Optional tuple of integers (e.g., [5, 9] for 5\u20139 AM)</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#task","title":"\ud83d\udd0d Task","text":"<p>Identify and classify three distinct failures in the generated arguments. For each, explain the issue and recommend an improvement.</p> Failure Issue Description Why It\u2019s Wrong Suggested Improvement 1. <code>date_range</code> as a string Uses informal range \"June 1st - June 7th\" The tool expects structured dates Add schema clarification and date format example in prompt 2. <code>num_passengers</code> as a string <code>\"2\"</code> instead of <code>2</code> Data type mismatch Add validation logic or LLM prompt hint to use integers 3. <code>preferred_time</code> as <code>\"any\"</code> Ignores user preference \"early morning\" Semantic omission Emphasize mapping user intent (\"early morning\") to structured format ([5, 9])"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#agentic-system-design-impact-of-agency-on-behavior-evaluation","title":"\ud83e\udd16 Agentic System Design: Impact of Agency on Behavior &amp; Evaluation","text":"<p>Scenario: An agent helps users troubleshoot software issues using tools: - <code>run_diagnostic_scan()</code> - <code>search_knowledge_base(query)</code> - <code>ask_clarifying_question_to_user(question)</code></p> <p>User reports: \"software crashes when I open it after update.\"</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#high-agency","title":"High Agency","text":"<ul> <li>Instruction: Resolve issues independently</li> <li>Agent Actions:</li> <li><code>run_diagnostic_scan()</code></li> <li>Parse logs, infer issue</li> <li><code>search_knowledge_base(\"fix for crash after update\")</code></li> </ul> <p>Alignment: Shows initiative and inference without waiting for clarification.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#low-agency","title":"Low Agency","text":"<ul> <li>Instruction: Prioritize safety and clarity</li> <li>Agent Actions:</li> <li><code>ask_clarifying_question_to_user(\"Did the crash begin after installing a specific update?\")</code></li> <li>Wait for response</li> <li><code>search_knowledge_base(\"software crash after update &lt;version info&gt;\")</code></li> </ul> <p>Alignment: Seeks clarity first to reduce risk of wrong actions.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#key-concept","title":"\ud83e\udde0 Key Concept","text":"<p>Agent behavior and evaluation metrics must be aligned to its designed autonomy level.</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#evaluating-a-complex-pdf-and-long-document-processing-pipeline","title":"\ud83d\udcda Evaluating a Complex PDF and Long Document Processing Pipeline","text":"<p>Scenario: You build an LLM pipeline for answering questions from 500-page scanned PDFs with: - Dense academic text - Tables and diagrams with captions</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/07-specific-arch-data-modalities/#evaluation-challenges-and-strategies","title":"\ud83c\udfaf Evaluation Challenges and Strategies","text":"Challenge Description Evaluation Method 1. OCR / Table Extraction Accuracy Extracting correct values from diagrams, tables Spot-check extracted tables against source images 2. Chunked Context Synthesis Answers require combining chunks Use multi-hop QA tests across sections 3. Diagram and Caption Interpretation Diagrams are rich in answers Evaluate QA performance specifically on caption-only queries <p>\ud83d\udcdd Summary: These exercises demonstrate how to evaluate tool use, agent autonomy, and document processing pipelines within specialized LLM systems.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/","title":"Continuous Integration and Deployment Evaluation Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>CI: Building a Safety Net Against Regressions</li> <li>CD &amp; Online Monitoring: Tracking Real-World Performance</li> <li>The Continuous Improvement Flywheel</li> <li>Practical Considerations and Common Pitfalls</li> <li>Summary</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#introduction","title":"Introduction","text":"<p>Maintaining and improving Large Language Model (LLM) pipeline performance over time, amidst evolving user needs, data drift, and foundation model changes, requires a continuous evaluation approach. As outlined in Lesson 9 - Continuous Integration and Deployment, this chapter focuses on embedding the Measure phase (Section 5) into an ongoing engineering lifecycle that drives the Improve phase (Figure 2). Applying Continuous Integration (CI) and Continuous Deployment/Delivery (CD) to LLM pipelines presents unique challenges, including non-determinism, subjective quality, scarce labeled data, and black-box foundation model updates. This necessitates approaches like \"unit tests for known unknowns\" (CI) and \"online monitoring for unknown unknowns\" (CD).</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#ci-building-a-safety-net-against-regressions","title":"CI: Building a Safety Net Against Regressions","text":"<p>Continuous Integration (CI) for LLM pipelines ensures code changes integrate correctly without introducing regressions. Unlike traditional ML CI, which measures overall accuracy on large test sets, LLM CI focuses on preventing regressions for known failure modes using curated, smaller golden datasets. These datasets don\u2019t predict overall production accuracy but ensure stability as pipelines evolve.</p> <ul> <li>Golden Dataset:</li> <li>Creation: Requires human review and labeling, often using collaborative evaluation methods (Section 4).</li> <li>Purpose: Runs on every proposed pipeline change (e.g., prompt tweaks, model swaps) to evaluate outputs with automated evaluators (Section 5).</li> <li>Content: Includes examples covering core features, representative critical past failures (regression tests), challenging edge cases, and examples exercising different pipeline components or paths.</li> <li>Size: Typically 100+ examples.</li> <li>Evaluation Type: Reference-based.</li> <li>Maintenance: Ongoing process; add new failure modes or edge cases from online monitoring or error analysis to expand the CI \"safety net.\"</li> <li>Pitfalls:</li> <li>Offline Accuracy Misinterpretation: Golden datasets are small, curated, and not representative, offering weak statistical guarantees for live traffic performance.</li> <li>Data Leakage: Including golden set traces in main pipeline or LLM-as-Judge prompts artificially inflates performance, invalidating checks. CI pass rates indicate regression prevention, not production performance.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#cd-online-monitoring-tracking-real-world-performance","title":"CD &amp; Online Monitoring: Tracking Real-World Performance","text":"<p>Continuous Deployment (CD) and online monitoring track system behavior in production to uncover true performance, new failure modes, and \"unknown unknowns.\"</p> <ul> <li>Observability:</li> <li>Comprehensive Logging: For each production request (or representative sample), log:<ul> <li>Initial input and metadata (e.g., user/session IDs).</li> <li>All intermediate LLM calls (prompts, responses, reasoning).</li> <li>All tool calls (names, arguments, responses, errors).</li> <li>Retrieved documents in RAG pipelines.</li> <li>Final output and user feedback.</li> <li>Evaluator outputs.</li> </ul> </li> <li>Purpose: Enables reconstruction of full user interactions.</li> <li>Concepts:<ul> <li>Trace: Entire end-to-end processing of a single user request or workflow.</li> <li>Span: Discrete computational step within a trace.</li> <li>Session: Groups related traces over time, providing context for multi-turn conversations.</li> </ul> </li> <li>Running Automated Evaluators:</li> <li>Deploy automated evaluators (Section 5) on sampled production traces, often asynchronously.</li> <li>Compute corrected success rate (\u03b8\u0302) and 95% confidence interval for each failure mode using bootstrapping (Section 5.3) for statistically grounded real-world success estimates without labels.</li> <li>Trigger alerts if success rate or lower confidence bound falls below thresholds.</li> <li>Guardrails: Synchronous Online Evaluation:</li> <li>Lightweight, code-based checks (e.g., regex filters, schema validation, blacklist checks) run synchronously before final output.</li> <li>Require very low false failure rates for critical safety checks (e.g., detecting PII, toxicity) or enforcing output formats.</li> <li>Actions on failure: Reject (block output), Retry (re-run pipeline due to non-determinism), or Fallback (switch to alternative model/logic).</li> <li>Judge Drift:</li> <li>LLM-as-Judge evaluators may drift due to foundation model updates or changing quality criteria.</li> <li>Maintenance:<ul> <li>Pin model versions for CI and production.</li> <li>Periodically re-align judges against new human labels to recompute True Positive Rate (TPR) and True Negative Rate (TNR).</li> <li>Update judge prompts or few-shot examples if TPR/TNR falls below thresholds.</li> <li>Re-evaluate alignment when the judge\u2019s underlying LLM changes.</li> </ul> </li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#the-continuous-improvement-flywheel","title":"The Continuous Improvement Flywheel","text":"<p>CI and CD form a continuous improvement flywheel, iteratively refining LLM applications by connecting monitoring and evaluation to the Improve phase. The process includes:</p> <ol> <li>Develop &amp; Analyze (Initial): Develop the initial pipeline and conduct Error Analysis (Section 3) to identify key failure modes and understand data.</li> <li>Measure &amp; Build Evals: Translate qualitative failures into quantifiable metrics, implement/validate automated evaluators (Section 5), use collaborative methods (Section 4) for subjective rubrics, and build the initial golden dataset.</li> <li>CI Setup: Integrate golden dataset and automated evaluators into the CI pipeline for regression testing.</li> <li>Deploy (CD) with Observability: Ship the instrumented pipeline, deploy automated evaluators on sampled production traffic (often asynchronously), and pin LLM-as-Judge models.</li> <li>Monitor Online Performance: Track corrected success rates (\u03b8\u0302) and confidence intervals for key failure modes using live data, dashboard metrics, and set alerts. Track product metrics (e.g., user satisfaction, task completion rates).</li> <li>Identify Drift, New Failures, or Product Issues: Analyze online evaluation data for model/retriever drift or subtle regressions. Product metric drops (e.g., user engagement) signal undetected issues. Conduct proactive manual trace inspection (\"failure hunting\") for novel/ subtle issues.</li> <li>Re-Analyze (Error Analysis): Perform targeted Error Analysis (Section 3) on problematic traces when new issues or drift are confirmed, especially after product changes, user demographic shifts, or AI performance issues indicated by evaluators/product metrics.</li> <li>Update Evaluation Artifacts: Augment CI golden dataset with new failure examples, refine existing evaluators, or implement new ones. Re-align judges by updating prompts or few-shot examples if TPR/TNR drops.</li> <li>Improve Pipeline: Implement pipeline changes based on error analysis and online monitoring insights (Section 11).</li> <li>Re-deploy &amp; Iterate: Ship the improved pipeline and updated evaluation artifacts, resume monitoring, and expect increased success rates.</li> </ol> <p>This flywheel emphasizes evaluation as an ongoing, iterative process.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#practical-considerations-and-common-pitfalls","title":"Practical Considerations and Common Pitfalls","text":"<p>Operationalizing LLM evaluation requires addressing practical challenges and avoiding pitfalls:</p> <ul> <li>Tooling: Essential for observability, annotation, and sensemaking, including systems for logging/trace viewing (Section 9.2) and simple annotation interfaces (Section 10) for efficient human review.</li> <li>Stagnant CI Golden Datasets: Without updates from new production phenomena, CI suites become less effective at catching relevant regressions.</li> <li>Superficial Golden Dataset Curation: Quality and diversity are critical, not just size. Datasets must stress test core features, past failures, and challenging queries.</li> <li>Over-reliance on Automated Monitoring: LLM-as-Judge evaluators may miss subtle/novel failure modes; regular human \"failure hunting\" is critical. Product metrics signal undetected issues.</li> <li>Conflicting Metrics: Define, prioritize, and establish trade-off strategies for conflicting quality dimensions (e.g., conciseness vs. factual accuracy).</li> <li>Disconnect Between AI and Product Metrics: Investigate AI quality as a factor in product metric drops (e.g., user satisfaction) and define corresponding failure modes/evaluators.</li> <li>Delayed LLM-as-Judge Re-evaluation: Regularly recalculate TPR/TNR against fresh human labels, especially for subjective criteria or judge model changes, to ensure accurate success rate estimates.</li> <li>Insufficient Traceability: Comprehensive logging of all LLM calls, tool interactions, and retrieved context is essential for effective debugging.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/08-cid/#summary","title":"Summary","text":"<p>Operationalizing LLM evaluation adapts CI/CD to LLM systems. CI prevents regressions through automated checks on golden datasets, ensuring stability against known failures. CD measures real performance via online monitoring, tracking corrected success rates with confidence intervals and ensuring evaluator reliability through judge pinning and re-validation. Together, CI and CD form a continuous improvement loop where online monitoring feeds into error analysis, evaluation artifact updates, pipeline fixes, and redeployment. This approach establishes evaluation as a continuous engineering process, integrating automation with critical human oversight to build reliable and improvable LLM applications.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/","title":"Interfaces for Continuous Human Review and Error Analysis Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>The Case for Custom Review Interfaces</li> <li>Principles of Effective Review Interfaces</li> <li>Case Study: EvalGen Interface and Insight</li> <li>Selecting Traces from the \u201cFirehose\u201d for Human Review</li> <li>Navigating Trace Groups and Discovering Patterns</li> <li>Integrating Human Review into the Bigger Engineering Workflow</li> <li>Example Walkthrough: Reviewing Real Estate Assistant Emails</li> <li>Case Study: DocWrangler for Prompt Refinement</li> <li>Summary</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#introduction","title":"Introduction","text":"<p>Continuous human review is critical for maintaining and improving Large Language Model (LLM) pipelines, as automated evaluators alone cannot catch subtle errors, surface unexpected issues, or refine nuanced quality definitions. As outlined in Lesson 10 - Interfaces for Continuous Human Review and Error Analysis, human oversight complements the Analyze-Measure-Improve lifecycle (Figure 2) by addressing LLM-as-Judge drift and evolving quality criteria. The challenge is to involve humans effectively given the vast number of production traces, as slow or cumbersome review processes can stall the Improve phase.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#the-case-for-custom-review-interfaces","title":"The Case for Custom Review Interfaces","text":"<p>Investing in custom-built review interfaces maximizes the speed, quality, and impact of human feedback.</p> <ul> <li>Limitations of Existing Tools:</li> <li>Common tools like spreadsheets and generic log viewers struggle with complex LLM traces (long inputs, intermediate reasoning, tool calls, RAG context, final outputs).</li> <li>They make navigation tedious, structured/free-form feedback difficult, and lack LLM-specific features, leading to reviewer fatigue and lower-quality annotations.</li> <li>Benefits of Custom Interfaces: Improve consistency and efficiency by presenting trace information in digestible, visually intuitive ways (e.g., displaying emails as formatted emails, using color-coded tags for document annotations).</li> <li>Ease of Creation: Modern web frameworks or LLMs can rapidly generate basic interfaces from natural language prompts, reducing the need for extensive frontend development.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#principles-of-effective-review-interfaces","title":"Principles of Effective Review Interfaces","text":"<p>Effective review interfaces are guided by Human-Computer Interaction (HCI) principles:</p> <ul> <li>Visibility of System Status: Show which trace is being reviewed, how many remain, and feedback status.</li> <li>Match Between System and Real World: Use familiar terms from rubrics and evaluation criteria.</li> <li>User Control and Freedom: Allow easy navigation, undo actions, and deferring uncertain cases.</li> <li>Consistency and Standards: Maintain consistent layouts and terminology across traces.</li> <li>Error Prevention: Use distinct labeling controls and require rationales for \u201cFail\u201d judgments.</li> <li>Recognition Rather Than Recall: Visually present predefined failure modes to reduce cognitive load.</li> <li>Flexibility and Efficiency of Use: Offer keyboard shortcuts and intuitive design for novices and experts.</li> <li>Aesthetic and Minimalist Design: Emphasize signal over noise, clearly highlighting inputs and outputs.</li> <li>Essential Interface Elements: Display full LLM traces (collapsing less important sections by default), support structured feedback (checkboxes, buttons, quick ratings like thumbs up/down), and include open-ended feedback fields.</li> <li>Features to Improve Speed and Quality:</li> <li>Inline annotation for highlighting issues.</li> <li>Keyboard shortcuts for rapid actions.</li> <li>Visual hierarchy to emphasize key trace components.</li> <li>Batch review and bulk actions for labeling multiple examples.</li> <li>Progressive disclosure to simplify the interface.</li> <li>Feedback templates for common comments.</li> <li>\u201cDefer\u201d option for uncertain cases to maintain label accuracy.</li> <li>Contextual metadata display and filters.</li> <li>Progress indicators for reviewed counts.</li> </ul> <p>The speed of data review directly correlates with the engineering velocity of an AI application.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#case-study-evalgen-interface-and-insight","title":"Case Study: EvalGen Interface and Insight","text":"<p>EvalGen is a lightweight, early-stage interface for human evaluation, showing one output at a time, supporting binary feedback, open-ended comments, editable labels, and a basic rubric builder. It prioritizes traces where multiple automated LLM judges disagree.</p> <ul> <li>Insight: User studies revealed that human reviewers experience criteria drift, refining their understanding of \u201ccorrectness,\u201d revising past judgments, and adding new failure categories. Review tools must treat evaluation as an evolving sensemaking process, not a static labeling task.</li> <li>Features: EvalGen supports dynamic rubric updates, allowing reviewers to propose new failure modes during review. It logs reviewer comments to track evolving quality criteria, enabling iterative refinement of evaluation standards and integration with the Analyze-Measure-Improve cycle.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#selecting-traces-from-the-firehose-for-human-review","title":"Selecting Traces from the \u201cFirehose\u201d for Human Review","text":"<p>Given the impossibility of reviewing every production trace, smart strategies prioritize traces likely to uncover quality issues or improve the system:</p> <ul> <li>Random Sampling: Provides an unbiased overview of performance but may miss rare issues.</li> <li>Uncertainty Sampling: Focuses on traces where automated evaluators disagree or are unsure (e.g., failing some criteria but not others, inconsistent LLM judge results with temperature &gt; 0, or conflicting evaluators). These are often the most informative for human review.</li> <li>Failure-Driven Sampling: Targets obviously problematic cases identified by guardrail failures, timeouts, or user feedback. High-precision but may miss subtle issues.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#navigating-trace-groups-and-discovering-patterns","title":"Navigating Trace Groups and Discovering Patterns","text":"<p>Many issues become visible only when examining groups of traces:</p> <ul> <li>Clustering: Groups traces by metadata (e.g., client persona, feature used) or semantic similarity (using embeddings of inputs/outputs) to reveal latent categories and systematic failures.</li> <li>Search and Similarity Tools: Enable reviewers to find similar examples by keyword, metadata filters, or conceptual content. Blending group-level tools with targeted sampling directs attention to the most informative data.</li> <li>Pattern Discovery: Interfaces should support visualizations (e.g., failure heatmaps, clustering plots) to highlight recurring issues, such as consistent misinterpretations in specific contexts.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#integrating-human-review-into-the-bigger-engineering-workflow","title":"Integrating Human Review into the Bigger Engineering Workflow","text":"<p>For human review to be valuable, it must integrate into the team\u2019s engineering workflow:</p> <ul> <li>Automated Systems: Schedule daily/weekly trace selection and notify reviewers.</li> <li>Alerts: Route high-priority issues (e.g., guardrail failures) to on-call reviewers immediately.</li> <li>Feedback Flow:</li> <li>Golden Dataset: Add human-labeled traces (especially failures) to the CI golden dataset for regression testing.</li> <li>LLM-as-Judge Evaluators: Use human labels to monitor judge accuracy, revising prompts or few-shot examples if True Positive Rate (TPR) or True Negative Rate (TNR) drops.</li> <li>Lightweight Actions: Support actions like adding traces to the CI golden set or filing prefilled bug reports. Dashboards should show review activity and trends to prioritize fixes.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#example-walkthrough-reviewing-real-estate-assistant-emails","title":"Example Walkthrough: Reviewing Real Estate Assistant Emails","text":"<p>This section illustrates the evolution from a basic spreadsheet (awkward, slow, error-prone) to a lightweight custom UI (clear layout, faster decisions) and an advanced interface. The advanced UI features:</p> <ul> <li>Rich, readable layouts for traces, including intermediate reasoning/tool usage.</li> <li>Visual formatting for key entities (e.g., property details).</li> <li>Structured feedback options (predefined tags, hotkeys, metadata).</li> <li>Group-level exploration tools (clustering, search) for pattern detection and bulk labeling.</li> </ul> <p>Small UI improvements significantly enhance review speed and label quality.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#case-study-docwrangler-for-prompt-refinement","title":"Case Study: DocWrangler for Prompt Refinement","text":"<p>DocWrangler, an IDE for LLM-powered document processing, accelerates iterative development by displaying the original document, generated output, and prompt side-by-side. It supports:</p> <ul> <li>Open Coding: Reviewers jot natural-language comments on outputs to identify recurring issues.</li> <li>Prompt Refinement: Uses comments and the current prompt to suggest updates, presenting both versions side-by-side to close the feedback loop.</li> </ul> <p>This approach helps reviewers refine their understanding of errors and improve prompts iteratively.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/09-human-review-error-analysis/#summary","title":"Summary","text":"<p>Continuous human review is critical for high-quality LLM applications, as automated evaluation alone is insufficient. Custom interfaces designed with HCI principles enable efficient, high-quality feedback by presenting clear trace context, structured feedback mechanisms, and features to reduce cognitive load. Smart sampling (random, uncertainty-based, failure-driven) focuses human attention on valuable traces. Integrating review into the engineering workflow via automated scheduling, alerts, and feedback loops (e.g., updating golden datasets, refining LLM-as-Judge prompts) ensures actionable insights. By enabling rapid problem identification and iteration, these interfaces build more reliable LLM systems.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/","title":"Improvement Guide for LLM Applications","text":""},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Accuracy Optimization</li> <li>Quick Wins and Best Practices for Cost Reduction</li> <li>Leveraging LLM Provider Caching</li> <li>Advanced Strategy: Implementing Model Cascades</li> <li>Summary</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#introduction","title":"Introduction","text":"<p>The Improve phase, as outlined in Lesson 11 - Improvement, is where insights from the Analyze and Measure phases of the Analyze-Measure-Improve lifecycle (Figure 2) are applied to enhance Large Language Model (LLM) pipelines. This phase focuses on improving both the quality of results (accuracy optimization) and system efficiency (cost optimization). Effective improvements rely on established evaluators and monitoring systems to ensure robust and cost-effective LLM systems.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#accuracy-optimization","title":"Accuracy Optimization","text":"<p>Improving accuracy increases the likelihood of the LLM producing correct, helpful, or expected outputs. Strategies progress from quick, low-effort wins to significant structural and model-level changes.</p> <ul> <li>Quick Wins (Prompt Refinement):</li> <li>Clarify Ambiguous Wording: Update prompts with explicit language or disambiguating examples to address LLM misunderstandings.</li> <li>Add a Few Examples: Include 2-3 representative input/output pairs targeting observed failure cases (distinct from CI test cases) to guide the model.</li> <li>Use Role-Based Guidance: Assign a persona (e.g., \u201cYou are a careful tax advisor...\u201d) to steer tone and reasoning for open-ended tasks.</li> <li>Ask for Step-by-Step Reasoning: For logical or multi-step tasks, instruct the LLM to \u201cthink step by step\u201d to improve correctness and completeness.</li> <li>Structural Changes:</li> <li>Break Task into Smaller Steps: Decompose complex tasks into a sequence of smaller LLM calls for targeted prompts and easier error isolation (e.g., extracting preferences, querying a database, filtering results, generating a message).</li> <li>Tune RAG Steps: Enhance retrieval quality by reworking the retriever query, adjusting document chunking, changing the number of retrieved chunks, or adding a re-ranking step.</li> <li>Fix Tool Misuse: Clarify tool prompt descriptions (e.g., acceptable values/formats) to address incorrect tool calls or argument errors.</li> <li>Add Lightweight Checks: Implement validations (e.g., regexes for malformed outputs, length constraints, blacklists) to catch common failure modes.</li> <li>Heavier Fixes:</li> <li>Fine-Tuning: For persistent issues in high-value tasks with hundreds of labeled examples, fine-tune the model to lock in improvements (slower, more expensive).</li> <li>Add Human Review Loops: For high-stakes or subtle failure modes, build lightweight interfaces for regular human review of sampled outputs to inform fine-tuning datasets.</li> <li>Prompt Optimization: Treat prompts as tunable artifacts, testing variations against golden sets using formal techniques (e.g., grid search, reinforcement learning) after exhausting other methods.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#quick-wins-and-best-practices-for-cost-reduction","title":"Quick Wins and Best Practices for Cost Reduction","text":"<p>Cost optimization delivers the same quality more efficiently with low-effort strategies.</p> <ul> <li>Tiered Model Selection:</li> <li>Assess task requirements within the pipeline.</li> <li>Use smaller, faster, cheaper models for simple, high-volume tasks (e.g., text classification, data extraction, intent recognition).</li> <li>Reserve premium models for tasks requiring sophisticated reasoning or high-stakes accuracy (e.g., routing complex chatbot queries to a more expensive model).</li> <li>Divide and Conquer with Task Decomposition:</li> <li>Break workflows into sub-tasks, assigning cheaper models to intermediate stages.</li> <li>For RAG, use a cost-effective embedding model for retrieval, a fast LLM or re-ranker for filtering, and a capable model only for final answer synthesis to reduce tokens sent to expensive models.</li> <li>Cutting Tokens in Prompts:</li> <li>Optimize Input/Prompt Tokens: Use concise instructions and tune the number of retrieved documents in RAG.</li> <li>Pre-process Inputs into Summaries: Use a cheaper model to summarize long documents for subsequent calls.</li> <li>Optimize Output Tokens: Instruct the LLM to be concise or use compact formats like YAML instead of prose or JSON to reduce generated tokens.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#leveraging-llm-provider-caching","title":"Leveraging LLM Provider Caching","text":"<p>Many LLM API providers use caching to reduce latency and cost by reusing intermediate key/value (KV) states.</p> <ul> <li>Mechanism: Reuses cached KV states for prompts starting with the same token sequence (prefix), computing states only for new suffixes.</li> <li>Maximizing Cache Reuse: Place constant instructions before user-provided or changing data and maintain consistent formatting/phrasing across requests.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#advanced-strategy-implementing-model-cascades","title":"Advanced Strategy: Implementing Model Cascades","text":"<p>Model cascades approximate the accuracy of a trusted, expensive \u201coracle\u201d model at lower cost using a cheaper \u201cproxy\u201d model.</p> <ul> <li>Workflow: The proxy model attempts to answer a query. If confident, its answer is returned; otherwise, the query escalates to the oracle model.</li> <li>Confidence Quantification: Use log probabilities from LLM APIs to estimate confidence. For classification tasks, convert logprobs to normalized probabilities. For generative tasks, use heuristics like average log probability per token or total sequence log probability, requiring experimentation.</li> <li>Building and Tuning the Cascade: Set confidence thresholds for the proxy model, tuning separate thresholds for classification classes or a global threshold for generative tasks. Evaluate proxy predictions against oracle outputs using a binary scoring function (e.g., LLM-as-Judge checking if proxy output matches oracle information). Aim for a desired match rate with the oracle while maximizing proxy usage. Extend to multi-stage cascades with models ordered by cost.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/01-learning-path/10-imp-cost-optimzation/#summary","title":"Summary","text":"<p>The Improve phase is a continuous process covering accuracy and cost optimization. Accuracy improvements start with prompt refinement (clarifying wording, adding examples, role-based guidance, step-by-step reasoning), progress to structural changes (task decomposition, RAG tuning, tool fixes, lightweight checks), and may require heavier fixes (fine-tuning, human review loops, prompt optimization). Cost optimization includes tiered model selection, task decomposition, token reduction, caching, and model cascades. All improvements rely on robust evaluation and monitoring, with successful changes integrated into the CI pipeline to ensure persistence. This iterative process adapts to changes in models, data, and use cases, maintaining reliable LLM systems.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/","title":"\ud83d\udd0d GitHub Copilot Evaluation &amp; Error Analysis Insights","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-links","title":"\ud83d\ude80 Quick Links","text":"<ul> <li>GitHub Copilot: Evaluation Strategies and Insights</li> <li>Error Analysis: Building Custom Data Annotation Apps</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#github-copilot-evaluation-strategies-and-insights","title":"\ud83e\udde0 GitHub Copilot: Evaluation Strategies and Insights","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#video-reference","title":"\ud83c\udfa5 Video Reference","text":"<p>Evaluation Strategies for GitHub Copilot (YouTube)</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-navigation","title":"\ud83d\udd0e Quick Navigation","text":"<ul> <li>Introduction: Importance of Evaluations</li> <li>Categories of Evaluation</li> <li>Harness Lib: Offline Verifiable Evaluation</li> <li>AB Testing for Shipping Changes</li> <li>LLM as Judge for Subjective Evaluation</li> <li>Key Learnings and Meta Insights</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#introduction-importance-of-evaluations","title":"Introduction: Importance of Evaluations","text":"<p>GitHub Copilot evolved from a basic code completer to a robust AI-assisted coding tool through rigorous evaluations. Originally met with skepticism, it matured as evaluations offered strong, consistent signals to engineers. Evaluations became the foundation for iterative product improvement and were seen as essential for shaping Copilot's development direction.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#categories-of-evaluation","title":"Categories of Evaluation","text":"<p>Evaluations at GitHub Copilot spanned a gradient from purely programmatic to deeply subjective:</p> <ul> <li>Algorithmic: Validate outputs using schema rules, length checks, or compilation tests.</li> <li>Verifiable: Ensure code behaves correctly through real executions and test validations.</li> <li>LLM as Judge: Use LLMs for qualitative judgments when human review doesn't scale.</li> <li>AB Testing: Industry-standard user-based feedback loop for real-world performance.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#harness-lib-offline-verifiable-evaluation","title":"Harness Lib: Offline Verifiable Evaluation","text":"<p>A modular library used for offline testing code completions:</p> <ul> <li>Test Sample Pipeline:</li> <li>Filtered high-quality open-source repos.</li> <li>Extracted functions tied to passing unit tests.</li> <li> <p>Generated completions and validated with preexisting unit tests.</p> </li> <li> <p>Key Learnings:</p> </li> <li>Avoided training data contamination by working closely with OpenAI.</li> <li>Balanced subjective realism (production-like traffic) and flexibility.</li> <li>Offered both notebook experimentation and pipeline automation.</li> <li>Highlighted the need for modularity and caching.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#ab-testing-for-shipping-changes","title":"AB Testing for Shipping Changes","text":"<p>AB testing played a vital role in deploying updates gradually:</p> <ul> <li>Key Metrics:</li> <li>Completion acceptance rate.</li> <li>Characters retained post-edit.</li> <li> <p>Latency.</p> </li> <li> <p>Guardrail Metrics:</p> </li> <li> <p>Dozens of secondary diagnostics to detect anomalies.</p> </li> <li> <p>Tradeoffs:</p> </li> <li>Maximizing one metric could compromise another.</li> <li>Required detective work and robust data science analysis.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#llm-as-judge-for-subjective-evaluation","title":"LLM as Judge for Subjective Evaluation","text":"<p>As Copilot Chat emerged, subjective evaluation grew vital:</p> <ul> <li>Challenges:</li> <li> <p>Judging conversation usefulness and productivity, not just output.</p> </li> <li> <p>Evolution:</p> </li> <li>Started with manual comparison.</li> <li>GPT-4 initially failed with vague rubrics.</li> <li> <p>Bullet-point grading criteria improved consistency.</p> </li> <li> <p>Outcome:</p> </li> <li>LLMs became scalable judges for non-code tasks.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#key-learnings-and-meta-insights","title":"Key Learnings and Meta Insights","text":"<ul> <li>Two Personas of Eval:</li> <li>Regression-safe engineers.</li> <li> <p>Experimental prototype builders.</p> </li> <li> <p>Timing Matters:</p> </li> <li> <p>\"Vibe checks\" early on, structured evals after prototype stabilization.</p> </li> <li> <p>Surprises:</p> </li> <li>Users prefer shorter completions.</li> <li> <p>Many guardrail metrics were born from past failures.</p> </li> <li> <p>Ultimate Insight:</p> </li> <li>Evals are tools for building, not blockers to innovation.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#error-analysis-building-custom-data-annotation-apps","title":"\ud83e\uddea Error Analysis: Building Custom Data Annotation Apps","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#video-reference_1","title":"\ud83c\udfa5 Video Reference","text":"<p>Error Analysis Workshop with Hamel Husain (YouTube)</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#quick-navigation_1","title":"\ud83d\udd0e Quick Navigation","text":"<ul> <li>Motivation for Error Analysis</li> <li>What is Error Analysis?</li> <li>Multi-Stage Process</li> <li>Case Study: AI Email Recruiter</li> <li>Importance of Custom Annotation Tools</li> <li>Annotation Best Practices</li> <li>From Notes to Eval Metrics</li> <li>Key Takeaways</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#motivation-for-error-analysis","title":"Motivation for Error Analysis","text":"<p>Experts argue that manual inspection of model outputs is the highest ROI activity in AI product development. Focusing on the data\u2014rather than tools or frameworks\u2014prevents many early pitfalls. Observing raw outputs reveals patterns faster than any metric or automation.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#what-is-error-analysis","title":"What is Error Analysis?","text":"<p>A practical and iterative technique for dealing with errors in AI systems. It:</p> <ul> <li>Guides where to invest engineering time.</li> <li>Helps convert \"bad vibes\" into actionable fixes.</li> <li>Informs what to measure and track over time.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#multi-stage-process","title":"Multi-Stage Process","text":"<ol> <li>Manual Review: Inspect model outputs and jot down observations.</li> <li>Categorization: Group similar issues, potentially with LLM support.</li> <li>Pivot Table: Map outputs to issue categories and count them.</li> <li>Iterative Fixes: Adjust prompts, models, or frameworks. Create new metrics if needed.</li> </ol> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#case-study-ai-email-recruiter","title":"Case Study: AI Email Recruiter","text":"<p>Using Llama 370B, emails to candidates often sounded off:</p> <ul> <li>Sounded like rejections unintentionally.</li> <li>Confused sender/recipient.</li> <li>Referenced irrelevant resume info.</li> <li>Contained generic, verbose language.</li> </ul> <p>Without error analysis, prompt changes led to new issues\u2014a \"whack-a-mole\" problem.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#importance-of-custom-annotation-tools","title":"Importance of Custom Annotation Tools","text":"<p>Generic tools slow the process. Instead:</p> <ul> <li>Build interfaces tailored to your data type.</li> <li>Render natural formats (emails, plots, code).</li> <li>Add:</li> <li>Open-ended feedback box.</li> <li>Hotkeys and navigation.</li> <li>Save/export options.</li> </ul> <p>Fast to build with AI coding tools; often &lt;10 mins.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#annotation-best-practices","title":"Annotation Best Practices","text":"<ul> <li>Start Free-form: Don't classify too early.</li> <li>Manual is Mandatory: Don\u2019t LLM your first pass.</li> <li>Record Critical Feedback: Look for high-severity problems early.</li> <li>Use Voice Transcription if needed.</li> <li>Stop When You Stop Learning: Not at a fixed number.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#from-notes-to-eval-metrics","title":"From Notes to Eval Metrics","text":"<ul> <li>Convert notes into failure modes.</li> <li>Build a pivot table to prioritize fixes.</li> <li>Develop concrete, domain-specific metrics.</li> <li>Decide whether it's a retrieval or prompt issue.</li> <li>Use LLMs to scale evaluations post-categorization.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/01-youtube/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Build your own annotation UI.</li> <li>Delay categorization\u2014start with free-form notes.</li> <li>Keep annotating until discoveries taper off.</li> <li>Error analysis is the foundation of model improvement.</li> <li>It's the highest ROI task in AI development.</li> </ol> <p>\ud83d\udd1d Back to top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/","title":"AI Research Papers and Insights Guide","text":""},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Shankar &amp; Sim (2024/2025) \u2013 Generative AI on the Loose</li> <li>Shankar et al. (2024d) \u2013 Clarification</li> <li>Bommasani et al. (2021) \u2013 Foundation Models</li> <li>Zaharia et al. (2024) \u2013 Compound AI Systems</li> <li>Ward and Feldstein (2024) \u2013 Status Analysis</li> <li>Steven Feldstein \u2013 AI Governance &amp; Surveillance</li> <li>Francis Rhys Ward (2025) \u2013 AI Personhood Theory</li> <li>Lian et al. (2025) \u2013 Generative Foundation Models Handbook</li> <li>Hendrycks et al. (2021) \u2013 Unsolved Problems in ML Safety</li> <li>Rein et al. (2024) \u2013 Status Relevant Findings</li> <li>Jain et al. (2024/2025) \u2013 AI in Writing</li> <li>Shobhit Jain et al. (2025) \u2013 Medical Chatbot using GenAI</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#introduction","title":"Introduction","text":"<p>This guide summarizes key research papers and insights in generative AI, large language models (LLMs), and related fields, focusing on their contributions to AI development, safety, ethics, and applications. The selected works span foundational concepts, system architectures, governance, and domain-specific implementations, providing a comprehensive overview for researchers and practitioners.</p> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#shankar-sim-20242025-generative-ai-on-the-loose","title":"Shankar &amp; Sim (2024/2025) \u2013 Generative AI on the Loose","text":"<p>Generative AI on the Loose: Impact of Improved AI and Expert Oversight on Knowledge Sharing</p> <ul> <li>Overview: An empirical study analyzing the interplay between advanced GenAI systems (e.g., GPT-4) and expert moderation on platforms like Stack Overflow.</li> <li>Core Findings:</li> <li>Combining high-performing GenAI with strict expert oversight reduces content volume but improves quality and reliability.</li> <li>Relaxed moderation leads to a surge in lower-quality or hallucinated content.</li> <li>Novice users benefit most from expert moderation for quality maintenance.</li> <li>Methodology: Natural experiment around GPT-4 adoption and Stack Overflow policy changes, using mixed methods to measure content volume and quality across user skill tiers.</li> <li>Implications: GenAI requires moderation for quality; platform design (e.g., moderation policies) significantly impacts content trustworthiness.</li> <li>Reference: Shankar &amp; Sim (June 2024 / Feb 2025 revision), SSRN: Generative AI on the Loose</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#shankar-et-al-2024d-clarification","title":"Shankar et al. (2024d) \u2013 Clarification","text":"<p>No separate publication titled Shankar et al. (2024d) is indexed in generative AI or AI safety literature as of July 2025. It likely refers to: - The fourth revision of Shankar &amp; Sim (2024/2025), \u201cGenerative AI on the Loose.\u201d - An unpublished or internal work not yet public.</p> <ul> <li>Known Public Record: The Shankar &amp; Sim (2024/2025) SSRN paper is the most likely source.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#bommasani-et-al-2021-foundation-models","title":"Bommasani et al. (2021) \u2013 Foundation Models","text":"<p>On the Opportunities and Risks of Foundation Models</p> <ul> <li>Overview: A landmark Stanford report introducing foundation models\u2014large-scale models pretrained via self-supervised learning, adaptable for downstream tasks.</li> <li>Key Contributions:</li> <li>Defines foundation models (e.g., BERT, GPT-3, CLIP).</li> <li>Highlights emergent behaviors and transfer capabilities across modalities.</li> <li>Warns of homogenization risks from shared base models.</li> <li>Urges interdisciplinary study across ethics, policy, and technical design.</li> <li>Risks Highlighted:</li> <li>Model biases and societal inequities.</li> <li>Lack of interpretability and safety mechanisms.</li> <li>Environmental costs of training.</li> <li>Over-centralization in deployment and research.</li> <li>Use Case Domains: Education, healthcare, legal systems, human-AI interaction.</li> <li>Reference: Bommasani et al., 2021 (arXiv)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#zaharia-et-al-2024-compound-ai-systems","title":"Zaharia et al. (2024) \u2013 Compound AI Systems","text":"<p>Compound AI Systems</p> <ul> <li>Overview: Presents a framework for building compound AI systems\u2014orchestrating LLMs, retrievers, tools, and planners for complex workflows beyond single-model prompting.</li> <li>Core Architecture:</li> <li>Agents &amp; Registries: Interface with external tools, databases, models.</li> <li>Streams: Data and instruction flow between components.</li> <li>Planners: Optimize task decomposition and routing.</li> <li>Key Advantages:</li> <li>Enhances flexibility and modularity.</li> <li>Reduces reliance on monolithic scaling.</li> <li>Enables faster iteration and lower inference costs.</li> <li>Real-World Impacts: Used in tool-augmented LLM agents, retrieval-augmented generation (RAG), and real-time business pipelines.</li> <li>Reference: Zaharia et al., 2024 (arXiv)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#ward-and-feldstein-2024-status-analysis","title":"Ward and Feldstein (2024) \u2013 Status Analysis","text":"<p>No direct publication titled Ward and Feldstein, 2024 is located in generative AI or LLM research as of July 2025. It may refer to works by Steven Feldstein or Francis Rhys Ward, or an unpublished/internal paper.</p> <ul> <li>Note: See sections on Steven Feldstein \u2013 AI Governance &amp; Surveillance and Francis Rhys Ward (2025) \u2013 AI Personhood Theory for related works. Provide more context or keywords to pinpoint the correct reference.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#steven-feldstein-ai-governance-surveillance","title":"Steven Feldstein \u2013 AI Governance &amp; Surveillance","text":"<p>AI Governance &amp; Surveillance</p> <ul> <li>Overview: Steven Feldstein\u2019s research focuses on AI\u2019s global political implications, particularly surveillance technologies and digital repression.</li> <li>Key Concepts:</li> <li>Impact on authoritarian governance and digital human rights.</li> <li>Role of international regulations and national policies in curbing abuse.</li> <li>Ethical limits of algorithmic surveillance and social scoring.</li> <li>Related Reading:</li> <li>Brookings Institution - AI and Digital Authoritarianism</li> <li>Journal articles on AI governance and policy</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#francis-rhys-ward-2025-ai-personhood-theory","title":"Francis Rhys Ward (2025) \u2013 AI Personhood Theory","text":"<p>Towards a Theory of AI Personhood</p> <ul> <li>Overview: Explores philosophical and legal frameworks for AI personhood, questioning when AI models might be treated as entities with moral/legal standing.</li> <li>Key Topics:</li> <li>Criteria for personhood: autonomy, sentience, continuity, language competence.</li> <li>Consequences of attributing agency to AI systems.</li> <li>Alignment issues if LLMs satisfy moral responsibility criteria.</li> <li>Discussion: Raises questions for AGI safety, alignment research, and AI ethics in legal systems.</li> <li>Reference: Ward, F.R., 2025 (arXiv)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#lian-et-al-2025-generative-foundation-models-handbook","title":"Lian et al. (2025) \u2013 Generative Foundation Models Handbook","text":"<p>Generative Foundation Models: A Comprehensive Beginner\u2019s Handbook</p> <ul> <li>Overview: A hands-on exploration of generative foundation models, covering theoretical foundations and practical implementations (published May 2025 on SSRN, often referred to as 2023 in drafts).</li> <li>Covered Architectures:</li> <li>Transformer for sequence modeling.</li> <li>Vision Transformer (ViT) for image representation.</li> <li>Mamba for linear-time modeling using state-space methods.</li> <li>U-Net for image synthesis.</li> <li>Denoising Diffusion Probabilistic Models (DDPMs).</li> <li>Diffusion Transformer (DiT) for transformer-based diffusion.</li> <li>Retentive Network (RetNet) for attention-free sequence modeling.</li> <li>Latent Diffusion Models (LDMs) for high-resolution autoencoding.</li> <li>Text-to-3D techniques (e.g., DreamFusion, Magic3D).</li> <li>Features &amp; Format:</li> <li>Intuitive overviews with rigorous mathematical formulations.</li> <li>PyTorch-style pseudo-code for each architecture.</li> <li>Consistent notation (e.g., bold vectors, italic scalars).</li> <li>Reflection questions per section for deeper understanding.</li> <li>Why It Matters:</li> <li>Unified learning path for generative model families.</li> <li>Bridges language, image, and 3D generation.</li> <li>Ideal for AI learners, engineers, and researchers.</li> <li>Reference: Lian, J.J., 2025, SSRN</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#hendrycks-et-al-2021-unsolved-problems-in-ml-safety","title":"Hendrycks et al. (2021) \u2013 Unsolved Problems in ML Safety","text":"<p>Unsolved Problems in ML Safety</p> <ul> <li>Overview: A foundational paper establishing a technical roadmap for ML safety research, focusing on challenges posed by large-scale models.</li> <li>Core Safety Research Areas:</li> <li>Robustness: Reliable behavior under adversarial/unforeseen conditions.</li> <li>Monitoring: Early detection of anomalies, misuse, or unintended behaviors.</li> <li>Alignment: Designing models adhering to human values.</li> <li>Systemic Safety: Addressing risks from deployment contexts and organizational failures.</li> <li>Motivation &amp; Impact:</li> <li>Highlights safety gaps as ML systems grow more capable.</li> <li>Provides concrete research directions.</li> <li>Informs subsequent safety frameworks and organizations (e.g., Center for AI Safety).</li> <li>Related Research:</li> <li>An Overview of Catastrophic AI Risks (Hendrycks et al., 2023): Categorizes risks (malicious use, AI race dynamics, organizational failures, rogue systems).</li> <li>ETHICS Benchmark (ICLR 2021): Aligning AI with human values via commonsense ethics and fairness.</li> <li>Reference: Hendrycks et al., 2021 (arXiv)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#rein-et-al-2024-status-relevant-findings","title":"Rein et al. (2024) \u2013 Status Relevant Findings","text":"<p>No standalone paper titled Rein et al., 2024 is indexed in LLM or generative AI literature as of July 2025. Likely references include:</p> <ul> <li>REIN (CVPR 2024) \u2013 Efficient Fine-Tuning of Vision Foundation Models:</li> <li>Overview: Introduces REIN, a parameter-efficient fine-tuning technique for Vision Foundation Models (e.g., ConvNeXt, DINOv2) for semantic segmentation.</li> <li>Key Features:<ul> <li>Uses learnable instance tokens for minimal training overhead (&lt;1% additional parameters).</li> <li>Strong generalization across domains (e.g., Cityscapes, Cross-Organ).</li> <li>Scalable adaptation without retraining base encoders.</li> </ul> </li> <li>Reference: Wei et al., 2024 (arXiv)</li> <li>Rein et al., 2023 (Benchmark Reference):</li> <li>Cited in benchmark papers (e.g., GPQA, MMLU-X) for expert-level generalization and metric formulation, possibly an unpublished draft or internal suite.</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#jain-et-al-20242025-ai-in-writing","title":"Jain et al. (2024/2025) \u2013 AI in Writing","text":"<p>Generative AI in Writing Research Papers</p> <ul> <li>Overview: Investigates how generative AI tools (e.g., GPT) impact scholarly writing, introducing new algorithmic biases and concerns about authorship, accuracy, and academic integrity.</li> <li>Key Points:</li> <li>Generative text may introduce fabricated references and unverifiable claims.</li> <li>Questions whether GenAI-assisted papers require disclosure or rejection.</li> <li>Discusses uncertainty in peer review systems caused by LLMs.</li> <li>Reference: R. Jain &amp; A. Jain, 2024 (ResearchGate)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/02-additional-resource/02-research-papers/#shobhit-jain-et-al-2025-medical-chatbot-using-genai","title":"Shobhit Jain et al. (2025) \u2013 Medical Chatbot using GenAI","text":"<p>Medical Chatbot using GenAI</p> <ul> <li>Overview: Designs and evaluates a domain-specific medical chatbot powered by generative AI, emphasizing ethical boundaries, hallucination reduction, and prompt conditioning in healthcare.</li> <li>Relevance:</li> <li>Domain-specific fine-tuning and knowledge integration.</li> <li>Early evaluation of chatbot reliability in critical use cases.</li> <li>Reference: Jain et al., 2025 (ResearchGate)</li> </ul> <p>Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/","title":"Expert Q&amp;A: Evaluation, Labeling, Tracing, and Prompt Iteration in LLM Systems","text":""},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#quick-navigation","title":"\ud83d\udd17 Quick Navigation","text":"<ul> <li>Reconciling Scores &amp; Rubrics</li> <li>AWS vs Notebooks for Prototyping</li> <li>User Engagement Predictions &amp; Evaluation Strategy</li> <li>Error Analysis: End-to-End or Step-by-Step?</li> <li>Universal Tracing &amp; Logging Standards</li> <li>Multiple SMEs &amp; Sampling Strategies</li> <li>Fine-Tuning Tradeoffs &amp; Recursive Judge Loop</li> <li>Iterative Prompt Optimization</li> <li>Debugging Long, Complex Agent Traces</li> <li>Fix Now or Finish Axial Coding First?</li> <li>Markdown Extraction from PDFs/Scans</li> <li>LLM Judge Benefits &amp; Prompt Suggestions</li> <li>Origin of Evaluation Frameworks</li> </ul>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-reconciling-scores-rubrics","title":"Q: Reconciling Scores &amp; Rubrics","text":"<p>How do you reconcile having concrete scores with iteratively improving the rubric during inter-annotator discussions... Does a low Cohen's Kappa signal a rubric issue?</p> <p>A: - Low Cohen\u2019s Kappa often signals poor rubric clarity or misaligned annotator understanding. - Disagreement might stem from large surface area or non-expert annotation. - High-enough agreement is acceptable; perfect alignment is not required and often too costly. - The goal is shared understanding, not perfection.</p> <p>\ud83d\udcda Reference: - Cohen\u2019s Kappa Explained \u2013 StatQuest</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-aws-vs-notebooks-for-prototyping","title":"Q: AWS vs Notebooks for Prototyping","text":"<p>Should you fight through AWS Bedrock complexity or use a notebook to keep learning momentum?</p> <p>A: - Use notebooks to keep progressing. - AWS is complex\u2014even experienced engineers struggle with it. - You can move to deployment later.</p> <p>\ud83d\udcda Reference: - Why AWS is hard \u2013 Forrest Brazeal</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-user-engagement-prediction","title":"Q: User Engagement Prediction","text":"<p>How to build binary evaluations around response likelihood? Should you begin with error analysis?</p> <p>A: - Yes\u2014start with error analysis. - It\u2019s the most impactful way to guide what you build and test. - Use it to balance unit tests vs trace-level analysis.</p> <p>\ud83d\udcda Reference: - Open Coding in Practice (YouTube)</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-error-analysis-end-to-end","title":"Q: Error Analysis: End-to-End","text":"<p>Should error analysis be trace-level rather than on every single LLM step?</p> <p>A: - Yes, review trace start and end. - If end is good, skip; if bad, find the first error, code it, and move on. - Keeps analysis tractable and fast.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-universal-tracing-logging","title":"Q: Universal Tracing &amp; Logging","text":"<p>Is it worth building a universal trace logger? Are logging standards emerging?</p> <p>A: - Python/TypeScript dominate vendor SDKs. - Vendor lock-in is hard to avoid. - OpenTelemetry helps, but homegrown S3 + custom annotation interfaces are valid. - No robust standards exist for LLM-specific logging yet.</p> <p>\ud83d\udcda Reference: - Phoenix Tracing OSS - OpenTelemetry Tracing</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-multiple-smes-sampling","title":"Q: Multiple SMEs &amp; Sampling","text":"<p>Is it OK to have many SMEs across regions/products? What sampling strategy is best?</p> <p>A: - Yes, as long as scopes are clear. - Appoint a \"benevolent dictator\" per dimension. - Start with random sampling, then move to stratified and exploratory methods.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-fine-tuning-tradeoffs","title":"Q: Fine-Tuning Tradeoffs","text":"<p>When does fine-tuning stop being worth it? Can the LLM judge be fine-tuned for a recursive loop?</p> <p>A: - Fine-tuning can help with specific structural/stylistic tasks. - But model updates (e.g., GPT-4.1) can make your fine-tune obsolete quickly. - Judge fine-tuning is valid\u2014but you may need to tune both model and judge.</p> <p>\ud83d\udcda Reference: - Anthropic on Model Evaluation</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-prompt-iteration-challenges","title":"Q: Prompt Iteration Challenges","text":"<p>How do you fix one part of a prompt without breaking others?</p> <p>A: - Split into smaller sub-agents if needed. - If all else fails, try a more powerful model. - Fine-tuning is a last resort if prompts are already very clean.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-debugging-agent-traces","title":"Q: Debugging Agent Traces","text":"<p>How do you debug long-agent workflows with long delays (e.g., 30 min execution)?</p> <p>A: - Use analytics + test harnesses. - Gather a few failing examples and iterate. - Use guardrails to realign the agent mid-process. - Start with one agent; add more only if needed.</p> <p>\ud83d\udcda Reference: - GitHub Copilot Eval Strategy \u2013 YouTube</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-quick-fixes-vs-evals","title":"Q: Quick Fixes vs Evals","text":"<p>Should you immediately fix obvious issues or wait until axial coding is complete?</p> <p>A: - Fix obvious and high-impact errors immediately. - Use evals for more complex, nuanced issues. - Not all problems need a full judge or eval cycle.</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-markdown-from-pdfs","title":"Q: Markdown from PDFs","text":"<p>If using a library to convert documents to Markdown, when should LLMs be involved?</p> <p>A: - For deterministic format conversion, don\u2019t use LLMs. - For nuanced interpretation (e.g., scanned docs), LLMs may help. - Write evals + iterate prompts if using LLMs for this.</p> <p>\ud83d\udcda Reference: - pdf2markdown Tools \u2013 GitHub</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-llm-judge-time-saving","title":"Q: LLM Judge Time Saving","text":"<p>What gains come from using an LLM judge, even with imperfect alignment? Do prompt suggestions apply to task or judge prompts?</p> <p>A: - Even \u201cpretty good\u201d judges save time and provide feedback loops. - Use disagreement as fuel for prompting improvements. - Prompt suggestions can improve either task or judge prompts.</p> <p>\ud83d\udcda Reference: - AlignEval Project (GitHub) - Doc ETL \u2013 Evaluation Workflow</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"03-deploy-ops/7-ai-evals/03-q%26a/01-q%26a/#q-eval-framework-origins","title":"Q: Eval Framework Origins","text":"<p>How was the evaluation framework developed? How do you build such frameworks for new domains like agentic systems?</p> <p>A: - Frameworks are distilled from real-world consulting experiences and recurring failure points. - Combines dimensionality reduction + open coding. - The \"3 gulfs\" model was conceived by Shreya and refined through teaching and field work.</p> <p>\ud83d\udcda Reference: - Evaluation-Driven Development for Agentic Systems (Arawjo et al., 2024)</p> <p>\ud83d\udd1d Back to Top</p>"},{"location":"04-applied-research/","title":"\ud83e\uddea Applied Research","text":"<p>\ud83d\udcc2 Navigation Tip This section bridges theory with practical innovation. Use the left-hand menu to explore real-world use cases, peer-reviewed research papers, and frontier areas like Agentic AI.</p> <p>\ud83d\udca1 Suggested order: Begin with Use Cases for applied examples, dive into Research Papers for foundational studies, then explore Agentic AI for cutting-edge directions in autonomy and reasoning.</p>"},{"location":"04-applied-research/01-use-cases/","title":"\ud83e\udde0 Use Cases","text":"<p>Explore real-world implementations and advanced experiments in Generative AI. This section highlights applied scenarios, model behaviors, and evaluation learnings to inspire practical solutions and innovation.</p>"},{"location":"04-applied-research/01-use-cases/#example-projects","title":"\ud83d\ude80 Example Projects","text":"<p>These curated examples showcase real deployment challenges and applied GenAI workflows:</p> <ul> <li> <p>QR Code Diffusion Models   Use of generative diffusion models to encode and personalize QR visuals.</p> </li> <li> <p>RAG Evaluation Pitfalls   Challenges in Retrieval-Augmented Generation and lessons from evaluation metrics.</p> </li> </ul> <p>\ud83d\udcc2 Back to Applied Research Overview</p> <p>August 2, 2025</p> <p>Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/","title":"\ud83d\udce6 QR Code Generation Using Diffusion Models","text":""},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>\ud83e\udded Introduction</li> <li>\ud83d\udccc Key Points</li> <li>\ud83e\uddf5 Detailed Summary</li> <li>\ud83e\udde0 Lessons Learned</li> <li>\ud83d\udcda References</li> </ul>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#introduction","title":"\ud83e\udded Introduction","text":"<p>This transcript captures a detailed conversation between Hamel Husain and Charles Frye, highlighting the development and iterative improvement of a project centered around generating visually appealing and scannable QR codes using diffusion models.</p> <p>Charles, representing Modal, shares deep insights into overcoming technical challenges in QR code generation\u2014emphasizing the strategic role of evaluations (evals) and serverless compute scaling. This project stands as a practical case study in applying machine learning principles, evaluation frameworks, and cloud resources to deliver reliable generative model outputs.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#key-points","title":"\ud83d\udccc Key Points","text":"<ul> <li>\ud83c\udf00 Diffusion Models were used to generate QR codes that are both aesthetically pleasing and scannable.</li> <li>\ud83d\udd0d Early issues: QR codes looked good visually but failed to scan reliably.</li> <li>\ud83c\udfaf Operationalization of fuzzy terms like \"looks good\" and \"scans\" became a priority.</li> <li>\ud83e\udde0 A control net was added to the diffusion model to guide generation via brightness patterns.</li> <li>\ud83c\udf08 An aesthetic predictor model was used to rank the visual quality of QR codes.</li> <li>\ud83e\udd16 A QR code reader package automated the scanning evaluation process.</li> <li>\u2696\ufe0f Trade-off prioritization: Scanning reliability &gt; Aesthetics.</li> <li>\ud83d\uddbc\ufe0f Manual labeling of QR code images was performed to train/validate eval models.</li> <li>\ud83d\udc1e A critical bug in evaluation code was found and fixed\u2014it had been misclassifying non-scannable codes.</li> <li>\u2601\ufe0f Inference-time compute scaling boosted success rates during generation.</li> <li>\u2699\ufe0f Modal\u2019s serverless platform enabled efficient compute scaling for dev and deployment.</li> <li>\ud83d\udd01 Iterative process included multiple rounds of parameter tuning and evaluation.</li> <li>\u2705 Final system achieved 95% QR code scanning success, satisfying the SLO.</li> <li>\ud83e\uddea Evaluation timing tip: Implement evals during the second iteration, not the first.</li> <li>\ud83d\udcb8 Charles elaborates on trade-offs between cost, latency, and quality during scaling.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#detailed-summary","title":"\ud83e\uddf5 Detailed Summary","text":"<p>This case study dives into a project led by Charles Frye from Modal, which aimed to generate QR codes that are both visually appealing and functionally scannable using diffusion models. The project originally suffered from the common issue of aesthetic success without practical reliability, as the QR codes looked impressive but frequently failed to scan.</p> <p>To address this, Charles\u2019s team operationalized the goals of \u201clooks good\u201d and \u201cscans.\u201d They incorporated a control net into the diffusion process to guide image generation based on brightness patterns, preserving the QR code\u2019s structure while still allowing for visual customization.</p> <p>The team also implemented an aesthetic predictor model to rank visual outputs, and a QR reader tool to automate the scan-check process. Prioritization became a key engineering decision: they chose to optimize scanning reliability over subjective aesthetics, ensuring end-user functionality.</p> <p>During iterative development, a bug in the evaluation pipeline was discovered that had been misclassifying unscannable QR codes. Fixing this improved the quality of the evaluation system and helped the model generate reliably scannable outputs.</p> <p>To scale performance, they used inference-time compute scaling\u2014allocating more resources dynamically to boost the QR code success rate. This process was made seamless through Modal\u2019s serverless platform, which allowed rapid scaling without infrastructure overhead.</p> <p>The team conducted manual labeling of QR outputs to create high-quality evaluation datasets and fine-tuned model parameters across multiple rounds. As a result, they reached a 95% scan success rate, aligning with their defined Service Level Objective (SLO).</p> <p>Charles also shared strategic advice: introduce evaluation frameworks in the second iteration of a project, once core functionality is stable. He emphasized that early evaluations can be a distraction, whereas well-timed evals lead to actionable insights and performance improvements.</p> <p>He also outlined how to balance trade-offs: - \ud83d\udcb0 Cost: High compute = better outputs but higher billing. - \u23f1\ufe0f Latency: Real-time applications must minimize delay. - \ud83d\udcc8 Quality: Optimal outputs may require aggressive tuning and inference resources.</p> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#lessons-learned","title":"\ud83e\udde0 Lessons Learned","text":"<ul> <li>Define evaluation criteria early, but implement evals iteratively.</li> <li>Use automation for reliability checks (like QR readers).</li> <li>Don't neglect manual labeling\u2014it adds crucial ground truth.</li> <li>Use serverless compute platforms like Modal to avoid DevOps bottlenecks.</li> <li>Always prioritize end-user functionality over subjective output quality when needed.</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/qr-code-def-models/#references","title":"\ud83d\udcda References","text":"<ul> <li>Modal \u2014 Cloud platform for running ML workloads</li> <li>Diffusion Models \u2014 Illustrated Guide</li> <li>OpenCV QR Code Detection</li> <li>Aesthetic Prediction with CLIP</li> <li>\ud83c\udf99\ufe0f Maven Labs: Interview Recording (Charles Frye on QR Evals)</li> <li>\ud83d\uddbc\ufe0f Google Slides: Evaluation Strategy Slides</li> <li>\ud83c\udfa8 qart.codes \u2014 Artistic QR Code Generator (Open Source)</li> </ul> <p>\ud83d\udd1d Back to top</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/","title":"\ud83d\udcca 10x Your RAG Evaluation by Avoiding These Pitfalls","text":"<p>Key Topics: RAG Evaluation, Pitfall Avoidance, Engineering Practices, Indexing, Hallucinations</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Core Message</li> <li>2. Key Principle: Look at Your Data</li> <li>3. The 7 Pitfalls to Avoid</li> <li>4.1 Corpus Coverage</li> <li>4.2 Information Extraction</li> <li>4.3 Chunk Quality</li> <li>4.4 Query Rejection &amp; Elicitation</li> <li>4.5 Retrieval Sufficiency</li> <li>4.6 Hallucination &amp; Citation Verification</li> <li>4.7 Data Volatility &amp; Reproducibility</li> <li>5. Indexing Evaluation Tips</li> <li>6. Late Interaction (e.g., ColBERT)</li> <li>7. Final Thoughts</li> <li>8. References &amp; Tools</li> <li>\u2b06\ufe0f Back to top</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#1-core-message","title":"1. Core Message","text":"<p>\ud83d\udd25 \"Evaluate everything you can\u2014all the time\u2014independently if possible. And most importantly: look at your data.\" \u2013 Skylar Payne</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#2-key-principle-look-at-your-data","title":"2. Key Principle: Look at Your Data","text":"<p>AI and RAG systems often fail silently\u2014issues accumulate as technical debt. The only way to address these failures is to: - Build systematic evaluation into each RAG pipeline component - Avoid being blind to where problems originate - Empower debugging via component-level visibility</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#3-the-7-pitfalls-to-avoid","title":"3. The 7 Pitfalls to Avoid","text":""},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#41-corpus-coverage","title":"4.1 Corpus Coverage","text":"<ul> <li>Problem: You can't retrieve what's not documented.</li> <li>Fix: </li> <li>Maintain a representative query set.</li> <li>Measure % of queries returning relevant chunks.</li> <li>Audit your corpus for coverage gaps.</li> </ul> <p>\ud83d\udca1 Example: A dev support bot failed to answer \"Salesforce integration\" because no documentation existed, despite the feature being supported.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#42-information-extraction","title":"4.2 Information Extraction","text":"<ul> <li>Problem: Poor extraction from PDFs, code blocks, tables, or complex formats.</li> <li>Fix: </li> <li>Manually verify text extracted into chunks.</li> <li>Validate semantic structure: tables, headers, Markdown formats.</li> <li>Be cautious with OCR models or third-party converters.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#43-chunk-quality","title":"4.3 Chunk Quality","text":"<ul> <li>Problem: Over-chunked or under-chunked content harms retrieval and generation.</li> <li>Fix:</li> <li>Sample and review very large and very small chunks.</li> <li>Track retrieval frequency of chunks:<ul> <li>High frequency? \u2192 Consider putting in prompt directly.</li> <li>Never retrieved? \u2192 Irrelevant or corrupted.</li> </ul> </li> </ul> <p>\ud83d\udca1 Tip: You might not need chunking if pages are small\u2014feed entire documents.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#44-query-rejection-elicitation","title":"4.4 Query Rejection &amp; Elicitation","text":"<ul> <li>Problem: Garbage-in = garbage-out. Many queries are vague or malformed.</li> <li>Fix:</li> <li>Reject ambiguous queries (e.g., \u201cthing broken\u201d).</li> <li>Use elicitation: prompt users to clarify.</li> <li>Track rejection/clarification rates in evaluation.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#45-retrieval-sufficiency","title":"4.5 Retrieval Sufficiency","text":"<ul> <li>Problem: Classic IR metrics (e.g., NDCG, precision@10) don\u2019t assess if enough info is retrieved.</li> <li>Fix:</li> <li>Manually label if retrieved docs are sufficient to answer the query.</li> <li>Think like SAT reading tests: Answer only using the provided passage.</li> <li>Train an \u201cAI judge\u201d to scale this later.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#46-hallucination-citation-verification","title":"4.6 Hallucination &amp; Citation Verification","text":"<ul> <li>Problem: LLMs fill in gaps with hallucinated content.</li> <li>Fix:</li> <li>Enforce in-text citations in generation outputs.</li> <li>Validate:<ul> <li>Citation exists</li> <li>Citation supports the claim via semantic matching</li> </ul> </li> <li>Reference: Jason Liu on Semantic Validation</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#47-data-volatility-reproducibility","title":"4.7 Data Volatility &amp; Reproducibility","text":"<ul> <li>Problem: Indexes or external APIs silently change over time (e.g., Google Search limits).</li> <li>Fix:</li> <li>Add timestamps or versioning to indexed data.</li> <li>Support time travel to reproduce eval results.</li> <li>Ensure evaluation consistency over time.</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#5-indexing-evaluation-tips","title":"5. Indexing Evaluation Tips","text":"<p>Evaluate each step of the document \u2192 index pipeline:</p> Step What to Evaluate Extraction Ensure parsing correctness from PDF, JSON, etc. Chunking Chunks should preserve context and be meaningful Embedding Confirm vectors map semantically to document intent Retrieval Retrieval should bring back expected content for key queries <p>Best Practice: Start with end-to-end error analysis, then drill down.</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#6-late-interaction-eg-colbert","title":"6. Late Interaction (e.g., ColBERT)","text":"<p>Skylar\u2019s opinion: - Too complex for most use cases - BM25 + embedding search often performs well enough - When retrieval fails, it\u2019s often due to:   - Poor chunking   - Missing query terms   - Not because of scoring method</p>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#7-final-thoughts","title":"7. Final Thoughts","text":"<ul> <li>AI engineers already have 80% of the skills\u2014just need guidance to cross the finish line.</li> <li>Treat evaluation like software QA:</li> <li>Add observability, logging, unit-like evals</li> <li>Focus on data quality and visibility over clever model tweaks</li> </ul>"},{"location":"04-applied-research/01-use-cases/rag-evals-avoid-model-pitfall/#8-references-tools","title":"8. References &amp; Tools","text":"<ul> <li>\ud83d\udd17 ML Tech Debt Paper (Google, 2015)</li> <li>\ud83d\udcc4 InstructorHQ Semantic Validation \u2013 Jason Liu</li> <li>\ud83d\udce6 Quarto \u2014 Tool Skylar used to create the slides</li> <li>\ud83d\udcd8 ColBERT Late Interaction (Not recommended unless justified)</li> </ul> <p>\u2b06\ufe0f Back to top</p>"},{"location":"04-applied-research/02-research-papers/","title":"Gen AI Atlas \u2014 Navigation Overview","text":"<p>Welcome to Gen AI Atlas! Explore the sections below to access core learning paths, advanced topics, and community resources.</p>"},{"location":"04-applied-research/02-research-papers/#foundations","title":"\ud83c\udfd7\ufe0f Foundations","text":"<p>Build your essential knowledge with modules in:</p> <ul> <li>Statistics</li> <li>Python Programming</li> <li>Data Science</li> </ul> <p>These resources provide the groundwork for all modern AI and data workflows.</p>"},{"location":"04-applied-research/02-research-papers/#generative-ai-core","title":"\ud83e\udd16 Generative AI Core","text":"<p>Deep dive into the world of generative models and natural language processing:</p> <ul> <li>Transformers &amp; Large Language Models (LLMs)</li> <li>Prompt Engineering</li> <li>NLP Techniques</li> </ul> <p>Master the technologies at the heart of today\u2019s AI revolution.</p>"},{"location":"04-applied-research/02-research-papers/#deployment-operations","title":"\ud83d\ude80 Deployment &amp; Operations","text":"<p>Go from research to production with real-world tools:</p> <ul> <li>LangChain</li> <li>LLMOps</li> <li>AI Evaluation (Evals)</li> </ul> <p>Learn best practices for building, deploying, and managing robust AI systems.</p>"},{"location":"04-applied-research/02-research-papers/#research-advanced-topics","title":"\ud83d\udd2c Research &amp; Advanced Topics","text":"<p>Stay ahead with explorations into:</p> <ul> <li>Cutting-edge Research</li> <li>Agentic AI</li> <li>Retrieval-Augmented Generation (RAG)</li> <li>Applied Use Cases</li> </ul> <p>Advance your skills and contribute to the future of AI.</p>"},{"location":"04-applied-research/02-research-papers/#community-qa","title":"\ud83c\udf10 Community &amp; Q&amp;A","text":"<p>Connect, share, and learn with the Gen AI Atlas community:</p> <ul> <li>Discussion Boards</li> <li>Blog Posts</li> <li>Q&amp;A Forums</li> </ul> <p>Find support, share insights, and grow your network.</p> <p>Navigate using the menu above to start your Gen AI journey!</p>"},{"location":"04-applied-research/03-agentic-ai/","title":"\ud83e\udd16 Agentic AI","text":"<p>Discover how intelligent agents can reason, act, and learn autonomously in complex environments. This section explores frameworks, evaluation methods, and applied systems for building agentic behaviors in generative AI.</p>"},{"location":"04-applied-research/03-agentic-ai/#evaluation-driven-development","title":"\u2699\ufe0f Evaluation-Driven Development","text":"<p>Explore how continuous evaluation informs the design of agentic workflows\u2014highlighting trace-based feedback loops, rubric iteration, and human-in-the-loop analysis.</p> <p>\ud83d\udcc2 Back to Applied Research Overview</p> <p>August 2, 2025</p> <p>Back to top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/","title":"Evaluation Driven Development","text":""},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#evaluation-driven-development-for-agentic-systems","title":"Evaluation Driven Development for Agentic Systems","text":"<p>The development process for Agentic Systems, particularly those based on Large Language Models (LLMs), is a continuous, iterative loop that prioritizes evaluation and feedback for successful evolution . This system aims to avoid common pitfalls by structuring the entire lifecycle from idea to production .</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Defining The Problem</li> <li>2. Building a Prototype</li> <li>3. Defining Performance Metrics</li> <li>4. Defining Evaluation Rules</li> <li>5. Building a Proof of Concept (PoC)</li> <li>6. Instrumenting the Application (Observability)</li> <li>7. Integrating with an Observability Platform</li> <li>8. Evaluating Traced Data</li> <li>9. Evolving the Application</li> <li>10. Exposing New Versions of the Application</li> <li>11. Continuous Development and Evolution of the Application</li> <li>12. Monitoring and Alerting</li> </ul>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#1-defining-the-problem","title":"1. Defining The Problem","text":"<p>This is the initial and vital step for any Agentic System development . *   Goal: Ensure the problem is clearly defined, bounded, and aligned with business goals . Agentic Systems use LLMs or other GenAI models to solve complex, real-world problems, often involving automation . *   Key Questions:     *   Is the problem best solved by AI or traditional software?      *   Who is the end user?      *   What are the edge cases?      *   What are the boundaries of acceptable behavior?  *   Important: Many AI projects fail not due to bad models, but due to solving the wrong problem . *   Roles Involved: AI Product Managers, Domain Experts, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#visual-reference","title":"\ud83d\udcca Visual Reference","text":""},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#2-building-a-prototype","title":"2. Building a Prototype","text":"<p>After confirming AI is a good fit, the next step is rapid prototyping . *   Goal: Primarily a learning phase to assess technical feasibility and de-risk the idea . *   Key Considerations:     *   Use Notebooks or no-code tools, small datasets, and off-the-shelf models .     *   Focus on learning, not initial performance .     *   Document everything to avoid repeating mistakes .     *   Involve prompt engineering and market research for potential tools (e.g., Voice to Text platforms) . *   Roles Involved: AI Product Managers, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#3-defining-performance-metrics","title":"3. Defining Performance Metrics","text":"<p>Every application should solve a real business problem, which needs to be grounded in specific metrics . *   Goal: Optimizing for a \"north star output metric\" (e.g., reduce headcount, improve user satisfaction, increase development velocity) and breaking it down into \"input metrics\" that the application will directly target (e.g., reduce average customer support ticket resolution time) . *   Important: Without properly setting this stage, the project risks being deprioritized for not demonstrating enough business value . Alignment with business stakeholders is crucial before implementation . *   Roles Involved: AI Product Managers, AI Engineers, Business Stakeholders .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#4-defining-evaluation-rules","title":"4. Defining Evaluation Rules","text":"<p>Given the complexities of metrics for LLMs (e.g., human alignment, coherence, factuality), defining exact evaluation rules is highly beneficial . *   Goal: Establish clear criteria for judging system responses, especially for chained LLM calls within an Agentic System topology . *   Key Considerations:     *   Prepare an evaluation dataset (Inputs \u2192 Expected Outputs) for each node in your Agentic System topology .     *   Define unacceptable responses (e.g., toxicity, hallucinations, unsafe suggestions) . *   Roles Involved: AI Product Managers, AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#5-building-a-proof-of-concept-poc","title":"5. Building a Proof of Concept (PoC)","text":"<p>This stage emphasizes getting the system into users' hands as quickly as possible . *   Goal: Rapidly push out a user-facing application to gather crucial \"unknown unknowns\" from user feedback [9, 10]. *   Key Considerations:     *   Use LLM APIs from providers like OpenAI, Google, Anthropic, etc., for quick development .     *   The PoC can be as simple as an Excel Spreadsheet with input/output pairs, as long as it helps move metrics forward and is exposed .     *   User feedback is key to shifting perspectives on application improvement . *   Important: If you can't push it out quickly, there's an issue with the process . A successful LLM PoC may look like an Excel Spreadsheet . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#6-instrumenting-the-application-observability","title":"6. Instrumenting the Application (Observability)","text":"<p>This is a key element in implementing Evaluation Driven Development . *   Goal: Implement observability best practices by logging an extensive set of metadata about everything happening within the LLM-based system . *   Key Considerations:     *   Log everything: prompts, completions, embeddings, latency, token counts, and user feedback .     *   Add additional metadata: prompt versions, user inputs, model versions used .     *   Ensure proper connection and ordering of operations within chains, as outputs of one LLM call often become inputs to the next .     *   Log multimodal data (PDFs, images, audio, video) .     *   Crucially, attach user feedback to the traces representing the user interaction when feedback was provided . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#7-integrating-with-an-observability-platform","title":"7. Integrating with an Observability Platform","text":"<p>Beyond just tracking data, efficient visualization and analysis are crucial . *   Goal: Utilize an observability platform for efficient search, visualization, prompt versioning, and automated evaluation capabilities . *   Key Considerations:     *   Store evaluation rules within the platform to apply them to traces .     *   Use platforms as Prompt Registries to analyze and group evaluation results by Prompt Groups, as your application is a chain of prompts .     *   Benefit from smart sampling algorithms for cost-effective storage of traces at scale, as storing all traces can become too expensive .     *   Utilize platform-specific tracing SDKs for seamless instrumentation . *   Important: Set this up early as it brings visibility to the \"black box\" of LLM applications . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#8-evaluating-traced-data","title":"8. Evaluating Traced Data","text":"<p>With instrumentation and platform integration, you can now measure your application effectively . *   Goal: Automatically run evaluations on the collected trace data, especially focusing on identifying failures . *   Key Considerations:     *   Assumption: Evaluation rules are stored in the Observability platform, and traces with human feedback are connected .     *   Automatically run evaluations on traces hitting the Observability Platform .     *   Filter out traces with failing evaluations or negative human feedback; this \"failing\" data will be the primary focus for improvement . *   Important: Running evaluations can be expensive (especially LLM-as-a-judge tactics), so sampling traces might be necessary . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#9-evolving-the-application","title":"9. Evolving the Application","text":"<p>This is where the application is improved based on data-driven insights . *   Goal: Enhance the application by focusing on failing evaluations and human feedback . *   Key Considerations:     *   Start with better prompt engineering, data preprocessing, and tool integration before increasing system complexity .     *   Increase complexity (e.g., Simple Prompts \u2192 RAG \u2192 Agentic RAG \u2192 Agents \u2192 Multi-agent systems) only if there's a hard requirement and the current topology is not up to the task .     *   Maintain a \"failing eval dataset\" that is continuously fed with new failing samples and is never 100% \"solved\" . Your goal is to achieve 100% but by adding more failing samples, you never get there . *   Important: Always involve Domain Experts at this stage for their insider knowledge and potential prompt suggestions . *   Roles Involved: AI Engineers, Domain Experts .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#10-exposing-new-versions-of-the-application","title":"10. Exposing New Versions of the Application","text":"<p>Rapid deployment of new versions is crucial for continuous improvement . *   Goal: Quickly release updates to incorporate fixes and improvements, leveraging invaluable feedback . *   Key Considerations:     *   Fast deployment improves user experience by fixing present problems, and some fixes can generalize to unknown problems .     *   Implement strict release tests and integrate evaluation datasets into CI/CD pipelines to ensure new releases do not degrade performance compared to previous versions . *   Roles Involved: AI Engineers, Domain Experts .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#11-continuous-development-and-evolution-of-the-application","title":"11. Continuous Development and Evolution of the Application","text":"<p>This represents the core iterative loop of the Evaluation Driven Development process . *   Cycle: Build \u2192 Trace, collect feedback \u2192 Evaluate \u2192 Focus on Failing Evals and Negative Feedback \u2192 Improve the application \u2192 Iterate . *   Goal: Continuously evolve the application, adding new functionalities (often as new routes in the Agentic System Topology) by following the same process of prototyping, defining metrics, and new evaluations [19, 20]. For example, a simple chatbot can evolve into a system that manages a shopping cart . *   Roles Involved: AI Engineers, Domain Experts, AI Product Managers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#12-monitoring-and-alerting","title":"12. Monitoring and Alerting","text":"<p>After implementing tracing and evaluation for development, monitoring almost comes out of the box . *   Goal: Reuse implemented evaluations and traces for production monitoring and configure specific alerting thresholds . *   Key Considerations:     *   Most relevant data for LLM-specific production monitoring is already available if application instrumentation was properly implemented .     *   Consider tracing and logging additional advanced metrics like TTFT (Time To First Token) and inter-token latency .     *   You will need to figure out the threshold for alerting . *   Important: Try to avoid Alert Fatigue by carefully configuring thresholds that trigger alerts and avoid False Positives as much as possible . *   Roles Involved: AI Engineers .</p> <p>\u2b06\ufe0f Back to Top</p>"},{"location":"04-applied-research/03-agentic-ai/evalulation-driven-development/#external-reference","title":"\ud83d\udd17 External Reference","text":"<p>Evaluation Driven Development for Agentic Systems - SwirlAI</p>"},{"location":"04-applied-research/04-tool-techniques/01-claude-amp/","title":"\u2705 Claude Code vs Amp \u2013 Feature Comparison","text":"Claude Code Amp \u2705 Working Dockerfile \u2705 Working Dockerfile \u2705 Working deploy CI \u2705 Working deploy CI \u2705 Built <code>todo</code> app with htmx and UI \u2705 Built <code>todo</code> app with htmx and UI \u274c Did not try to test locally before telling me to deploy \u274c Tried to test locally, saw docker wasn\u2019t running so couldn\u2019t, and proceeded to move on without prompting me \u274c Health check route at <code>/api/health</code>, but <code>fly.toml</code> points health check at <code>/</code> \u2705 Health check route at <code>/health</code> and <code>fly.toml</code> points health check at <code>/health</code> \u274c Built in-memory task storage. I did say keep it simple, but that was a bit much \u2705 Started with in-memory, but prompted me to ask whether I would like to use <code>sqlite</code> for persistent storage \u274c No persistent volume for data (in-memory) \u2705 Fly persistent volume \u2705 CSS in <code>css</code> file imported into <code>jinja</code> template \u26a0\ufe0f CSS styling in <code>jinja</code> template \u26a0\ufe0f Didn't recommend other best practices (like test/linting) when making the CI \u2705 Asked if I wanted to include linting/testing in CI \u26a0\ufe0f Deployed with 1 GB memory \u2705 256 MB memory (more appropriate for this particular task) \u2705 CI checks if app exists first in Fly \u26a0\ufe0f Assumes app already exists \u26a0\ufe0f Does not perform health check post-deploy \u2705 Does health check verification in CI"},{"location":"05-community/","title":"\ud83e\uddea Community","text":"<p>\ud83d\udcc2 Navigation Tip Leading Voices in AI</p> <p>Description: A curated directory of prominent figures in the AI field. This section links to influential researchers, innovators, and thought leaders whose work is shaping the future of artificial intelligence. Explore their profiles, publications, and contributions to stay connected with the forefront of AI advancements.</p>"},{"location":"05-community/01-ai-pals/","title":"\ud83e\uddea AI Pals","text":"<p>\ud83d\udcc2 Navigation Tip Leading Voices in AI</p> <p>Description: A curated directory of prominent figures in the AI field. This section links to influential researchers, innovators, and thought leaders whose work is shaping the future of artificial intelligence. Explore their profiles, publications, and contributions to stay connected with the forefront of AI advancements.</p>"},{"location":"05-community/01-ai-pals/hazellabs/","title":"Haize Labs \u2014 Distributional Bias &amp; Key Resources","text":""},{"location":"05-community/01-ai-pals/hazellabs/#quick-links","title":"\ud83d\udccc Quick Links","text":"<ul> <li>Verdict Documentation</li> <li>Distributional Bias (Cookbook)</li> <li>Evals Evals Evals</li> <li>Nimit.io</li> <li>GitHub \u2014 haizelabs/spoken</li> <li>GitHub \u2014 haizelabs/nyc-ai-reading</li> <li>GitHub \u2014 NYC AI Reading (6-26 session)</li> </ul>"},{"location":"05-community/01-ai-pals/hazellabs/#summary-of-distributional-bias-in-llm-as-a-judge","title":"\ud83d\udcc4 Summary of \"Distributional Bias in LLM-as-a-Judge\"","text":"<p>Source: Distributional Bias \u2014 Verdict Documentation</p> <p>Language models (LLMs) used as automated evaluators (LLM-as-a-judge) can suffer from distributional biases introduced by: - Model selection - Prompt design - Ordering of answer choices - Using the same model for generation and judgment  </p> <p>This guide outlines two major bias types and mitigation strategies.</p>"},{"location":"05-community/01-ai-pals/hazellabs/#1-positional-bias","title":"1. Positional Bias","text":"<ul> <li>Issue: The order of presented options can influence model scoring.  </li> <li>Mitigation Techniques:</li> <li>Multiple Evidence Calibration (MEC): Ask for reasoning before assigning scores.</li> <li>Balanced Prediction Calibration (BPC): Average results across all answer-order permutations.</li> <li>Max-Voting: Aggregate scores from multiple random shuffles.</li> <li>Prompt-Level Randomization: Shuffle options before each evaluation.</li> </ul>"},{"location":"05-community/01-ai-pals/hazellabs/#2-self-preference-bias","title":"2. Self-Preference Bias","text":"<ul> <li>Issue: LLMs tend to give higher scores to outputs they generated themselves.  </li> <li>Mitigation: Use different models for generation and evaluation.</li> </ul>"},{"location":"05-community/01-ai-pals/hazellabs/#about-haize-labs-verdict","title":"\ud83c\udfe2 About Haize Labs &amp; Verdict","text":"<ul> <li>Haize Labs builds AI tools for evaluation, bias mitigation, and robustness testing.  </li> <li>Verdict is their flagship product \u2014 a declarative, composable framework for LLM-as-a-judge pipelines with:</li> <li>Cross-provider execution</li> <li>Parallel processing</li> <li>Integrated bias mitigation features</li> </ul>"},{"location":"05-community/01-ai-pals/hazellabs/#key-external-links","title":"\ud83d\udd17 Key External Links","text":"URL Title / Description https://verdict.haizelabs.com/docs/ Verdict Documentation \u2014 Comprehensive guide to building and configuring LLM-as-a-judge workflows. https://verdict.haizelabs.com/docs/cookbook/distributional-bias/ Distributional Bias \u2014 Cookbook entry detailing positional and self-preference biases. https://evalsevalsevals.com/info Evals Evals Evals \u2014 Hub for evaluation benchmarks, datasets, and methodologies. https://nimit.io/ Nimit.io \u2014 Portfolio and project index for Nimit Kalra, associated with Haize Labs."},{"location":"05-community/01-ai-pals/hazellabs/#github-repositories","title":"\ud83d\udcc2 GitHub Repositories","text":"<ul> <li>haizelabs/spoken \u2014 Framework for evaluating spoken dialogue and voice AI systems, measuring accuracy, bias, and robustness.</li> <li>haizelabs/nyc-ai-reading \u2014 Resource repository for NYC AI Reading Group meetings, readings, and discussions.</li> <li>verdict \u2014 Verdict is a declarative framework for specifying and executing compound LLM-as-a-judge systems.</li> </ul>"},{"location":"05-community/01-ai-pals/hazellabs/#haize-labs-white-paper-pdf","title":"Haize Labs White Paper \u2014 PDF","text":""},{"location":"05-community/01-ai-pals/hazellabs/#-download-the-pdf","title":"- Download the PDF","text":""},{"location":"05-community/01-ai-pals/hazellabs/#summary-tables","title":"\ud83d\udcca Summary Tables","text":"<p>Haize Labs Product &amp; Research Overview</p> Topic Description Distributional Bias Positional and self-preference biases in LLM-based evaluators, with strategies to mitigate each. Verdict Framework Modular, declarative framework for constructing reliable, composable LLM-judge pipelines. Research &amp; Whitepaper Describes architecture, primitives, benchmarks, and SOTA results. Open-Source (GitHub) Access to the Verdict library for experimentation and integration. Client Example Real-world use in Vogent\u2019s voice agent evaluation platform. Haize Labs Product Line Full-stack tools for judging, testing, monitoring, and strengthening AI systems in production."}]}